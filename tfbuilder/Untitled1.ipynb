{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import category, normalize\n",
    "\n",
    "letter = {'L'}\n",
    "space = {'Z'}\n",
    "letter_space = {'L', 'Z'}\n",
    "dia = {'M'}\n",
    "punc = {'P'}\n",
    "letter_dia = {'L', 'M'}\n",
    "udnorm = 'NFC'\n",
    "\n",
    "test1a = '-:Κεκρ;?ότη-ται᾿,᾿'\n",
    "test1b = 'Κεκρ;?ότηται᾿,᾿'\n",
    "test1c = '-:Κε.:κρ;?ότη,.τα...ι᾿,᾿'\n",
    "test2 = '''. A ...    Κεκ.,ρότη-ται;? . . .   ?κρη/πὶς ..ἀληθείας, ;ὦ παῖδες ὑμεῖς, ἡμῖν αὐτοῖς, \n",
    "ἁγίου νεὼ μεγάλου θεοῦ θεμέλιος γνώσεως ἀρραγής, προτροπὴ καλή, \n",
    "δι᾿ ὑπακοῆς εὐλόγου ζωῆς ἀιδίουὄρεξις, νοερῷ καταβληθεῖσα χωρίῳ.'''\n",
    "test3 = '᾽κείν᾽'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsplitPunc(word, norm=udnorm, clean=False):\n",
    "    '''This function splits off punctuation \n",
    "    from words on the RIGHT side of the word.\n",
    "    \n",
    "    returns (word, punc)\n",
    "    '''\n",
    "    w = normalize(norm, word)\n",
    "    afterWord = len(w)\n",
    "    for i in range(len(w) - 1, -1, -1):\n",
    "        if category(w[i])[0] not in letter_dia:\n",
    "            afterWord = i\n",
    "        else:\n",
    "            break\n",
    "    if clean:\n",
    "        return (''.join(c for c in w[0:afterWord] \\\n",
    "                          if category(c)[0] in letter_dia), w[afterWord:])\n",
    "    else:\n",
    "        return (w[0:afterWord], w[afterWord:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('κείν', '᾽')\n"
     ]
    }
   ],
   "source": [
    "print(rsplitPunc(test3, clean=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsplitPunc(word, norm=udnorm, clean=False):\n",
    "    '''This function splits off punctuation \n",
    "    from words on the LEFT side of the word.\n",
    "    \n",
    "    returns (punc, word)\n",
    "    '''\n",
    "    w = normalize(norm, word)\n",
    "    beforeWord = -1\n",
    "    for i in range(len(w)):\n",
    "        if category(w[i])[0] not in letter_dia:\n",
    "            beforeWord = i\n",
    "        else:\n",
    "            beforeWord +=1\n",
    "            break\n",
    "    if clean:\n",
    "        return (w[0:beforeWord], ''.join(c for c in w[beforeWord:] \\\n",
    "                                           if category(c)[0] in letter_dia))\n",
    "    else:\n",
    "        return (w[0:beforeWord], w[beforeWord:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsplitPunc(test1a, clean=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = {'L'}\n",
    "space = {'Z'}\n",
    "letter_space = {'L', 'Z'}\n",
    "dia = {'M'}\n",
    "punc = {'P'}\n",
    "letter_dia = {'L', 'M'}\n",
    "udnorm = 'NFC'\n",
    "\n",
    "def splitPunc(words, norm=udnorm, clean=False,\n",
    "              splitters=None, non_splitters=None):\n",
    "    '''This function splits off punctuation \n",
    "    from words on both sides of the word. \n",
    "    It returns a tuple with tuples, containing\n",
    "    the punctuation before, the word itself, \n",
    "    and punctuation after. It can be used for\n",
    "    multiple words\n",
    "    \n",
    "    clean=False:\n",
    "        if punctuation is within the word, the word\n",
    "        will be split into two, except for characters\n",
    "        defined in the non-splitters list.\n",
    "    clean=True:\n",
    "        punctuation within a word will be deleted, \n",
    "        except for characters defined in the splitters \n",
    "        list. In that case, the string will be split.\n",
    "    \n",
    "    \n",
    "    splitters=['character', 'character', ...]\n",
    "    non_splitters=['character', 'character', ...]\n",
    "        \n",
    "    \n",
    "    returns ((pre, word, after), (pre, word, after), ...)\n",
    "    '''\n",
    "    if splitters is None: splitters = ()\n",
    "    if non_splitters is None: non_splitters = ()\n",
    "    w = normalize(norm, words)\n",
    "    pP = 0\n",
    "    for i in range(len(w)):\n",
    "        if category(w[i])[0] in space and pP > 0:\n",
    "            pP += 1\n",
    "            preWord = w[0:pP].strip('\\n')\n",
    "            if preWord:\n",
    "                rest = splitPunc(w[pP:], clean=clean, splitters=splitters, \n",
    "                         non_splitters=non_splitters) if pP < len(w) else ()\n",
    "                return ((preWord, '', ''),) + rest\n",
    "            else:\n",
    "                continue\n",
    "        elif category(w[i])[0] not in letter_dia:\n",
    "            pP += 1  \n",
    "        else:\n",
    "            break\n",
    "    preWord = w[0:pP].strip('\\n') if pP else ''\n",
    "    pW = pP\n",
    "    for i in range(pP, len(w)):\n",
    "        if w[i] in non_splitters:\n",
    "            break\n",
    "        elif category(w[i])[0] in letter_dia:\n",
    "            pW += 1\n",
    "        else:\n",
    "            break\n",
    "    word = w[pP:pW]\n",
    "    pA = pW\n",
    "    nsplit = False\n",
    "    spaceBreak = False\n",
    "    sLoc = None\n",
    "    for i in range(pW, len(w)):\n",
    "        if clean:\n",
    "            if spaceBreak:\n",
    "                if not category(w[i])[0] in letter_dia:\n",
    "                    pA += 1\n",
    "                    if category(w[i])[0] in space:\n",
    "                        sLoc = pA\n",
    "                else:\n",
    "                    break\n",
    "            elif category(w[i])[0] in space:\n",
    "                pA += 1\n",
    "                spaceBreak = True\n",
    "                sLoc = pA\n",
    "            elif w[i] in splitters:\n",
    "                pA += 1\n",
    "                break\n",
    "            elif category(w[i])[0] in letter_dia:\n",
    "                pW = i + 1\n",
    "                pA = pW\n",
    "                word += w[i]\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                pA += 1\n",
    "        else:\n",
    "            if spaceBreak:\n",
    "                if not category(w[i])[0] in letter_dia:\n",
    "                    pA += 1\n",
    "                    if category(w[i])[0] in space:\n",
    "                        sLoc = pA\n",
    "                else:\n",
    "                    break\n",
    "            elif category(w[i])[0] in space:\n",
    "                pA += 1\n",
    "                spaceBreak = True\n",
    "                sLoc = pA\n",
    "            elif w[i] in non_splitters:\n",
    "                nsplit = True\n",
    "                continue\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                nsplit = False\n",
    "                pA += 1\n",
    "            elif category(w[i])[0] in letter_dia and nsplit == True:\n",
    "                pW = i + 1\n",
    "                pA = pW\n",
    "                word += w[i]\n",
    "            else:\n",
    "                break\n",
    "    if not sLoc:\n",
    "        sLoc = pA\n",
    "    afterWord = w[pW:sLoc].strip('\\n')\n",
    "    rest = splitPunc(w[sLoc:], clean=clean, splitters=splitters, \n",
    "                     non_splitters=non_splitters) if sLoc < len(w) else ()\n",
    "    return ((preWord, word, afterWord),) + rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test2)\n",
    "splitPunc(test2, clean=True, non_splitters=('-', '/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanWords(words, norm=udnorm, clean=False,\n",
    "               splitters=None, non_splitters=None): \n",
    "    \"\"\"cleanWords splits off any punctuation and \n",
    "    non-word characters from words in a string. \n",
    "    It can be used for cleaning single words,\n",
    "    or to tokenize full sentences.\n",
    "    \n",
    "    clean=False:\n",
    "        letter characters that have punctuation\n",
    "        inbetween but no space, are split on punctuation\n",
    "        exceptions can be defined in non_splitters\n",
    "    \n",
    "    clean=True:\n",
    "        words with punctuation within (without whitespace) \n",
    "        are glued together without punctuation\n",
    "        exceptions can be defined in splitters\n",
    "    \n",
    "    returns: ('string', 'string', ...)\n",
    "    \"\"\"\n",
    "    if splitters is None: splitters = ()\n",
    "    if non_splitters is None: non_splitters = ()\n",
    "    w = normalize(norm, words)\n",
    "    pP = 0\n",
    "    for i in range(len(w)):\n",
    "        if category(w[i])[0] not in letter_dia:\n",
    "            pP += 1\n",
    "        else:\n",
    "            break\n",
    "    pW = pP\n",
    "    for i in range(pP, len(w)):\n",
    "        if category(w[i])[0] in letter_dia:\n",
    "            pW += 1\n",
    "        else:\n",
    "            break\n",
    "    realWord = w[pP:pW]\n",
    "    pA = pW\n",
    "    nsplit = False\n",
    "    for i in range(pW, len(w)):\n",
    "        if clean:\n",
    "            if category(w[i])[0] in space:\n",
    "                break\n",
    "            elif w[i] in splitters:\n",
    "                break\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                pA += 1\n",
    "            elif category(w[i])[0] in letter_dia:\n",
    "                realWord += w[i]\n",
    "                pA += 1\n",
    "        else:\n",
    "            if w[i] in non_splitters:\n",
    "                nsplit = True\n",
    "                continue\n",
    "            elif category(w[i])[0] in letter_dia and nsplit == True:\n",
    "                pW = i + 1\n",
    "                pA = pW\n",
    "                realWord += w[i]\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                nsplit = False\n",
    "                pA += 1\n",
    "            else:\n",
    "                break\n",
    "    res = (realWord,) + \\\n",
    "          (cleanWords(w[pA:], clean=clean, \n",
    "                      splitters=splitters, non_splitters=non_splitters) \n",
    "           if pA < len(w) else ())\n",
    "    return res if not res == ('',) else ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test2)\n",
    "cleanWords(test2, clean=True,\n",
    "              splitters=('-'), non_splitters=('-', '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, norm=udnorm, punc=False, clean=False,\n",
    "              splitter=None, non_splitter=None, func=None):\n",
    "    \"\"\"tokenize feeds a sentence string\n",
    "    to splitWord, while concatenating the\n",
    "    resulting strings into one tuple.\n",
    "    \n",
    "    clean=False:\n",
    "        split on punctuation without whitespace\n",
    "    clean=True:\n",
    "        delete punctuation inside words\n",
    "    clean=None\n",
    "        \n",
    "    returns: ('string', 'string', ...)\n",
    "    \"\"\"\n",
    "    if func:\n",
    "        func(sentence)\n",
    "    else:\n",
    "        if punc:\n",
    "            if clean:\n",
    "                return tuple(f'{pre}{word}{post}' \\\n",
    "                    for pre, word, post in splitPunc(sentence, norm=udnorm, clean=True,\n",
    "                                                     splitter=splitter, non_splitter=non_splitter))\n",
    "            else:\n",
    "                return tuple(f'{pre}{word}{post}' \\\n",
    "                    for pre, word, post in splitPunc(sentence, norm=udnorm, clean=False,\n",
    "                                                     splitter=splitter, non_splitter=non_splitter))\n",
    "        else:\n",
    "            if clean:\n",
    "                return cleanWords(sentence, clean=True)\n",
    "            else:\n",
    "                return cleanWords(sentence, clean=False)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = {'punc': True, 'nop': False,}\n",
    "\n",
    "def func(word, **kwargs):\n",
    "    if kwargs['punc']:\n",
    "        print(word.lower())\n",
    "\n",
    "func('DEZE!', **error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictio = {'publicationStmt': {'concat': True, 'delimit': ', ', 'end': '.'}}\n",
    "for i in dictio:\n",
    "    if 'end' in dictio[i]:\n",
    "        print(i)\n",
    "        print(dictio[i]['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "attribs = {'a': 4, 'b': 1, 'c': 25, 'd': 1}\n",
    "sorted_attribs = sorted(attribs.items(), key=operator.itemgetter(1))\n",
    "print(sorted_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpertools.unicodetricks import plainLow\n",
    "from helpertools.data.greek_elisions import ELISIONS\n",
    "\n",
    "ELISIONS_norm = {k.strip('᾽'): v for k, v in ELISIONS.items()}\n",
    "\n",
    "def greekReplacements(word):\n",
    "    if word in ELISION_norm:\n",
    "        return ELISION_norm[word]\n",
    "    plain_word = plainLow(word)\n",
    "    # Deletion of movable-nu\n",
    "    if plain_word.endswith(('εν', 'σιν', 'στιν')) and len(midWord_pl) >= 3:\n",
    "        return word[:-1]\n",
    "    # Handling final-sigma\n",
    "    if plain_word.endswith('σ'):\n",
    "        return word[:-1] + 'ς'\n",
    "    # Handling various forms of ου\n",
    "    if plain_word in ('ουχ', 'ουκ'):\n",
    "        return word[:-1]\n",
    "    # Handling ἐξ\n",
    "    if plain_word == 'εξ':\n",
    "        return word[:-1] + 'κ'\n",
    "    \n",
    "print(len(ELISIONS))\n",
    "print(len(ELISIONS_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "import xmlschema\n",
    "import urllib.request\n",
    "from os import path\n",
    "from pprint import pprint\n",
    "\n",
    "# url = 'http://www.stoa.org/epidoc/schema/latest/tei-epidoc.rng'\n",
    "# scheme = urllib.request.urlretrieve(url, path.expanduser('~/github/pthu/tfbuilder/tfbuilder/temp/scheme.xsd'))\n",
    "# xml_scheme = xmlschema.XMLSchema(url)\n",
    "file = path.expanduser('~/github/pthu/sources/pt/tlg0555/tlg002/tlg0555.tlg002.opp-grc1.xml')\n",
    "\n",
    "# pprint()\n",
    "relaxng_doc = etree.parse(path.expanduser('~/github/pthu/tfbuilder/tfbuilder/temp/scheme.xsd'))\n",
    "print(relaxng_doc)\n",
    "relaxng = etree.RelaxNG(relaxng_doc)\n",
    "print(relaxng)\n",
    "\n",
    "doc = etree.parse(file)\n",
    "print(doc)\n",
    "relaxng.validate(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {'a|b'}\n",
    "# print(s)\n",
    "if 'a' in s:\n",
    "    print('yes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem = ['dit is een testinstru-   ', 'ment om te kijken of het werkt.    ', 'en anders is het ge- ', 'woon een proef!']\n",
    "\n",
    "\n",
    "data = ''.join([(line.strip() + ' ' if not line.strip().endswith('-') else line.strip()) for line in elem ])\\\n",
    "          .replace('<', '#!#<')\\\n",
    "          .replace('>', '>#!#')\\\n",
    "          .split('#!#')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'LUDOVICUS OF DINDORFIUS.    '\n",
    "if s.isupper():\n",
    "    print(s.title())\n",
    "    print(' '.join((c.title() if not c.lower() in {'of', 'the',} else c.lower()) for c in s.strip(' ').split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 'name, sdfsdf '\n",
    "# name = st[:st.find(',')]\n",
    "name = st[st.find(',')+2:]\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# ray.init()\n",
    "\n",
    "# @ray.remote\n",
    "# def f(x):\n",
    "#     return x * x\n",
    "\n",
    "# futures = [f.remote(i) for i in range(4)]\n",
    "# print(ray.get(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "lis = [i for i in range(1000)]\n",
    "\n",
    "def f(x):\n",
    "    return print(x * x)\n",
    "\n",
    "pool = Pool()\n",
    "pool.map(f, lis)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from pprint import pprint\n",
    "from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "from cltk.corpus.greek.alphabet import filter_non_greek\n",
    "# beta_to_uni.beta_code(word)\n",
    "\n",
    "beta_to_uni = Replacer()\n",
    "\n",
    "dictio = {}\n",
    "endings = set()\n",
    "\n",
    "with open(path.expanduser('~/github/pthu/sources/moveablenu.txt'), 'r+') as f:\n",
    "    for line in f.readlines():\n",
    "#         print(line)\n",
    "        line = line.lower().replace('j', 's').replace('c', 'x').replace('\\t', ' ')\n",
    "#         print(line)\n",
    "#         line = filter_non_greek(beta_to_uni.beta_code(line)).replace('σ ', 'ς ')\n",
    "#         print(line)\n",
    "        line = beta_to_uni.beta_code(line).replace('σ ', 'ς ')\n",
    "#         print(line)\n",
    "        nom, full = line.strip().split(' ', 1)\n",
    "        if nom.endswith('ν'):\n",
    "            dictio[nom] = full\n",
    "    for i in dictio:\n",
    "        endings.add(i[-3:])\n",
    "#         print(line)\n",
    "#     pprint(f.readlines())\n",
    "pprint(dictio)\n",
    "# pprint(endings)\n",
    "# pprint(list(sorted(endings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = 3\n",
    "headers = [f'level {i}' for i in range(1, levels+1)]\n",
    "headers.append('text')\n",
    "print(headers)\n",
    "print(headers[-8:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = {'{',}\n",
    "stop = {'}',}\n",
    "head = '..{'\n",
    "if set(head) & (start | stop):\n",
    "    print('got it!')\n",
    "    print(f'{head:<12} {start}')\n",
    "    \n",
    "s = 'dit is een tekst-'\n",
    "x, y = s.rsplit(' ', 1)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a='1', b='2', **kwargs):\n",
    "    for k, v in locals().items():\n",
    "        print(f'{k} = {v}')\n",
    "        \n",
    "func(a='k', l='4', m='c', perfect_arg='super')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_add = None\n",
    "if pre_add:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'word'\n",
    "print(word[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = {'a', 'b', 'c',}\n",
    "dictio = {'m': 1, 'k': 5}\n",
    "feat.update(dictio)\n",
    "print(feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = '<<'\n",
    "post = '>>'\n",
    "repl_word = 'dit is een test'\n",
    "\n",
    "if len(repl_word.split(' ')) > 1:\n",
    "    repl_word_split = list(enumerate(repl_word.split(' '), start=1))\n",
    "    print(repl_word_split)\n",
    "    pre_assigned = False\n",
    "    result = []\n",
    "    for n, w in repl_word_split:\n",
    "        if not pre_assigned:\n",
    "            result.append(tuple((pre, w, '')))\n",
    "            pre_assigned = True\n",
    "        elif n == len(repl_word_split):\n",
    "            result.append(tuple(('', w, post)))\n",
    "        else:\n",
    "            result.append(tuple(('', w, '')))\n",
    "    print(tuple(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "token_out = OrderedDict([('pre', {'text': False, 'metadata': 'interpunction before word'}),\n",
    "                      ('word', {'text': True, 'metadata': 'the original format of the word without interpunction'}),\n",
    "                      ('post', {'text': False, 'metadata': 'interpunction after word'}),\n",
    "                     ])\n",
    "print(token_out)\n",
    "for setting, value in enumerate(token_out):\n",
    "    print(setting, value)\n",
    "    \n",
    "dictio = {}\n",
    "dictio.update(token_out)\n",
    "print(dictio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "langsettings = {\n",
    "    'generic': {\n",
    "        'slot_type': 'word',\n",
    "        'intFeatures': set(),\n",
    "        'nonIntFeatures': {'otype', 'oslots',},\n",
    "        'struct_counter': dict(_sentence=1, _phrase=1),\n",
    "        'generic': {},\n",
    "    },\n",
    "    'greek': {**langsettings['generic'], \n",
    "              'slot_type': 'letter',\n",
    "              'generic': {'text': 'text'},\n",
    "              'greekissues': 'no issues'}\n",
    "\n",
    "}\n",
    "pprint(langsettings)\n",
    "pprint(langsettings['generic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_signs = {'start': {'{',},\n",
    "              'stop' : {'}',},}\n",
    "post = ' { '\n",
    "print(post.split(''.join(head_signs['start'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['level1', 'level2', 'level3']\n",
    "struct_counter = dict(_sentence=1, _phrase=1)\n",
    "\n",
    "structs  = tuple(tuple('_book',) + tuple(header) + tuple(struct_counter))\n",
    "print(structs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ''\n",
    "p += 'abcd'\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a',}\n",
    "b = {'b',}\n",
    "c = 'sadf'\n",
    "\n",
    "if set(c) & (a | b):\n",
    "    print(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "a = {'a': {'.', ','},\n",
    "     'b': f\"de parameters zijn: {{{' '.join(a['a'])}}}\",}\n",
    "pprint(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = {1:1}\n",
    "greek = {**generic, 2:2}\n",
    "latin = {3:3}\n",
    "p = {'generic': generic, \n",
    "     'greek': greek,\n",
    "     'latin': latin,}\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_out': OrderedDict([('pre',\n",
      "                            {'metadata': 'interpunction before word',\n",
      "                             'text': False}),\n",
      "                           ('orig',\n",
      "                            {'metadata': 'the original format of the word '\n",
      "                                         'without interpunction',\n",
      "                             'text': True}),\n",
      "                           ('post',\n",
      "                            {'metadata': 'interpunction after word',\n",
      "                             'text': False})])}\n",
      "{'orig', 'pre', 'post'}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "generic = {'token_out': OrderedDict([('pre', {'text': False, 'metadata': 'interpunction before word'}),\n",
    "                                 ('orig', {'text': True, 'metadata': 'the original format of the word without interpunction'}),\n",
    "                                 ('post', {'text': False, 'metadata': 'interpunction after word'}),\n",
    "                                 ])}\n",
    "# pprint(generic)\n",
    "\n",
    "# for i, part in enumerate(generic['token_out'].items()):\n",
    "#     print(i, part)\n",
    "    \n",
    "\n",
    "def token_features(token_out):\n",
    "    featuresInd = []\n",
    "    for i, (part, value) in enumerate(token_out.items()):\n",
    "#         print(i)\n",
    "#         print(part)\n",
    "#         print(value)\n",
    "        part = part\n",
    "        if value['text'] == False:\n",
    "            featuresInd.append((i, part))\n",
    "    return tuple(featuresInd)\n",
    "\n",
    "token_features(generic['token_out'])\n",
    "\n",
    "pprint(generic)\n",
    "s = {k for k in generic['token_out']} & {k for k in generic['token_out']}\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a=True, b=True, c=False, **kwargs):\n",
    "    print(locals())\n",
    "    kwargs.update(locals())\n",
    "    print(kwargs)\n",
    "    print(dictio['kwargs'])\n",
    "    return kwargs\n",
    "\n",
    "dictio = {'a': 1, 'b': 2}\n",
    "\n",
    "\n",
    "for i, p in func(**dictio).items():\n",
    "    print(i, p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Σ\n"
     ]
    }
   ],
   "source": [
    "print('Σ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = False\n",
    "if not type(m) == bool:\n",
    "    print('yes')\n",
    "\n",
    "s = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dit,dat,dut,dot,det\n"
     ]
    }
   ],
   "source": [
    "a = {'dit', 'dat', 'dut', 'dot', 'det'}\n",
    "print(','.join(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
