{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import category, normalize\n",
    "\n",
    "letter = {'L'}\n",
    "space = {'Z'}\n",
    "letter_space = {'L', 'Z'}\n",
    "dia = {'M'}\n",
    "punc = {'P'}\n",
    "letter_dia = {'L', 'M'}\n",
    "udnorm = 'NFC'\n",
    "\n",
    "test1a = '-:Κεκρ;?ότη-ται᾿,᾿'\n",
    "test1b = 'Κεκρ;?ότηται᾿,᾿'\n",
    "test1c = '-:Κε.:κρ;?ότη,.τα...ι᾿,᾿'\n",
    "test2 = '''   Κεκ.,ρότη-ται;?    ?κρη/πὶς ..ἀληθείας, ;ὦ παῖδες ὑμεῖς, ἡμῖν αὐτοῖς, \n",
    "ἁγίου νεὼ μεγάλου θεοῦ θεμέλιος γνώσεως ἀρραγής, προτροπὴ καλή, \n",
    "δι᾿ ὑπακοῆς εὐλόγου ζωῆς ἀιδίουὄρεξις, νοερῷ καταβληθεῖσα χωρίῳ.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsplitPunc(word, norm=udnorm, clean=False):\n",
    "    '''This function splits off punctuation \n",
    "    from words on the RIGHT side of the word.\n",
    "    \n",
    "    returns (word, punc)\n",
    "    '''\n",
    "    w = normalize(norm, word)\n",
    "    afterWord = len(w)\n",
    "    for i in range(len(w) - 1, -1, -1):\n",
    "        if category(w[i])[0] not in letter_dia:\n",
    "            afterWord = i\n",
    "        else:\n",
    "            break\n",
    "    if clean:\n",
    "        return (''.join(c for c in w[0:afterWord] \\\n",
    "                          if category(c)[0] in letter_dia), w[afterWord:])\n",
    "    else:\n",
    "        return (w[0:afterWord], w[afterWord:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rsplitPunc(test1a, clean=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsplitPunc(word, norm=udnorm, clean=False):\n",
    "    '''This function splits off punctuation \n",
    "    from words on the LEFT side of the word.\n",
    "    \n",
    "    returns (punc, word)\n",
    "    '''\n",
    "    w = normalize(norm, word)\n",
    "    beforeWord = -1\n",
    "    for i in range(len(w)):\n",
    "        if category(w[i])[0] not in letter_dia:\n",
    "            beforeWord = i\n",
    "        else:\n",
    "            beforeWord +=1\n",
    "            break\n",
    "    if clean:\n",
    "        return (w[0:beforeWord], ''.join(c for c in w[beforeWord:] \\\n",
    "                                           if category(c)[0] in letter_dia))\n",
    "    else:\n",
    "        return (w[0:beforeWord], w[beforeWord:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsplitPunc(test1a, clean=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitPunc(words, norm=udnorm, clean=False,\n",
    "              splitters=None, non_splitters=None):\n",
    "    '''This function splits off punctuation \n",
    "    from words on both sides of the word. \n",
    "    It returns a tuple with tuples, containing\n",
    "    the punctuation before, the word itself, \n",
    "    and punctuation after. It can be used for\n",
    "    multiple words\n",
    "    \n",
    "    clean=False:\n",
    "        if punctuation is within the word, the word\n",
    "        will be split into two, except for characters\n",
    "        defined in the non-splitters list.\n",
    "    clean=True:\n",
    "        punctuation within a word will be deleted, \n",
    "        except for characters defined in the splitters \n",
    "        list. In that case, the string will be split.\n",
    "    \n",
    "    \n",
    "    splitters=['character', 'character', ...]\n",
    "    non_splitters=['character', 'character', ...]\n",
    "        \n",
    "    \n",
    "    returns ((pre, word, after), (pre, word, after), ...)\n",
    "    '''\n",
    "    if splitters is None: splitters = ()\n",
    "    if non_splitters is None: non_splitters = ()\n",
    "    w = normalize(norm, words)\n",
    "    pP = 0\n",
    "    for i in range(len(w)):\n",
    "        if category(w[i])[0] not in letter_dia:\n",
    "            pP += 1\n",
    "        else:\n",
    "            break\n",
    "    preWord = w[0:pP].strip() if pP else ''\n",
    "    pW = pP\n",
    "    for i in range(pP, len(w)):\n",
    "        if w[i] in non_splitters:\n",
    "            break\n",
    "        elif category(w[i])[0] in letter_dia:\n",
    "            pW += 1\n",
    "        else:\n",
    "            break\n",
    "    word = w[pP:pW]\n",
    "    pA = pW\n",
    "    nsplit = False\n",
    "    for i in range(pW, len(w)):\n",
    "        if clean:\n",
    "            if category(w[i])[0] in space:\n",
    "                pA += 1\n",
    "                break\n",
    "            elif w[i] in splitters:\n",
    "                pA += 1\n",
    "                break\n",
    "            elif category(w[i])[0] in letter_dia:\n",
    "                pW = i + 1\n",
    "                pA = pW\n",
    "                word += w[i]\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                pA += 1\n",
    "        else:\n",
    "            if category(w[i])[0] in space:\n",
    "                pA += 1\n",
    "                break\n",
    "            elif w[i] in non_splitters:\n",
    "                nsplit = True\n",
    "                continue\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                nsplit = False\n",
    "                pA += 1\n",
    "            elif category(w[i])[0] in letter_dia and nsplit == True:\n",
    "                pW = i + 1\n",
    "                pA = pW\n",
    "                word += w[i]\n",
    "            else:\n",
    "                break\n",
    "    afterWord = w[pW:pA].strip()\n",
    "    rest = splitPunc(w[pA:], clean=clean, splitters=splitters, \n",
    "                     non_splitters=non_splitters) if pA < len(w) else ()\n",
    "    return ((preWord, word, afterWord),) + rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test2)\n",
    "splitPunc(test2, clean=False, non_splitters=('-', '/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanWords(words, norm=udnorm, clean=False,\n",
    "               splitters=None, non_splitters=None): \n",
    "    \"\"\"cleanWords splits off any punctuation and \n",
    "    non-word characters from words in a string. \n",
    "    It can be used for cleaning single words,\n",
    "    or to tokenize full sentences.\n",
    "    \n",
    "    clean=False:\n",
    "        letter characters that have punctuation\n",
    "        inbetween but no space, are split on punctuation\n",
    "        exceptions can be defined in non_splitters\n",
    "    \n",
    "    clean=True:\n",
    "        words with punctuation within (without whitespace) \n",
    "        are glued together without punctuation\n",
    "        exceptions can be defined in splitters\n",
    "    \n",
    "    returns: ('string', 'string', ...)\n",
    "    \"\"\"\n",
    "    if splitters is None: splitters = ()\n",
    "    if non_splitters is None: non_splitters = ()\n",
    "    w = normalize(norm, words)\n",
    "    pP = 0\n",
    "    for i in range(len(w)):\n",
    "        if category(w[i])[0] not in letter_dia:\n",
    "            pP += 1\n",
    "        else:\n",
    "            break\n",
    "    pW = pP\n",
    "    for i in range(pP, len(w)):\n",
    "        if category(w[i])[0] in letter_dia:\n",
    "            pW += 1\n",
    "        else:\n",
    "            break\n",
    "    realWord = w[pP:pW]\n",
    "    pA = pW\n",
    "    nsplit = False\n",
    "    for i in range(pW, len(w)):\n",
    "        if clean:\n",
    "            if category(w[i])[0] in space:\n",
    "                break\n",
    "            elif w[i] in splitters:\n",
    "                break\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                pA += 1\n",
    "            elif category(w[i])[0] in letter_dia:\n",
    "                realWord += w[i]\n",
    "                pA += 1\n",
    "        else:\n",
    "            if w[i] in non_splitters:\n",
    "                nsplit = True\n",
    "                continue\n",
    "            elif category(w[i])[0] in letter_dia and nsplit == True:\n",
    "                pW = i + 1\n",
    "                pA = pW\n",
    "                realWord += w[i]\n",
    "            elif category(w[i])[0] not in letter_dia:\n",
    "                nsplit = False\n",
    "                pA += 1\n",
    "            else:\n",
    "                break\n",
    "    res = (realWord,) + \\\n",
    "          (cleanWords(w[pA:], clean=clean, \n",
    "                      splitters=splitters, non_splitters=non_splitters) \n",
    "           if pA < len(w) else ())\n",
    "    return res if not res == ('',) else ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test2)\n",
    "cleanWords(test2, clean=True,\n",
    "              splitters=('-'), non_splitters=('-', '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, norm=udnorm, punc=False, clean=False,\n",
    "              splitter=None, non_splitter=None, func=None):\n",
    "    \"\"\"tokenize feeds a sentence string\n",
    "    to splitWord, while concatenating the\n",
    "    resulting strings into one tuple.\n",
    "    \n",
    "    clean=False:\n",
    "        split on punctuation without whitespace\n",
    "    clean=True:\n",
    "        delete punctuation inside words\n",
    "    clean=None\n",
    "        \n",
    "    returns: ('string', 'string', ...)\n",
    "    \"\"\"\n",
    "    if func:\n",
    "        func(sentence)\n",
    "    else:\n",
    "        if punc:\n",
    "            if clean:\n",
    "                return tuple(f'{pre}{word}{post}' \\\n",
    "                    for pre, word, post in splitPunc(sentence, norm=udnorm, clean=True,\n",
    "                                                     splitter=splitter, non_splitter=non_splitter))\n",
    "            else:\n",
    "                return tuple(f'{pre}{word}{post}' \\\n",
    "                    for pre, word, post in splitPunc(sentence, norm=udnorm, clean=False,\n",
    "                                                     splitter=splitter, non_splitter=non_splitter))\n",
    "        else:\n",
    "            if clean:\n",
    "                return cleanWords(sentence, clean=True)\n",
    "            else:\n",
    "                return cleanWords(sentence, clean=False)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deze!\n"
     ]
    }
   ],
   "source": [
    "error = {'punc': True, 'nop': False,}\n",
    "\n",
    "def func(word, **kwargs):\n",
    "    if kwargs['punc']:\n",
    "        print(word.lower())\n",
    "\n",
    "func('DEZE!', **error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictio = {'publicationStmt': {'concat': True, 'delimit': ', ', 'end': '.'}}\n",
    "for i in dictio:\n",
    "    if 'end' in dictio[i]:\n",
    "        print(i)\n",
    "        print(dictio[i]['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "attribs = {'a': 4, 'b': 1, 'c': 25, 'd': 1}\n",
    "sorted_attribs = sorted(attribs.items(), key=operator.itemgetter(1))\n",
    "print(sorted_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "from helpertools.unicodetricks import plainLow\n",
    "from helpertools.data.greek_elisions import ELISIONS\n",
    "\n",
    "ELISIONS_norm = {k.strip('᾽'): v for k, v in ELISIONS.items()}\n",
    "\n",
    "def greekReplacements(word):\n",
    "    if word in ELISION_norm:\n",
    "        return ELISION_norm[word]\n",
    "    plain_word = plainLow(word)\n",
    "    # Deletion of movable-nu\n",
    "    if plain_word.endswith(('εν', 'σιν', 'στιν')) and len(midWord_pl) >= 3:\n",
    "        return word[:-1]\n",
    "    # Handling final-sigma\n",
    "    if plain_word.endswith('σ'):\n",
    "        return word[:-1] + 'ς'\n",
    "    # Handling various forms of ου\n",
    "    if plain_word in ('ουχ', 'ουκ'):\n",
    "        return word[:-1]\n",
    "    # Handling ἐξ\n",
    "    if plain_word == 'εξ':\n",
    "        return word[:-1] + 'κ'\n",
    "    \n",
    "print(len(ELISIONS))\n",
    "print(len(ELISIONS_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
