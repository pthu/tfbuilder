{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import re, pickle, betacode.conv\n",
    "from os import path\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from itertools import takewhile\n",
    "from ordered_set import OrderedSet\n",
    "from unicodedata import category, normalize\n",
    "from collections import OrderedDict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Text Fabric imports\n",
    "from tf.fabric import Fabric, Timestamp\n",
    "from tf.convert.walker import CV\n",
    "\n",
    "# Local imports ##TODO! Cleanup...\n",
    "from helpertools.lemmatizer import lemmatize\n",
    "from helpertools.unicodetricks import *\n",
    "from helpertools.xmlparser import xmlSplitter, dataParser, metadataReader, attribsAnalysis #, lenAttribsDict, sectionElems\n",
    "from tf_config import langsettings, generic_metadata\n",
    "from data.tlge_metadata import tlge_metadata\n",
    "from data.attrib_errors import error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversion:\n",
    "    def __init__(self, data, **kwargs):\n",
    "        self.data        = data                                # Data in preprocessed XML or CSV\n",
    "        for setting, value in kwargs.items():                  # Set langsettings in tf_config as class attributes\n",
    "            setattr(self, setting, value)                      # NB 'lang' defines the part of langsettings\n",
    "        self.featuresInd = self.token_features(self.token_out) # Define indexes of features in token output tokenizer\n",
    "        \n",
    "        # Collect feature restricted metadata from tf_config\n",
    "        self.featureMeta = {\n",
    "            **{k: {'description': v['description']} for k, v in self.text_formats.items()}, \\\n",
    "            **{k: {'description': v['description']} for k, v in self.token_out.items()}, \\\n",
    "            **{k: {'description': v} for k, v in self.struct_counter_metadata.items()}\n",
    "        }\n",
    "        self.nonIntFeatures = kwargs['nonIntFeatures'].copy() | \\\n",
    "                              {k for k in self.token_out} | \\\n",
    "                              {k for k in self.text_formats}\n",
    "        \n",
    "        # Variables used in processing\n",
    "        self.res_text = None    # Handle text that ends with non_splitter\n",
    "        \n",
    "        \n",
    "            \n",
    "    def token_features(self, token_out):\n",
    "        featuresInd = []\n",
    "        for i, (part, value) in enumerate(token_out.items()):\n",
    "            if value['text'] == False:\n",
    "                featuresInd.append((i, part))\n",
    "            # Add to nonIntFeatures, because all stringparts are expected to be non-ints\n",
    "            self.nonIntFeatures.add(part)\n",
    "        return tuple(featuresInd)\n",
    "\n",
    "            \n",
    "    def process_text(self, text):\n",
    "        text_output = []\n",
    "              \n",
    "        #Handle wordbreaks\n",
    "        if self.res_text != None:\n",
    "            text, self.res_text = self.res_text + text, None\n",
    "        if text.endswith(self.tokenizer_args['non_splitters']):\n",
    "            text, self.res_text = text.rstrip(''.join(self.tokenizer_args['non_splitters'])).rsplit(' ', 1)\n",
    "            text += ' ' #Add space deleted by split(' ')\n",
    "\n",
    "        #Process text\n",
    "        # NB 'orig' is compulsory to have in self.text_formats!\n",
    "        for t in self.tokenizer(text, **self.tokenizer_args):\n",
    "            # Define original word\n",
    "            origAssigned = False\n",
    "            orig_word = self.text_formats['orig']['function'](t)\n",
    "\n",
    "            # NB The replace_func can return multiple tokens if words are split like greek crasis forms\n",
    "            for token in self.replace_func(t):\n",
    "                token_processed = {}\n",
    "                \n",
    "                #Assign orig format\n",
    "                if not origAssigned:\n",
    "                    token_processed['orig'] = orig_word\n",
    "                    origAssigned = True\n",
    "                else:\n",
    "                    token_processed['orig'] = ''\n",
    "                    \n",
    "                # Process text data\n",
    "                for form, values in self.text_formats.items():\n",
    "                    if form not in token_processed: #Prevent the replacement of 'orig'\n",
    "                        token_processed[form] = values['function'](token)\n",
    "                    \n",
    "                # Process feature data\n",
    "                for i, part in self.featuresInd:\n",
    "                    token_processed[part] = token[i]\n",
    "                \n",
    "                # Append dict to output list\n",
    "                text_output.append(token_processed)\n",
    "        \n",
    "        # Output list of dicts with text and feature data\n",
    "        return text_output  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Csv2tf(Conversion):\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(data, **kwargs)\n",
    "        self.head     = self.get_header(self.header)\n",
    "        self.sections = self.head[:-1] if self.header == True \\\n",
    "                                 else (list(filter(None, self.generic['citation_scheme'].lower().split('/'))) \\\n",
    "                                       if 'citation_scheme' in self.generic \\\n",
    "                                 else list(filter(None, input(\"No header data could be found; please enter an appropriate header: \").lower().split())) )\n",
    "        self.structs  = tuple(('_book',) + tuple(self.head[:-1]) + tuple(self.struct_counter))\n",
    "        self.otext = {\n",
    "            **{v['otext_name']: v['format'] for k, v in self.text_formats.items()}, \\\n",
    "            **{'sectionTypes': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'sectionFeatures': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'structureTypes': f'_book,{\",\".join(self.structs)}'}, \\\n",
    "            **{'structureFeatures': f'_book,{\",\".join(self.structs)}'}\n",
    "        }\n",
    "        \n",
    "        # Calculate metadata for struct levels\n",
    "        for num, struct in enumerate(self.structs[1:], 1):\n",
    "            self.featureMeta[struct] = {'description': f'structure feature of the {num}{\"st\" if num == 1 else \"\"}{\"nd\" if num == 2 else \"\"}{\"rd\" if num == 3 else \"\"}{\"th\" if num > 3 else \"\"} level',}\n",
    "\n",
    "        # Handle tlg head text marked by {head}\n",
    "        self.head_signs = {'start': {'{',},\n",
    "                           'stop' : {'}',},}\n",
    "        \n",
    "    def get_header(self, head):\n",
    "        \n",
    "        def check_header(measure, typed_input):\n",
    "            if len(typed_input) == measure:\n",
    "                return [h.lower() for h in typed_input]\n",
    "            else:\n",
    "                print(f'The inputed number of header titles is {len(typed_input)}, while it should be {measure}')\n",
    "                typed_input = list(filter(None, input(\"No header data could be found; please enter an appropriate header split by spaces:\").split()))\n",
    "                check_header(measure, typed_input)\n",
    "        \n",
    "        levels = len(self.data[0].split('\\t'))\n",
    "        if head == False:\n",
    "            levels = len(self.data[0].split('\\t'))\n",
    "            if levels == 0:\n",
    "                header = []\n",
    "            else:\n",
    "                header = check_header(levels, list(filter(None, input(\"No header data could be found; please enter an appropriate header: \").split())))\n",
    "        else:\n",
    "            if isinstance(head, (list, tuple)):\n",
    "                header = check_header(levels, head)\n",
    "            elif self.header == True:\n",
    "                header = self.data[0].split('\\t')\n",
    "                self.data = self.data[1:]\n",
    "                levels = len(self.data[0].split('\\t'))\n",
    "                header = check_header(levels, header)    \n",
    "            else:\n",
    "                print(\"something is wrong with the header...!\")\n",
    "        return [h.lower() for h in header]\n",
    "        \n",
    "    \n",
    "    def director(self, cv):\n",
    "        nonIntFeatures = self.nonIntFeatures.copy()    # keep track of features that are not ints\n",
    "        counter        = self.struct_counter.copy()    # keep track of calculated struct features defined in tf_config\n",
    "        udnorm         = self.udnorm            # define the Unicode norm used\n",
    "        lemma_counter  = [0, 0]                 # keep track of wordforms converted successfully to lemmata\n",
    "        cur            = {}                     # keep track of node number assignments\n",
    "        # VARIABLES TO PROCESS PREPROCESSED TLG-E OUTPUT\n",
    "        tlg_head       = False                  # if true text will be processed as head-feature\n",
    "        tlg_head_cont  = ''                     # variable to store head text elements\n",
    "        \n",
    "        # Define bookname and start first node assignment to cur\n",
    "        cur['_book'] = cv.node('_book')\n",
    "        book_title = self.generic['title'] if 'title' in self.generic else 'no title found in metadata'\n",
    "        book_title_full = self.generic['title_full'] if 'title_full' in self.generic else book_title\n",
    "        cv.feature(cur['_book'], _book=book_title)\n",
    "        cv.meta('_book', description=book_title_full)\n",
    "        nonIntFeatures.add('_book')\n",
    "        \n",
    "        # Initiate counters\n",
    "        for count in counter:\n",
    "            cur[count] = cv.node(count)\n",
    "            cv.feature(cur[count], **{count: counter[count]})\n",
    "            \n",
    "        refAssigned = False\n",
    "        w           = False\n",
    "        \n",
    "        # PROCESS CSV-DATA LINE BY LINE\n",
    "        for line in self.data:\n",
    "            # Split reference and text; NB text is always the last element!\n",
    "            splitline = line.split('\\t')\n",
    "            ref = splitline[:-1]\n",
    "            text = splitline[-1].strip()\n",
    "            if not text.endswith(self.tokenizer_args['non_splitters']):\n",
    "                text += ' '\n",
    "     \n",
    "            # PROCESS TEXT\n",
    "            # NB token_out is a dictionary with all the text/feature formats\n",
    "            for token_out in self.process_text(text):\n",
    "                pprint(token_out)\n",
    "                \n",
    "                # ------------------------------------------------\n",
    "                # Handle TLG head titles (words enclosed in {...})\n",
    "                if self.typ == 'tlge':\n",
    "                    if tlg_head == True:\n",
    "                        # In case 'pre' has the head end sign\n",
    "                        if self.head_signs['stop'] & ( set(token_out['pre']) | set(token_out['post']) ):\n",
    "                            tlg_head = False\n",
    "                            if 'head' in cur: cv.terminate(cur['head'])\n",
    "                            cur['head'] = cv.node('head')\n",
    "                            cv.meta('head', description=\"head title\",)\n",
    "                            nonIntFeatures.add('head')\n",
    "                            if self.head_signs['stop'] & set(token_out['pre']):\n",
    "                                content = tlg_head_cont\n",
    "                            if self.head_signs['stop'] & set(token_out['post']): \n",
    "                                content = tlg_head_cont + f\"{token_out['orig']}\"\n",
    "                            cv.feature(cur['head'], **{'head': content})\n",
    "                            tlg_head_cont = ''\n",
    "                            if self.head_signs['stop'] & set(token_out['post']): continue\n",
    "                        # In case the token is fully part of the tlg head\n",
    "                        else:\n",
    "                            tlg_head_cont += f\"{token_out['orig']}\"\n",
    "                            continue\n",
    "                    if tlg_head == False:\n",
    "                        if self.head_signs['start'] & set(token_out['pre']):\n",
    "                            # if the whole head is in one token_out\n",
    "                            if self.head_signs['stop'] & set(token_out['post']):\n",
    "                                if 'head' in cur: cv.terminate(cur['head'])\n",
    "                                content = f\"{token_out['orig']}\"\n",
    "                                cur['head'] = cv.node('head')\n",
    "                                cv.feature(cur['head'], **{'head': content})\n",
    "                                cv.meta('head', description=\"head title\",)\n",
    "                                nonIntFeatures.add('head')\n",
    "                                continue\n",
    "                            else:\n",
    "                                tlg_head = True\n",
    "                                tlg_head_cont += f\"{token_out['orig']}\"\n",
    "                                continue\n",
    "                        else:\n",
    "                            if self.head_signs['start'] & set(token_out['post']):\n",
    "                                tlg_head = True\n",
    "                                token_out['post'], tlg_head_cont = token_out['post'].split(''.join(self.head_signs['start']))\n",
    "                                tlg_head_cont = ''.join(self.head_signs['start']) + tlg_head_cont\n",
    "                # End tlg heads\n",
    "                # ----------------------------------------------\n",
    "\n",
    "                # Handle empty tokens that still have a pre feature, by adding them to the previous post and orig\n",
    "                if 'plain' in token_out and token_out['plain'] == '':\n",
    "                    if 'pre' in token_out and 'post' in token_out:\n",
    "                        try: # if there is already an existing slot number\n",
    "                            cv.resume(w)\n",
    "                            pre = token_out['pre']\n",
    "                            orig = cv.get('orig', w) + pre\n",
    "                            cv.feature(w, orig=orig)\n",
    "                            post = cv.get('post', w) + pre\n",
    "                            cv.feature(w, post=post)\n",
    "                            # Check phrase and sentence counters\n",
    "                            if set(pre) & self.phrase_delimit:\n",
    "                                if cv.linked(cur['_phrase']):\n",
    "                                    cv.terminate(cur['_phrase'])\n",
    "                                    counter['_phrase'] +=1\n",
    "                                    cur['_phrase'] = cv.node('_phrase')\n",
    "                                    cv.feature(cur['_phrase'], **{'_phrase': counter['_phrase']}) \n",
    "                            if set(pre) & self.sentence_delimit:\n",
    "                                for count in ('_phrase', '_sentence'):\n",
    "                                    if cv.linked(cur[count]):\n",
    "                                        cv.terminate(cur[count])\n",
    "                                        counter[count] +=1\n",
    "                                        cur[count] = cv.node(count)\n",
    "                                        cv.feature(cur[count], **{count: counter[count]})  \n",
    "                            cv.terminate(w)\n",
    "                        except:\n",
    "                            pass\n",
    "                    continue                                     \n",
    "\n",
    "                # HANDLE SECTIONING\n",
    "                if refAssigned == False:\n",
    "                    for ind, sec in enumerate(self.sections):\n",
    "                        if sec in cur and cv.active(cur[sec]):          # Check whether section level is active\n",
    "                            cur_sec = cv.get(sec, cur[sec])             # Get the current section value\n",
    "                            new_sec = ref[ind]                          # Get the section value in the present line\n",
    "                            if not cur_sec == new_sec:                  # Check whether the old and the new value are equal\n",
    "                                for s in self.sections[:ind:-1]:        # If not, terminate all lower section levels\n",
    "                                    cv.terminate(cur[s])\n",
    "                                cv.terminate(cur[sec])                  # Terminate the current section level\n",
    "                                cur[sec] = cv.node(sec)                 # Create new section node\n",
    "                                cv.feature(cur[sec], **{sec: ref[ind]}) # Add new value to the new section node\n",
    "                        else:                                           # In case the section is not present in cur OR not active\n",
    "                            cur[sec] = cv.node(sec)                     # Create new section node\n",
    "                            cv.feature(cur[sec], **{sec: ref[ind]})     # Add new value to section node\n",
    "                        if not ref[ind].isdigit():                      # Check whether the value is not an int\n",
    "                            nonIntFeatures.add(sec)                     # In case the value is no int, add the FEATURE to the set of nonIntFeatures\n",
    "                    refAssigned = True\n",
    "                    \n",
    "                # SLOT ASSIGNMENT!\n",
    "                # ================\n",
    "                w = cv.slot()\n",
    "                # Handle the data dictionary with text formats and features\n",
    "                for name, value in token_out.items():\n",
    "                    cv.feature(w, **{name: value})\n",
    "                    \n",
    "                # ================\n",
    "\n",
    "                # Check phrase and sentence counters\n",
    "                if 'post' in token_out:\n",
    "                    if set(token_out['post']) & self.phrase_delimit:\n",
    "                        if cv.linked(cur['_phrase']):\n",
    "                            cv.terminate(cur['_phrase'])\n",
    "                            counter['_phrase'] +=1\n",
    "                            cur['_phrase'] = cv.node('_phrase')\n",
    "                            cv.feature(cur['_phrase'], **{'_phrase': counter['_phrase']}) \n",
    "                    if set(token_out['post']) & self.sentence_delimit:\n",
    "                        for count in ('_phrase', '_sentence'):\n",
    "                            if cv.linked(cur[count]):\n",
    "                                cv.terminate(cur[count])\n",
    "                                counter[count] +=1\n",
    "                                cur[count] = cv.node(count)\n",
    "                                cv.feature(cur[count], **{count: counter[count]}) \n",
    "                \n",
    "                # Run lemma counter\n",
    "                if 'lemma' in token_out:\n",
    "                    if token_out['lemma'].startswith('*'):\n",
    "                        lemma_counter[1] +=1 \n",
    "                    else:\n",
    "                        lemma_counter[0] +=1\n",
    "                        \n",
    "        # In case the csv-file has a header, but is empty: make one empty slot\n",
    "        if not w:\n",
    "            if self.ignore_empty == False:\n",
    "                w = cv.slot()\n",
    "                for feat in self.nonIntFeatures:\n",
    "                    cur[feat] = cv.node(feat)\n",
    "                    cv.feature(cur[feat], **{feat: ''})\n",
    "                for feat in self.intFeatures:\n",
    "                    cur[feat] = cv.node(feat)\n",
    "                    cv.feature(cur[feat], **{feat: ''})\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        # Terminate structs (includes sections!)\n",
    "        for ntp in self.structs[::-1]:\n",
    "            if ntp in cur: cv.terminate(cur[ntp])\n",
    "        # Terminate any remaining active nodes in cur\n",
    "        for ntp in cur:\n",
    "            if ntp in cur: cv.terminate(cur[ntp])\n",
    "\n",
    "        # Calculate lemmatizer coverage of lemmata\n",
    "        if not lemma_counter == [0, 0]:\n",
    "            cv.meta('lemma', **{'coverage_ratio': f'{round(lemma_counter[0] / ((lemma_counter[0] + lemma_counter[1]) / 100 ), 2)}%'})\n",
    "\n",
    "        # Assign the correct valueType to features\n",
    "        for feature in cv.metaData:\n",
    "            if feature in nonIntFeatures:\n",
    "                cv.meta(feature, valueType='str')\n",
    "            else:\n",
    "                if feature == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    cv.meta(feature, valueType='int')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tlg2tf(Csv2tf):\n",
    "    def __init__():\n",
    "        super().__init__(self, data, lang='greek', **kwargs)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xml2tf(Conversion):\n",
    "    def __init__(self, data, lang='generic', **kwargs):\n",
    "        super().__init__(self, data, lang, **kwargs)\n",
    "\n",
    "    def director(self, cv):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(\n",
    "        input_path, \n",
    "        output_path,\n",
    "        tlg_out         = False,\n",
    "        ignore_empty    = True,\n",
    "        generic         = generic_metadata,  # Generic metadata from tf_config\n",
    "        lang            = 'generic',         # Chosen language as available in langsettings in tf_config\n",
    "        typ             = False,             # Used to introduce subclases of a language; e.g. 'tlge' in addition to 'greek'\n",
    "        header          = False,             # If True: first line of csv would be taken as header. Also tuple and list are allowed\n",
    "        version         = '1.0',             # Version number to be added in the metadata of every tf-file\n",
    "        langsettings    = langsettings,      # Reference to langsettings\n",
    "        multiprocessing = False,             # Can be used if many files need to be converted. If 'True', the program checks number of available cores authomatically; if int, it will try to use that number of cores \n",
    "        chunksize       = 1,                 # Defines the number of files to be send to each core in multiprocessing mode\n",
    "        silent          = False,             # Keeps TF messages silent\n",
    "    \n",
    "        ):\n",
    "    '''The convert function is the core of the tei2tf module\n",
    "    \n",
    "    It takes the following arguments:\n",
    "    in_path:  the path that contains the TEI formatted texts\n",
    "    out_path: the path to which the tf-files would be written\n",
    "    **kwargs: a dictionary that is usually derived from the\n",
    "              config.py file, that contains all important\n",
    "              parameters for the conversion (see documentation)\n",
    "    '''\n",
    "    tm           = Timestamp()\n",
    "    kwargs       = langsettings[lang]\n",
    "    dir_struct   = kwargs['dir_struct']\n",
    "    sLemmatizer  = kwargs['lemmatizer']()\n",
    "    count1       = 0     # counts the number input files\n",
    "    count2       = 0     # counts the number of successfully processed files\n",
    "    \n",
    "    # Add parameters to kwargs\n",
    "    kwargs['ignore_empty'] = ignore_empty\n",
    "    kwargs['generic']      = generic\n",
    "    kwargs['lang']         = lang\n",
    "    kwargs['typ']          = typ\n",
    "    kwargs['header']       = header\n",
    "    kwargs['version']      = version\n",
    "    \n",
    "    # input-output file management\n",
    "    inpath = path.expanduser(input_path)\n",
    "    outpath = path.expanduser(output_path)\n",
    "    \n",
    "    # Necessary to make process_file picklable for multiprocessing\n",
    "    global process_file\n",
    "    \n",
    "    def process_file(file):\n",
    "        nonlocal count1\n",
    "        nonlocal count2\n",
    "#         nonlocal silent\n",
    "        if file.endswith('.csv'):\n",
    "            count1 +=1\n",
    "            tm.info(f'parsing {file}')\n",
    "            filename = path.splitext(file)[0].split('/')[-1]\n",
    "            with open(file, 'r') as file_open:\n",
    "                data = file_open.readlines()\n",
    "                metadat = tlge_metadata[filename]\n",
    "                kwargs['generic'].update(tlge_metadata[filename])\n",
    "                \n",
    "                if tlg_out == True:\n",
    "                    dirs = kwargs['generic']['key'].split(' ')\n",
    "                # definition of output dir structure on the basis of metadata\n",
    "                else:\n",
    "                    dirs = []\n",
    "                    for i in dir_struct:\n",
    "                        assigned = False\n",
    "                        for j in i:\n",
    "                            if j in metadat:\n",
    "                                dirs.append(metadat[j])\n",
    "                                assigned = True\n",
    "                                break\n",
    "                        if assigned == False:\n",
    "                            dirs.append(f'unknown {\"-\".join(i)}')\n",
    "\n",
    "                # dirs is a list of lists of which the tagnames used are defined in config.py\n",
    "                # they usually correspond to something like (author, work, editor/edition)\n",
    "                # in case of multiple editions of the same work, a number will be prefixed\n",
    "                C = 1\n",
    "                if path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                    while path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                        C +=1\n",
    "                    else:\n",
    "                        TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "                else:\n",
    "                    TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "\n",
    "                # setting up the text-fabric engine\n",
    "                TF = Fabric(locations=TF_PATH, silent=silent)\n",
    "                cv = CV(TF, silent=silent)\n",
    "                # initiating the Conversion class that provides all\n",
    "                # necessary data and methods for cv.walk()\n",
    "                x = Csv2tf(data, **kwargs)\n",
    "                # running cv.walk() to generate the tf-files\n",
    "                good = cv.walk(\n",
    "                    x.director,\n",
    "                    slotType=x.slot_type,\n",
    "                    otext=x.otext,\n",
    "                    generic=x.generic,\n",
    "                    intFeatures=x.intFeatures,\n",
    "                    featureMeta=x.featureMeta,\n",
    "                    warn=True,\n",
    "                )\n",
    "                # Count number of successfully converted files\n",
    "                if good: \n",
    "                    count2 +=1\n",
    "                    tm.info('   |    Conversion was successful...\\n')\n",
    "                else:\n",
    "                    tm.info('   |    Unfortunately, conversion was not successful...')\n",
    "                    if ignore_empty == True:\n",
    "                        tm.info('   |    The most probable reason is that no slot numbers could be assigned...\\n')\n",
    "   \n",
    "        elif file.endswith('.xml'):\n",
    "            count1 +=1\n",
    "#             if count1 > 1: print('\\n')\n",
    "            tm.info(f'parsing {file}')\n",
    "\n",
    "            # creation of data to extract metadata\n",
    "            # and to inject later into the Conversion object\n",
    "            data = dataParser(xmlSplitter(file), lang=lang)\n",
    "            body_index, metadat = metadataReader(data, lang=lang, **langsettings['metadata'])\n",
    "            metadat.update(metadata)\n",
    "    #         pprint(metadata)\n",
    "\n",
    "            # definition of output dir structure on the basis of metadata\n",
    "            dirs = []\n",
    "            for i in dir_struct:\n",
    "                assigned = False\n",
    "                for j in i:\n",
    "                    if j in metadat:\n",
    "                        dirs.append(metadat[j])\n",
    "                        assigned = True\n",
    "                        break\n",
    "                if assigned == False:\n",
    "                    dirs.append(f'unknown {\"-\".join(i)}')\n",
    "\n",
    "            # dirs is a list of lists of which the tagnames used are defined in config.py\n",
    "            # they usually correspond to something like (author, work, editor/edition)\n",
    "            # in case of multiple editions of the same work, a number will be prefixed\n",
    "            C = 1\n",
    "            if path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                while path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                    C +=1\n",
    "                else:\n",
    "                    TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "            else:\n",
    "                TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "\n",
    "            # setting up the text-fabric engine\n",
    "            TF = Fabric(locations=TF_PATH, silent=silent)\n",
    "            cv = CV(TF, silent=silent)\n",
    "            # initiating the Conversion class that provides all\n",
    "            # necessary data and methods for cv.walk()\n",
    "            x = Xml2tf(data[body_index:], metadat, sLemmatizer=sLemmatizer, lang=lang, **langsettings)\n",
    "            # running cv.walk() to generate the tf-files\n",
    "            good = cv.walk(\n",
    "                x.director,\n",
    "                slotType=slot_type,\n",
    "                otext=x.otext,\n",
    "                generic=x.generic,\n",
    "                intFeatures=x.intFeatures,\n",
    "                featureMeta=x.featureMeta,\n",
    "                warn=True,\n",
    "            )\n",
    "            # Count number of successfully converted files\n",
    "            if good: \n",
    "                count2 +=1\n",
    "                tm.info(f'   |    Conversion of {file.split(\"/\")[-1]} was successful...!\\n')\n",
    "            else:\n",
    "                tm.info('   |    Unfortunately, conversion of {file.split(\"/\")[-1]} was not successful...\\n')\n",
    "                \n",
    "    # Define list of files to be processed\n",
    "    file_list = glob(f'{inpath}/**/*.*', recursive=True)\n",
    "        \n",
    "    if multiprocessing:\n",
    "        if not type(multiprocessing) == bool:\n",
    "            # Manual assignment of cores\n",
    "            pool = Pool(processes=multiprocessing)\n",
    "        else:\n",
    "            pool = Pool()\n",
    "        # Manual assignment of chunksize if many files need to be consumed\n",
    "        # Manual assignment might improve performance\n",
    "        pool.imap_unordered(process_file, file_list, chunksize=chunksize)\n",
    "    #     pool.imap_unordered(process_file, file_list)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "    else:\n",
    "        for file in file_list:\n",
    "            process_file(file)\n",
    "        \n",
    "    tm.info(f'{count2} of {count1} works have successfully been converted!')\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(input_path, output_path, lang='generic', typ=False, **kwargs):\n",
    "#     # For how to change the kwargs arguments: https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict\n",
    "    \n",
    "    \n",
    "#     if typ == 'tlge':\n",
    "#         kwargs['head_signs'] =  {'start': '{',\n",
    "#                                  'stop': '}',}\n",
    "        \n",
    "#     #Check for original or preprocessed tlg-E files\n",
    "    \n",
    "#     elif typ == 'mss':\n",
    "#         pass\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert('~/github/tlgu-1/out/csv', \n",
    "#         '~/github/tlgu-1/out/csv/tf',\n",
    "#         tlg_out=True,\n",
    "#         lang='greek', \n",
    "#         typ='tlge', \n",
    "#         header=True,\n",
    "#         multiprocessing=False,\n",
    "#         chunksize=1,\n",
    "#         silent=True,\n",
    "#        )\n",
    "\n",
    "convert('~/github/tlgu-1/debug/test', \n",
    "        '~/github/tlgu-1/out/csv/tf', \n",
    "        tlg_out=True,\n",
    "        lang='greek', \n",
    "        typ='tlge', \n",
    "        header=True,\n",
    "        multiprocessing=False,\n",
    "        chunksize=50,\n",
    "        silent=True,\n",
    "       )\n",
    "\n",
    "# Test\n",
    "# convert('~/github/tlgu-1/TEST/csv_test', '~/github/tlgu-1/TEST/csv_test/out', lang='greek', typ='tlge', header=True, multiprocessing=False, silent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
