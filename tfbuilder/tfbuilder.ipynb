{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import re, pickle, betacode.conv\n",
    "from os import path\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from itertools import takewhile\n",
    "from ordered_set import OrderedSet\n",
    "from unicodedata import category, normalize\n",
    "from collections import OrderedDict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Text Fabric imports\n",
    "from tf.fabric import Fabric, Timestamp\n",
    "from tf.convert.walker import CV\n",
    "\n",
    "# Local imports ##TODO! Cleanup...\n",
    "from helpertools.lemmatizer import lemmatize\n",
    "from helpertools.unicodetricks import *\n",
    "from helpertools.xmlparser import xmlSplitter, dataParser, metadataReader, attribsAnalysis #, lenAttribsDict, sectionElems\n",
    "from tf_config import langsettings, generic_metadata\n",
    "from data.tlge_metadata import tlge_metadata\n",
    "from data.attrib_errors import error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversion:\n",
    "    def __init__(self, data, lang='generic', typ=None, **kwargs):\n",
    "        self.data        = data                                # Data in preprocessed XML or CSV\n",
    "        self.lang        = lang                                # Language\n",
    "        self.typ         = typ                                 # Subspecification of language; e.g. tlge\n",
    "        self.generic     = generic_metadata                    # Generic TF metadata\n",
    "        for setting, value in kwargs[self.lang].items():       # Set langsettings in tf_config as class attributes\n",
    "            setattr(self, setting, value)                      # NB 'lang' defines the part of langsettings\n",
    "        self.featuresInd = self.token_features(self.token_out) # Define indexes of features in token output tokenizer\n",
    "        \n",
    "        # Collect feature restricted metadata from tf_config\n",
    "        self.featureMeta = {\n",
    "            **{v['name']: {'description': v['metadata']} for k, v in self.text_formats.items()}, \\\n",
    "            **{k: {'description': v['metadata']} for k, v in self.token_out.items()},\n",
    "        }\n",
    "            \n",
    "        # Variables used in processing\n",
    "        self.res_text = None    # Handle text that ends with non_splitter\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        print('The current class attributes are:')\n",
    "        for key, value in self.__dict__.items():\n",
    "            print(f'{key:<20} = {value}')\n",
    "            \n",
    "            \n",
    "    def token_features(self, token_out):\n",
    "        featuresInd = []\n",
    "        for i, part in enumerate(self.token_out):\n",
    "            if part['text'] == False:\n",
    "                featuresInd.append((i, part))\n",
    "            # Add to nonIntFeatures, because all stringparts are expected to be non-ints\n",
    "            self.nonIntFeatures.add(part)\n",
    "        return tuple(featuresInd)\n",
    "\n",
    "            \n",
    "    def process_text(self, text):\n",
    "        text_output = []\n",
    "              \n",
    "        #Handle wordbreaks\n",
    "        if self.res_text != None:\n",
    "            text, self.res_text = self.res_text + text, None\n",
    "        if text.endswith(self.non_splitters):\n",
    "            text, self.res_text = text.rstrip(''.join(self.non_splitters)).rsplit(' ', 1)\n",
    "\n",
    "        #Process text\n",
    "        # NB 'orig' is compulsory to have in self.text_formats!\n",
    "        for t in self.tokenizer(text, **self.tokenizer_args):\n",
    "            # Define original word\n",
    "            origAssigned = False\n",
    "            orig_word = self.text_formats['orig']['function'](t)\n",
    "\n",
    "            # NB The replace_func can return multiple tokens if words are split like greek crasis forms\n",
    "            for token in self.replace_func(t):\n",
    "                token_processed = {}\n",
    "                \n",
    "                #Check if token has only features and no text\n",
    "                if \n",
    "                \n",
    "                #Assign orig format\n",
    "                if not origAssigned:\n",
    "                    token_processed['orig'] = orig_word\n",
    "                    origAssigned = True\n",
    "                else:\n",
    "                    token_processed['orig'] = ''\n",
    "                    \n",
    "                # Process text data\n",
    "                for form in self.text_formats:\n",
    "                    token_processed[form]['name'] = form.function(token)\n",
    "                    \n",
    "                # Process feature data\n",
    "                for i, part in self.featuresInd:\n",
    "                    token_processed[part] = token[i]\n",
    "                \n",
    "                # Append dict to output list\n",
    "                text_output.append(token_processed)\n",
    "                \n",
    "        return text_output  \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Csv2tf(Conversion):\n",
    "    def __init__(self, data, lang='generic', header=False, **kwargs):\n",
    "        super().__init__(self, data, lang, **kwargs)\n",
    "        self.header   = self.get_header(header)\n",
    "        self.sections = self.header if header == True \\\n",
    "                                 else ( list(filter(None, self.generic['citation_scheme'].split('/'))) \\\n",
    "                                       if 'citation_scheme' in self.generic \\\n",
    "                                 else list(filter(None, input(\"No header data could be found; please enter an appropriate header: \").split())) )\n",
    "        self.structs  = tuple(('_book',) + tuple(self.header) + tuple(self.struct_counter))\n",
    "        self.otext = {\n",
    "            **{k: v['format'] for k, v in self.text_formats.items()}, \\\n",
    "            **{'sectionTypes': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'sectionFeatures': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'structureTypes': f'_book,{\",\".join(self.structs)}'}, \\\n",
    "            **{'structureFeatures': f'_book,{\",\".join(self.structs)}'}\n",
    "        }\n",
    "        self.featureMeta = {v['name']: {'description': v['metadata']} \\\n",
    "                            for k, v in self.text_formats.items()}\n",
    "        \n",
    "        for i, struct in enumerate(self.structs, 1):\n",
    "            self.featureMeta[struct] = {'description': f'structure feature of the {num}{\"st\" if num == 1 else \"\"}{\"nd\" if num == 2 else \"\"}{\"rd\" if num == 3 else \"\"}{\"th\" if num > 3 else \"\"} level',}\n",
    "\n",
    "        # Handle tlg head text marked by {head}\n",
    "        self.head_signs = {'start': {'{',},\n",
    "                           'stop' : {'}',},}\n",
    "        \n",
    "    def get_header(head):\n",
    "        \n",
    "        def check_header(measure, typed_input):\n",
    "            if len(typed_input) == measure:\n",
    "                print('header successfully entered!')\n",
    "                return typed_input\n",
    "            else:\n",
    "                print(f'The inputed number of header titles is {len(typed_input)}, while it should be {measure}')\n",
    "                typed_input = list(filter(None, input(\"No header data could be found; please enter an appropriate header: \").split()))\n",
    "                check_header(measure, typed_input)\n",
    "        \n",
    "        levels = len(self.data[0].split('\\t'))+\n",
    "        \n",
    "        if head == False:\n",
    "            levels = len(self.data[0].split('\\t'))\n",
    "            if levels == 0:\n",
    "                header = []\n",
    "            else:\n",
    "                header = check_header(levels, list(filter(None, input(\"No header data could be found; please enter an appropriate header: \").split())))\n",
    "        else:\n",
    "            if isinstance(head, (list, tuple)):\n",
    "                header = check_header()\n",
    "                header = head\n",
    "            else:\n",
    "                header = self.data[0].split('\\t')[:-1]\n",
    "                self.data = self.data[1:]\n",
    "        return header\n",
    "        \n",
    "    \n",
    "    def director(self, cv):\n",
    "        nonIntFeatures = self.nonIntFeatures\n",
    "        counter        = self.struct_counter\n",
    "        udnorm         = self.udnorm\n",
    "        \n",
    "        lemma_counter        = [0, 0]\n",
    "        cur                  = {}\n",
    "        \n",
    "        #Designate bookname and start first node assignment\n",
    "        cur['_book'] = cv.node('_book')\n",
    "        book_title = self.generic['title'] if 'title' in self.generic else 'no title found in metadata'\n",
    "        book_title_full = self.generic['title_full'] if 'title_full' in self.generic else book_title\n",
    "        cv.feature(cur['_book'], _book=book_title)\n",
    "        cv.meta('_book', description=book_title_full)\n",
    "        nonIntFeatures.add('_book')\n",
    "        \n",
    "        #Declaration of global variables used in the process_text() method!\n",
    "        tlg_head = False\n",
    "        head_res = None\n",
    "        \n",
    "        for line in self.data:\n",
    "            splitline = line.split('\\t')\n",
    "            ref = splitline[:-1]\n",
    "            text = splitline[-1].strip()\n",
    "        \n",
    "            # Handle sectioning\n",
    "            ind = 0\n",
    "            for sec in self.sections:\n",
    "                num = ind + 1\n",
    "                if sec in cur and cv.active(cur[sec]):\n",
    "                    cur_sec = cv.get(sec, cur[sec])\n",
    "                    new_sec = ref[ind]\n",
    "                    if not cur_sec == new_sec:\n",
    "                        for s in self.sections[:ind:-1]:\n",
    "                            cv.terminate(cur[s])\n",
    "                        cv.terminate(cur[sec])\n",
    "                        cur[sec] = cv.node(sec)\n",
    "                        cv.feature(cur[sec], **{sec: ref[ind]})\n",
    "#                         cv.meta(sec, description=f'structure feature of the {num}{\"st\" if num == 1 else \"\"}{\"nd\" if num == 2 else \"\"}{\"rd\" if num == 3 else \"\"}{\"th\" if num > 3 else \"\"} level',)\n",
    "                else:\n",
    "                    cur[sec] = cv.node(sec)\n",
    "                    cv.feature(cur[sec], **{sec: ref[ind]})\n",
    "#                     cv.meta(sec, description=f'structure feature of the {num}{\"st\" if num == 1 else \"\"}{\"nd\" if num == 2 else \"\"}{\"rd\" if num == 3 else \"\"}{\"th\" if num > 3 else \"\"} level',)\n",
    "                if not ref[ind].isdigit():\n",
    "                    nonIntFeatures.add(sec)\n",
    "                ind +=1\n",
    "                    \n",
    "            # Process text\n",
    "            # NB token_out is a dictionary with all the text/feature formats\n",
    "            for token_out in self.process_text(text):\n",
    "                \n",
    "                \n",
    "                # Handle TLG heads {head words}\n",
    "                if self.typ == 'tlge':\n",
    "                    if tlg_head == True:\n",
    "                        # In case 'pre' has the head end sign\n",
    "                        if self.head_signs['stop'] & set(token_out['pre']):\n",
    "                            tlg_head = False\n",
    "                        # In case 'post' has the head end sign\n",
    "                        elif self.head_signs['stop'] & set(token_out['post']):\n",
    "                            if 'head' in cur and not cv.linked(cur['head']):\n",
    "                                content = f\"{cv.get('head', cur['head'])}{token_out['pre']}{token_out['orig']{token_out['post']}\"\n",
    "                                cv.feature(cur['head'], **{'head': content})\n",
    "                            tlg_head = False\n",
    "                            continue\n",
    "                        # In case the token is fully part of the tlg head\n",
    "                        else:\n",
    "                            # In case already parts of head exist\n",
    "                            if 'head' in cur and not cv.linked(cur['head']):\n",
    "                                content = f\"{cv.get('head', cur['head'])}{token_out['pre']}{token_out['orig']{token_out['post']}\"\n",
    "                                cv.feature(cur['head'], **{'head': content})\n",
    "                            # In case a new head has to be made\n",
    "                            else:\n",
    "                                if 'head' in cv.activeTypes() and cv.linked(cur['head']):\n",
    "                                    cv.terminate(cur['head'])\n",
    "                                cur['head'] = cv.node('head')\n",
    "                                content = f\"{token_out['pre']}{token_out['orig']{token_out['post']}\"\n",
    "                                if head_res:\n",
    "                                    content = head_res + content\n",
    "                                    head_res = None\n",
    "                                cv.feature(cur['head'], **{'head': content})\n",
    "                                cv.meta('head', description=\"head title\",)\n",
    "                                nonIntFeatures.add('head')\n",
    "                            continue\n",
    "\n",
    "                    if tlg_head == False:\n",
    "                        if self.head_signs['start'] & set(token_out['pre']):\n",
    "                            self.tlg_head = True\n",
    "                            if 'head' in cv.activeTypes() and cv.linked(cur['head']):\n",
    "                                cv.terminate(cur['head'])\n",
    "                            cur['head'] = cv.node('head')\n",
    "                            content = f\"{token_out['pre']}{token_out['orig']{token_out['post']}\"\n",
    "                            cv.feature(cur['head'], **{'head': content})\n",
    "                            cv.meta('head', description=\"head title\",)\n",
    "                            nonIntFeatures.add('head')\n",
    "                            continue\n",
    "                        else:\n",
    "                            if self.head_signs['start'] & set(token_out['post']):\n",
    "                                self.tlg_head = True\n",
    "                                token_out['post'], head_res = token_out['post'].split(''.join(head_signs['start']))\n",
    "                                head_res = ''.join(head_signs['start']) + head_res\n",
    "\n",
    "                # Handle empty slots that still have features, by adding them to the previous slot\n",
    "                \n",
    "\n",
    "                # SLOT ASSIGNMENT!\n",
    "                w = cv.slot()\n",
    "                # Handle the data dictionary with text formats and features\n",
    "                for name, value in token_out.items():\n",
    "                    cv.feature(w, **{name: value})\n",
    "                \n",
    "                # Run lemma counter\n",
    "                if 'lemma' in token_out:\n",
    "                    if token_out['lemma'].startswith('*'):\n",
    "                        lemma_counter[1] +=1 \n",
    "                    else:\n",
    "                        lemma_counter[0] +=1\n",
    "        \n",
    "    \n",
    "        for ntp in self.structs[::-1]:\n",
    "            if ntp in cur: cv.terminate(cur[ntp])\n",
    "        for ntp in cur:\n",
    "            if ntp in cur: cv.terminate(cur[ntp])\n",
    "\n",
    "        if not lemma_counter == [0, 0]:\n",
    "            cv.meta('lemma', **{'coverage_ratio': f'{round(lemma_counter[0] / ((lemma_counter[0] + lemma_counter[1]) / 100 ), 2)}%'})\n",
    "        cv.meta('_sentence', description=f\"sentences defined by the following delimiters: {self.langsettings['sentence_delimit']}\",)\n",
    "        cv.meta('_phrase', description=f\"phrases defined by the following delimiters: {self.langsettings['phrase_delimit']}\",)\n",
    "        for feature in cv.metaData:\n",
    "            if feature in nonIntFeatures:\n",
    "                cv.meta(feature, valueType='str')\n",
    "            else:\n",
    "                if feature == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    cv.meta(feature, valueType='int')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tlg2tf(Csv2tf):\n",
    "    def __init__():\n",
    "        super().__init__(self, data, lang='greek', **kwargs)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xml2tf(Conversion):\n",
    "    def __init__(self, data, lang='generic', **kwargs):\n",
    "        super().__init__(self, data, lang, **kwargs)\n",
    "\n",
    "    def director(self, cv):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(input_path, output_path, lang='generic', typ=False, **kwargs):\n",
    "    # For how to change the kwargs arguments: https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict\n",
    "    \n",
    "    \n",
    "    if typ == 'tlge':\n",
    "        kwargs['head_signs'] =  {'start': '{',\n",
    "                                 'stop': '}',}\n",
    "        \n",
    "    #Check for original or preprocessed tlg-E files\n",
    "    \n",
    "    elif typ == 'mss':\n",
    "        pass\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
