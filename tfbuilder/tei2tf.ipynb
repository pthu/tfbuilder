{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TEI XML to Text-Fabric Convertor\n",
    "\n",
    "XML-TEI textfiles can be converted to [Text-Fabric format](https://dans-labs.github.io/text-fabric/Model/File-formats/) by using this convertor. It has been designed for Greek, but it should also work with minimal adjustments for other languages (except for the implemented lemmatizer).\n",
    "\n",
    "See this [readme](https://github.com/pthu/patristics) for more information about the corpus and this work.\n",
    "\n",
    "See this [notebook](https://nbviewer.jupyter.org/github/annotation/banks/blob/master/programs/convert.ipynb) for a simple setup for a tf conversion if you like to build your own convertor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import betacode.conv\n",
    "from pprint import pprint\n",
    "\n",
    "from os import path\n",
    "from glob import glob\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import takewhile\n",
    "from ordered_set import OrderedSet\n",
    "from unicodedata import category, normalize\n",
    "from tf.fabric import Fabric, Timestamp\n",
    "from tf.convert.walker import CV\n",
    "from pprint import pprint\n",
    "from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "from cltk.corpus.greek.alphabet import filter_non_greek\n",
    "from greek_normalisation.normalise import Normaliser\n",
    "# from greek_normalisation.norm_data import ELISIONS, MOVABLE\n",
    "\n",
    "# Local imports\n",
    "from helpertools.lemmatizer import lemmatize\n",
    "from helpertools.unicodetricks import *\n",
    "from helpertools.xmlparser import xmlSplitter, elemParser, metadataReader, attribsAnalysis, lenAttribsDict, sectionElems\n",
    "from tf_config import langsettings\n",
    "from data.attrib_errors import error_dict\n",
    "from convertor_metadata import convertor_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = Timestamp()\n",
    "\n",
    "class Conversion:\n",
    "    def __init__(self, data, meta, sLemmatizer=None, lang='generic', **kwargs):\n",
    "        self.data              = data\n",
    "        self.lang              = lang\n",
    "        self.langsettings      = kwargs\n",
    "        self.attribs_dict, \\\n",
    "        self.section_tags, \\\n",
    "        self.open_section_tags = attribsAnalysis(self.data, lang=self.lang, **kwargs)\n",
    "        self.len_attribs_dict  = lenAttribsDict(self.attribs_dict)\n",
    "        self.section_dict, \\\n",
    "        self.sections          = sectionElems(self.attribs_dict, self.section_tags, **self.langsettings)\n",
    "        self.struct_list       = OrderedSet()\n",
    "        self.metadata          = meta\n",
    "        self.lemmatizer        = sLemmatizer\n",
    "\n",
    "        # TF SPECIFIC VARIABLES\n",
    "        self.slotType         = self.langsettings['slot_type']\n",
    "        self.intFeatures      = set()\n",
    "        self.generic          = meta\n",
    "        #TODO: add entry 'availableStructure'\n",
    "                        \n",
    "        # Definition of text formats\n",
    "        self.otext = {\n",
    "            **{k: v['format'] for k, v in self.langsettings['text_formats'].items()}, \\\n",
    "            **{'sectionTypes': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'sectionFeatures': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'structureTypes': f'{\",\".join(self.sections)}'}, \\\n",
    "            **{'structureFeatures': f'{\",\".join(self.sections)}'}\n",
    "        }\n",
    "#         pprint(self.otext)\n",
    "                      # TODO: add 'sectionTypes', 'sectionFeatures',  'structureTypes' 'structureFeatures' \n",
    "\n",
    "        # These are the feature metadata that are present in all tf-packages to be produced... \n",
    "        # Other metadata will be added during the run of the director()...\n",
    "        self.featureMeta = {v['name']: {'description': v['metadata']} \\\n",
    "                            for k, v in self.langsettings['text_formats'].items()}\n",
    "#         pprint(self.featureMeta)\n",
    "\n",
    "\n",
    "    def director(self, cv):\n",
    "        Comment = False\n",
    "        nonIntFeatures = {'otype', 'oslots',}\n",
    "        excludeTags = set()\n",
    "        counter = dict(_sentence=1, _phrase=1)\n",
    "        cur = {}\n",
    "        tagList = []\n",
    "        secElems = []\n",
    "        orderedSectionSet = OrderedSet()\n",
    "        closedSectionList = []\n",
    "        lemma_counter = [0, 0]\n",
    "        \n",
    "        tagList.append('_book')\n",
    "        cur['_book'] = cv.node('_book')\n",
    "        cv.feature(cur['_book'], _book=self.metadata['title'])\n",
    "        nonIntFeatures.add('_book')\n",
    "        \n",
    "        TEXT = False\n",
    "        udnorm = self.langsettings['udnorm']\n",
    "        \n",
    "        for elem in self.data:\n",
    "            code, content = elemParser(elem, lang=self.lang, **self.langsettings)\n",
    "            \n",
    "            if code == 'text':\n",
    "                assigned = False\n",
    "                for tag in tagList:\n",
    "                    if tag in self.langsettings['non_text_elems'] and tag in cv.activeTypes():\n",
    "                        elem = normalize(udnorm, elem)\n",
    "                        cv.feature(cur[tag], **{tag: elem})\n",
    "                        cv.meta(tag, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                        nonIntFeatures.add(tag)\n",
    "                        assigned = True\n",
    "                        break\n",
    "                if assigned == True:\n",
    "                    continue\n",
    "     \n",
    "                if self.lang == 'greek':\n",
    "                    try:\n",
    "                        elem.encode('ascii')\n",
    "                        elem = normalize(udnorm, betacode.conv.beta_to_uni(elem))\n",
    "                    except UnicodeEncodeError:\n",
    "                        elem = normalize(udnorm, elem)\n",
    "                        \n",
    "                for token in self.langsettings['tokenizer'](elem, **self.langsettings['tokenizer_args']):\n",
    "                    # midWord_pl will be used for various normalization actions\n",
    "                    word_orig = token[1]\n",
    "                    token_norm = self.langsettings['replace_func'](token) \\\n",
    "                              if 'replace_func' in self.langsettings \\\n",
    "                              else token\n",
    "                    pre, word, post = token_norm\n",
    "                    \n",
    "                    if not plainLow(word):\n",
    "                        if set(pre) & langsettings['sentence_delimit']:\n",
    "                            cv.terminate(cur['_sentence'])\n",
    "                            counter['_sentence'] +=1\n",
    "                        try:\n",
    "                            cv.resume(w)\n",
    "                            orig = cv.get('orig', w) + pre\n",
    "                            try:\n",
    "                                post = cv.get('post', w) + pre\n",
    "                                cv.feature(w, post=post)\n",
    "                            except:\n",
    "                                pass\n",
    "                            cv.feature(w, orig=orig)\n",
    "                            cv.terminate(w)\n",
    "                            continue\n",
    "                        except UnboundLocalError:\n",
    "                            continue\n",
    "                    if TEXT == False:\n",
    "                        self.struct_list.update(('_phrase', '_sentence'))\n",
    "                    TEXT = True\n",
    "                    \n",
    "                    for struct in self.struct_list:\n",
    "                        if struct not in cv.activeTypes():\n",
    "                            if struct in {'_phrase', '_sentence'}:\n",
    "                                cur[struct] = cv.node(struct)\n",
    "                                cv.feature(cur[struct], _sentence=counter[struct])\n",
    "                            else:\n",
    "                                cur[struct] = cv.node(struct)\n",
    "                                cv.feature(cur[struct], **{struct: 0})\n",
    "                    w = cv.slot()\n",
    "                        \n",
    "                    for _, form in self.langsettings['text_formats'].items():\n",
    "                        name = form['name']\n",
    "                        func = form['function']\n",
    "                        if name == 'orig':\n",
    "                            cv.feature(w, name=func(token))\n",
    "                        elif name == 'lemma':\n",
    "                            lemma = func(token, self.lemmatizer)\n",
    "                            if lemma.startswith('*'):\n",
    "                                lemma_counter[1] +=1\n",
    "                            else:\n",
    "                                lemma_counter[0] +=1\n",
    "                        else:\n",
    "                            cv.feature(w, name=func(token_norm))\n",
    "                            \n",
    "                        if pre != '':\n",
    "                            cv.feature(w, pre=pre)\n",
    "                            cv.meta('pre', description='pre gives non-letter characters at the start of a word',)\n",
    "                            nonIntFeatures.add('pre')\n",
    "                        if post != '':\n",
    "                            cv.feature(w, post=post)\n",
    "                            cv.meta('post', description='post gives non-letter characters at the end of a word',)\n",
    "                            nonIntFeatures.add('post')\n",
    "                            if set(post) & self.langsettings['sentence_delimit'] and TEXT == True:\n",
    "                                cv.terminate(cur['_sentence'])\n",
    "                                counter['_sentence'] +=1\n",
    "                                TEXT = False\n",
    "                            if set(post) & self.langsettings['phrase_delimit'] and TEXT == True:\n",
    "                                cv.terminate(cur['_phrase'])\n",
    "                                counter['_phrase'] +=1\n",
    "                                TEXT = False\n",
    "                                           \n",
    "                                            \n",
    "            elif code == 'closeTag':\n",
    "                if tagList[-1] in secElems:\n",
    "                    if not cv.linked(cur[tagList[-1]]): # CHECK whether this works as expected!\n",
    "                        cv.slot()\n",
    "                    index = secElems.index(tagList[-1])\n",
    "                    for ntp in secElems[:index:-1]:\n",
    "                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                cv.terminate(cur[tagList[-1]])\n",
    "                del tagList[-1]\n",
    "                \n",
    "                \n",
    "            elif code in {'openAttrTag','closedAttrTag'}:\n",
    "                tag, attribs = content\n",
    "                tag_name  = tuple((tag, tuple(key for key in attribs.keys() \\\n",
    "                              if key not in self.langsettings['ignore_attrib_keys'])))\n",
    "                # Section elements\n",
    "                if tag_name in self.section_dict:\n",
    "                    sec, val = self.section_dict[tag_name]\n",
    "                    if len(attribs) == 1:\n",
    "                        section = sec\n",
    "                        value = attribs[val]\n",
    "                    else:\n",
    "                        section = attribs[sec]\n",
    "                        value = attribs[val]\n",
    "                    if section in cur: cv.terminate(cur[section])\n",
    "                    tagList.append(section)\n",
    "                    if not section in self.langsettings['ignore_section_values']:\n",
    "                        for el in secElems[:-1]:\n",
    "                            if not el in cv.activeTypes():\n",
    "                                cur[el] = cv.node(el)\n",
    "                                cv.feature(cur[el], **{el: 0})\n",
    "                        secElems.append(section)\n",
    "                        orderedSectionSet.add(section)\n",
    "                        cur[section] = cv.node(section)\n",
    "                        cv.feature(cur[section], **{section: value})\n",
    "                        continue\n",
    "                    else:\n",
    "                        cur[section] = cv.node(section)\n",
    "                        cv.feature(cur[section], **{section: value})\n",
    "                        continue\n",
    "                # Non-section elements\n",
    "                else:\n",
    "                    if 'n' in attribs:\n",
    "                        value = attribs['n']\n",
    "                        name = max(self.attribs_dict[tag_name], \n",
    "                                   key=lambda key: self.attribs_dict[tag_name][key] \\\n",
    "                                   if not key == 'n' else OrderedSet())\n",
    "                    else:\n",
    "                        name = max(attribs_dict[tag_name], \n",
    "                                    key=lambda key: attribs_dict[tag_name][key])\n",
    "                        value = max(attribs_dict[tag_name], \n",
    "                                          key=lambda key: attribs_dict[tag_name][key] \\\n",
    "                                          if not value == name else OrderedSet())\n",
    "                    tagList.append(attribs[name])\n",
    "                    cur[attribs[name]] = cv.node(attribs[name])\n",
    "                    cv.feature(cur[attribs[name]], **{name: value})\n",
    "                    continue\n",
    "            \n",
    "            elif code == 'openTag':\n",
    "                tag_name = content[1:-1]\n",
    "                tagList.append(tag_name)\n",
    "                if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                if not tag_name in excludeTags:\n",
    "                    if tag_name in counter:\n",
    "                        counter[tag_name] +=1\n",
    "                    else:\n",
    "                        counter[tag_name] = 1    \n",
    "                    cur[tag_name] = cv.node(tag_name)\n",
    "                    cv.feature(cur[tag_name], **{tag_name: counter[tag_name]})\n",
    "                    cv.meta(tag_name, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "#                 else:\n",
    "#                     if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "#                     cur[tag_name] = cv.node(tag_name)\n",
    "                continue\n",
    "                \n",
    "            elif code == 'openCloseTag':\n",
    "                tag_name = content[1:-2]\n",
    "                counter[tag_name] = 1 if tag_name not in counter else counter[tag_name] + 1\n",
    "                if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                cur[tag_name] = cv.node(tag_name)\n",
    "                cv.feature(cur[tag_name], **{tag_name: counter[tag_name]})\n",
    "                cv.meta(tag_name, description=\"open-close-tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "\n",
    "            elif code == 'comment':\n",
    "                continue\n",
    "                \n",
    "            elif code == 'bodyStop':\n",
    "                for ntp in cur:\n",
    "                    if not ntp in secElems and not ntp == '_book':\n",
    "                        cv.terminate(cur[ntp])\n",
    "                for ntp in secElems[::-1]:\n",
    "                    cv.terminate(cur[ntp])\n",
    "                cv.terminate(cur['_book'])\n",
    "                del tagList[-1]\n",
    "                break\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "            \n",
    "# #             print(elem)\n",
    "# #             print(tagList)\n",
    "#             elem = elem.strip()\n",
    "#             if Comment == False:\n",
    "#                 if commentStartRE.fullmatch(elem): #DONE\n",
    "#                     if commentFullRE.fullmatch(elem):\n",
    "#                         continue\n",
    "#                     Comment = True\n",
    "#                     continue\n",
    "\n",
    "#                 elif openTagRE.fullmatch(elem): #DONE\n",
    "# #                     print(f'openTagRE = {elem}')\n",
    "#                     # These are the features linked to the coming nodes\n",
    "\n",
    "\n",
    "\n",
    "#                 elif openAttrTagRE.fullmatch(elem):\n",
    "# #                     print(f'openAttrTagRE = {elem}')\n",
    "#                     # These are the features linked to coming nodes\n",
    "#                     elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "#                     tag_split = elem.find(' ')\n",
    "#                     attribs = {key: val.strip() for key, val in [elem.split('=\"') for elem in elem[tag_split:-1].strip().split('\" ')]}\n",
    "#                     for key, val in attribs.items():\n",
    "#                         if val.strip('\"') in CORR_ATTRIB_VALS:\n",
    "#                             attribs[key] = CORR_ATTRIB_VALS[val.strip('\"')]\n",
    "#                         else:\n",
    "#                             attribs[key] = val.strip('\"')\n",
    "#                     if NegatedEditionTag == False:        \n",
    "#                         if 'type' in attribs:\n",
    "#                             if attribs['type'] == 'edition':\n",
    "#                                 NegatedEditionTag = True\n",
    "#                                 self.generic['urn'] = attribs['n'] if 'n' in attribs else 'not provided'\n",
    "#                                 continue\n",
    "#                     tag_name = elem[1:tag_split]\n",
    "#                     if tag_name.startswith('div'):\n",
    "#                         tag_name = 'div'\n",
    "#                     tag = tuple((tag_name, tuple(key for key in attribs.keys() if key not in {'corresp', 'merge', 'resp'})))\n",
    "#                     if tag_name in excludeTags:\n",
    "#                         if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "#                         tagList.append(tag_name)\n",
    "#                         cur[tag_name] = cv.node(tag_name)\n",
    "#                         continue\n",
    "#                     highest_value_attrib = max(self.len_attribs_dict[tag], \n",
    "#                                                key=lambda key: self.len_attribs_dict[tag][key])\n",
    "#                     sec = False\n",
    "                    \n",
    "#                     for v in attribs.values():\n",
    "#                         if v in self.section_elems[:]:\n",
    "#                             sec = True\n",
    "#                             value = v\n",
    "#                     if sec == True:\n",
    "#                         for k, v in attribs.items():\n",
    "#                             if v == value:\n",
    "#                                 if v == self.section_elems[0] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[::-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'first section level'\n",
    "#                                 elif len(self.section_elems) > 1 and v == self.section_elems[1] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:0:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'second section level'\n",
    "#                                 elif len(self.section_elems) > 2 and v == self.section_elems[2] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:1:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'third section level'\n",
    "#                                 elif len(self.section_elems) > 3 and v == self.section_elems[3] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:2:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'fourth section level'    \n",
    "#                                 elif len(self.section_elems) > 4 and v == self.section_elems[4] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:3:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'fifth section level'    \n",
    "#                                 elif len(self.section_elems) > 5 and v == self.section_elems[5] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:4:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'sixth section level'   \n",
    "#                                 else: #elif k == 'n': # in case k == 'n'!\n",
    "# #                                     print(f'openAttrTagRE = {elem}')\n",
    "#                                     v = attribs['subtype'] if 'subtype' in attribs else v\n",
    "#                                     tagList.append(v)\n",
    "#                                     content = attribs[highest_value_attrib].strip()\n",
    "#                                     desc = 'not provided'\n",
    "#                                     if v in self.section_elems:\n",
    "#                                         index = self.section_elems.index(v) - 1\n",
    "#                                         for ntp in self.section_elems[:index:-1]:\n",
    "#                                             if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     if v in cur: cv.terminate(cur[v])\n",
    "#                                     cur[v] = cv.node(v)\n",
    "#                                     if not content.isdigit():\n",
    "#                                         nonIntFeatures.add(v)\n",
    "#                                     cv.feature(cur[v], **{v: content})\n",
    "#                                     cv.meta(v, description=desc,)\n",
    "#                                     break\n",
    "#                                 tagList.append(v)\n",
    "#                                 content = attribs['n'].strip() if 'n' in attribs else attribs[highest_value_attrib].strip()\n",
    "#                                 if v in cur: cv.terminate(cur[v])\n",
    "#                                 cur[v] = cv.node(v)\n",
    "#                                 if not content.isdigit():\n",
    "#                                     nonIntFeatures.add(v)\n",
    "#                                 cv.feature(cur[v], **{v: content})\n",
    "#                                 if 'corresp' in attribs:\n",
    "#                                     cv.feature(cur[v], **{'corresp': attribs['corresp']})\n",
    "#                                     nonIntFeatures.add('corresp')\n",
    "#                                     cv.meta('corresp', description='this feature shows a correspondence with another source at the place indicated')\n",
    "#                                 cv.meta(v, description=desc,)\n",
    "#                                 break\n",
    "#                     else:\n",
    "#                         # If only one attrib differs: it cannot be made clear which name to choose, hence choose everything\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "#                         attribList = []\n",
    "#                         for attr in self.len_attribs_dict[tag]:\n",
    "#                             if self.len_attribs_dict[tag][attr] > 1:\n",
    "#                                 attribList.append(attr)\n",
    "#                         if len(attribList) > 1:\n",
    "                            \n",
    "#                             tag_name += '-' + '-'.join([v for k, v in attribs.items() \n",
    "#                                                         if k in attribList \n",
    "#                                                         and not k == highest_value_attrib\n",
    "#                                                         and not v[0].isdigit()])\n",
    "#                             if tag_name.endswith('-'):\n",
    "#                                 tag_name += '-'.join([v for k, v in attribs.items() \n",
    "#                                                         if k in attribList \n",
    "#                                                         and not v[0].isdigit()])\n",
    "#                         content = attribs['n'] if 'n' in attribs else attribs[highest_value_attrib]\n",
    "#                         tagList.append(tag_name)\n",
    "#                         if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "#                         cur[tag_name] = cv.node(tag_name)\n",
    "#                         if not content.isdigit():\n",
    "#                             nonIntFeatures.add(tag_name)\n",
    "#                         cv.feature(cur[tag_name], **{tag_name: content})\n",
    "#                         cv.meta(tag_name, description=\"not provided\",)\n",
    "#                         continue\n",
    "                        \n",
    "#                 elif closedAttrTagRE.fullmatch(elem):\n",
    "# #                     print(f'closedAttrTagRE = {elem}')\n",
    "#                     elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "#                     tag_split = elem.find(' ')\n",
    "#                     attribs = {key: val.strip() for key, val in [elem.split('=\"') for elem in elem[tag_split:-2].strip().split('\" ')]}\n",
    "#                     for key, val in attribs.items():\n",
    "#                         if val.strip('\"') in CORR_ATTRIB_VALS:\n",
    "#                             attribs[key] = CORR_ATTRIB_VALS[val.strip('\"')]\n",
    "#                         else:\n",
    "#                             attribs[key] = val.strip('\"')\n",
    "#                     tag_name = elem[1:tag_split]\n",
    "#                     if tag_name.startswith('div'):\n",
    "#                         tag_name = 'div'\n",
    "#                     tag = tuple((tag_name, tuple(key for key in attribs.keys() if key not in {'corresp', 'merge', 'resp'})))\n",
    "#                     highest_value_attrib = max(self.len_attribs_dict[tag], \n",
    "#                                                key=lambda key: self.len_attribs_dict[tag][key])\n",
    "#                     sec = False\n",
    "#                     for v in attribs.values():\n",
    "#                         if v in self.section_elems[:]:\n",
    "#                             sec = True\n",
    "#                             value = v\n",
    "#                             break\n",
    "#                     if sec == True:\n",
    "#                         for k, v in attribs.items():\n",
    "#                             if v == value:\n",
    "#                                 if v == self.section_elems[0] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[::-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'first section level'\n",
    "#                                 elif len(self.section_elems) > 1 and v == self.section_elems[1] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:0:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'second section level'\n",
    "#                                 elif len(self.section_elems) > 2 and v == self.section_elems[2] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:1:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'third section level'\n",
    "#                                 elif len(self.section_elems) > 3 and v == self.section_elems[3] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:2:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'fourth section level'    \n",
    "#                                 elif len(self.section_elems) > 4 and v == self.section_elems[4] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:3:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'fifth section level'    \n",
    "#                                 elif len(self.section_elems) > 5 and v == self.section_elems[5] and not k == 'n':\n",
    "#                                     for ntp in self.section_elems[:4:-1]:\n",
    "#                                         if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     desc = 'sixth section level'    \n",
    "#                                 else: #elif k == 'n': # in case k == 'n'!\n",
    "# #                                     print(f'openAttrTagRE = {elem}')\n",
    "#                                     v = attribs['subtype'] if 'subtype' in attribs else v\n",
    "#                                     content = attribs[highest_value_attrib].strip()\n",
    "#                                     desc = 'not provided'\n",
    "#                                     if v in self.section_elems:\n",
    "#                                         index = self.section_elems.index(v) - 1\n",
    "#                                         for ntp in self.section_elems[:index:-1]:\n",
    "#                                             if ntp in cur: cv.terminate(cur[ntp])\n",
    "#                                     if v in cur: cv.terminate(cur[v])\n",
    "#                                     cur[v] = cv.node(v)\n",
    "#                                     if not content.isdigit():\n",
    "#                                         nonIntFeatures.add(v)\n",
    "#                                     cv.feature(cur[v], **{v: content})\n",
    "#                                     cv.meta(v, description=desc,)\n",
    "#                                     break\n",
    "#                                 content = attribs['n'].strip() if 'n' in attribs else attribs[highest_value_attrib].strip()\n",
    "#                                 if v in cur: cv.terminate(cur[v])\n",
    "#                                 cur[v] = cv.node(v)                            \n",
    "#                                 if tag in self.opentags:\n",
    "#                                     n = cv.slot()    \n",
    "#                                 if not content.isdigit():\n",
    "#                                     nonIntFeatures.add(v)\n",
    "#                                 cv.feature(cur[v], **{v: content})\n",
    "#                                 if 'corresp' in attribs:\n",
    "#                                     cv.feature(cur[v], **{'corresp': attribs['corresp']})\n",
    "#                                     nonIntFeatures.add('corresp')\n",
    "#                                     cv.meta('corresp', description='this feature shows a correspondence with another source at the place indicated')\n",
    "#                                 cv.meta(v, description=desc,)\n",
    "#                                 break\n",
    "\n",
    "#                     else:\n",
    "#                         attribList = []\n",
    "#                         for attr in self.len_attribs_dict[tag]:\n",
    "#                             if self.len_attribs_dict[tag][attr] > 1:\n",
    "#                                 attribList.append(attr)\n",
    "#                         if len(attribList) > 1:\n",
    "#                             tag_name += '-' + '-'.join([v for k, v in attribs.items() \n",
    "#                                                         if k in attribList \n",
    "#                                                         and not k == highest_value_attrib\n",
    "#                                                         and not v[0].isdigit()])\n",
    "#                             if tag_name.endswith('-'):\n",
    "#                                 tag_name += '-'.join([v for k, v in attribs.items() \n",
    "#                                                         if k in attribList \n",
    "# #                                                         and not k == highest_value_attrib\n",
    "#                                                         and not v[0].isdigit()])\n",
    "#                         content = attribs['n'].strip() if 'n' in attribs else attribs[highest_value_attrib].strip()\n",
    "#                         if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "#                         cur[tag_name] = cv.node(tag_name)\n",
    "#                         if not content.isdigit():\n",
    "#                             nonIntFeatures.add(tag_name)\n",
    "#                         cv.feature(cur[tag_name], **{tag_name: content})\n",
    "#                         cv.meta(tag_name, description=\"not given\",)\n",
    "#                         continue\n",
    "\n",
    "#                 elif opencloseTagRE.fullmatch(elem):\n",
    "# #                     print(f'opencloseTagRE = {elem}')\n",
    "\n",
    "#                 else: # These are the text nodes themselves\n",
    "#                     if re.fullmatch(r'\\s*', elem):\n",
    "#                         continue\n",
    "#                     else:\n",
    "                        \n",
    "# #                                         cur['_sentence'] = cv.node('_sentence')\n",
    "# # #                                         cv.feature(cur['_sentence'], _sentence=counter['_sentence'])\n",
    "# #                                         cv.feature(w, _sentence=counter['_sentence'])\n",
    "                                        \n",
    "#             else:\n",
    "#                 if commentStopRE.fullmatch(elem):\n",
    "#                     Comment = False\n",
    "#                 continue\n",
    "        \n",
    "        \n",
    "        nonIntFeatures.update(('word', 'orig', 'main', 'norm', 'plain', 'beta_plain', 'lemma'))        \n",
    "        cv.meta('lemma', **{'coverage ratio': f'{round(lemma_counter[0] / ((lemma_counter[0] + lemma_counter[1]) / 100 ), 2)}%'})\n",
    "        for feature in cv.metaData:\n",
    "            if feature in nonIntFeatures:\n",
    "                cv.meta(feature, valueType='str')\n",
    "            else:\n",
    "                if feature == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    cv.meta(feature, valueType='int')\n",
    "        # Final check of tags\n",
    "        tm.indent(level=1)\n",
    "        if len(tagList) == 0:\n",
    "            tm.info('No tag mistake(s) found...')\n",
    "        else:\n",
    "            tm.info(str(len(tagList)) + ' tag error(s) found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(input_path, output_path, lang='generic',\n",
    "            version='1.0', metadata=convertor_metadata):\n",
    "    '''The convert function is the core of the tei2tf module\n",
    "    \n",
    "    It takes the following arguments:\n",
    "    in_path:  the path that contains the TEI formatted texts\n",
    "    out_path: the path to which the tf-files would be written\n",
    "    **kwargs: a dictionarry that is usually derived from the\n",
    "              config.py file, that contains all important\n",
    "              parameters for the conversion (see documentation)\n",
    "    '''\n",
    "    from tf_config import langsettings\n",
    "    langsettings = langsettings[lang]\n",
    "    udnorm       = langsettings['udnorm']\n",
    "    tm           = Timestamp()\n",
    "    slot_type    = langsettings['slot_type']\n",
    "    dir_struct   = langsettings['dir_struct'] #TODO write function that derives the requested data!\n",
    "    count1       = 0     # counts the number input files\n",
    "    count2       = 0     # counts the number of successfully processed files\n",
    "    sLemmatizer  = langsettings['lemmatizer']()\n",
    "    \n",
    "    # input-output file management\n",
    "    inpath = path.expanduser(input_path)\n",
    "    outpath = path.expanduser(output_path)\n",
    "\n",
    "    # Looping through the inpath and running the tf-conversion\n",
    "    for xmlfile in glob(f'{inpath}/**/*grc*.xml', recursive=True):\n",
    "        count1 +=1\n",
    "        if count1 > 1: print('\\n')\n",
    "        tm.info(f'parsing {xmlfile}\\n')\n",
    "        \n",
    "        # creation of data to extract metadata\n",
    "        # and to inject later into the Conversion object\n",
    "        data = xmlSplitter(xmlfile)\n",
    "        body_index, metadat = metadataReader(data, lang=lang, **langsettings['metadata'])\n",
    "        metadata.update(metadat)\n",
    "#         pprint(metadata)\n",
    "\n",
    "        # definition of output dir structure on the basis of metadata\n",
    "        dirs = []\n",
    "        for i in dir_struct:\n",
    "            assigned = False\n",
    "            for j in i:\n",
    "                if j in metadata:\n",
    "                    dirs.append(metadata[j])\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if assigned == False:\n",
    "                dirs.append(f'unknown {\"-\".join(i)}')\n",
    "        \n",
    "        # dirs is a list of lists of which the tagnames used are defined in config.py\n",
    "        # they usually correspond to something like (author, work, editor/edition)\n",
    "        # in case of multiple editions of the same work, a number will be prefixed\n",
    "        if path.isdir(f'{outpath}/{\"/\".join(dirs)}/tf/{version}'):\n",
    "            C = 1\n",
    "            while path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                C +=1\n",
    "            else:\n",
    "                TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "        else:\n",
    "            TF_PATH = f'{outpath}/{\"/\".join(dirs)}/tf/{version}'\n",
    "            \n",
    "        # setting up the text-fabric engine\n",
    "        TF = Fabric(locations=TF_PATH)\n",
    "        cv = CV(TF)\n",
    "        # initiating the Conversion class that provides all\n",
    "        # necessary data and methods for cv.walk()\n",
    "        x = Conversion(data[body_index:], metadata, sLemmatizer=sLemmatizer, lang=lang, **langsettings)\n",
    "        # running cv.walk() to generate the tf-files\n",
    "        good = cv.walk(\n",
    "            x.director,\n",
    "            slotType=slot_type,\n",
    "            otext=x.otext,\n",
    "            generic=x.generic,\n",
    "            intFeatures=x.intFeatures,\n",
    "            featureMeta=x.featureMeta,\n",
    "            warn=False,\n",
    "        )\n",
    "        # Count number of successfully converted files\n",
    "        if good: \n",
    "            count2 +=1\n",
    "            tm.info('Conversion was successful...')\n",
    "    tm.info(f'{count2} of {count1} works have successfully been converted!')\n",
    " \n",
    "    \n",
    "convert('~/github/pthu/pilot/sources/pt', '~/github/pthu/out', lang='greek')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO1 = '~/github/pthu'\n",
    "# REPO2 = REPO1 + '/sources'\n",
    "# VERSION = '1.0'\n",
    "\n",
    "# # Define subcorpus to convert\n",
    "# SOURCE = 'athenaeus'\n",
    "# LOC = 'athenaeus'\n",
    "# # Define the source where the (sub)corpus can be found\n",
    "# SRC_DIR = os.path.expanduser(f'{REPO2}/{SOURCE}')\n",
    "# # Define the export path\n",
    "# TF_DIR = os.path.expanduser(f'{REPO1}/{LOC}') \n",
    "# # Define the version of the export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup of the Convertor Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_ATTRIB_VALS = {\n",
    "    '{http://www.w3.org/XML/1998/namespace}id': 'id',\n",
    "    '{http://www.w3.org/XML/1998/namespace}lang': 'lang',\n",
    "    'boo': 'book',\n",
    "    'fo1otnote': 'footnote',\n",
    "    'fo6tnote': 'footnote',\n",
    "    'foo1tnote': 'footnote',\n",
    "    'foodnote': 'footnote',\n",
    "    'foonote': 'footnote',\n",
    "    'footn2ote': 'footnote',\n",
    "    'footno1te': 'footnote',\n",
    "    'footno3te': 'footnote',\n",
    "    'footnot': 'footnote',\n",
    "    'footnot1e': 'footnote',\n",
    "    'footnot2e': 'footnote',\n",
    "    'footnote1': 'footnote',\n",
    "    'footnote2': 'footnote',\n",
    "    'footnte': 'footnote',\n",
    "    'footnτote': 'footnote',\n",
    "    'footote': 'footnote',\n",
    "    'fotnote': 'footnote',\n",
    "    'Τfootnote': 'footnote',\n",
    "    'm5arginal': 'marginal',\n",
    "    'margi4nal': 'marginal',\n",
    "    'margial': 'marginal',\n",
    "    'marginael': 'marginal',\n",
    "    'marginai': 'marginal',\n",
    "    'marginale': 'marginal',\n",
    "    'marginalp': 'marginal',\n",
    "    'marginapl': 'marginal',\n",
    "    'marginaΕl': 'marginal',\n",
    "    'marginaΣl': 'marginal',\n",
    "    'marginl': 'marginal',\n",
    "    'margipnal': 'marginal',\n",
    "    'margnal': 'marginal',\n",
    "    'margpinal': 'marginal',\n",
    "    'marinalΑB': 'marginal',\n",
    "    'marpginal': 'marginal',\n",
    "    'märginal': 'marginal',\n",
    "    ' chapter': 'chapter',\n",
    "    ' section': 'section',\n",
    "    'antistrohe': 'antistrophe',\n",
    "    'chap0ter': 'chapter',\n",
    "    'chapter1': 'chapter',\n",
    "    'chapterer': 'chapter',\n",
    "    'chapters': 'chapter',\n",
    "    'chaptser': 'chapter',\n",
    "    'chaspter': 'chapter',\n",
    "    'ephymn.': 'ephymn',\n",
    "    'sction': 'section',\n",
    "    'sectionn': 'section',\n",
    "    'setence': 'sentence',\n",
    "    'setion': 'section',\n",
    "    'secton': 'section',\n",
    "    'subdsection': 'subsection',\n",
    "    'subection': 'subsection',\n",
    "    'pargraph': 'paragraph',\n",
    "}\n",
    "\n",
    "\n",
    "# Update the list of James Tauber with some additional forms\n",
    "ELISION.update(\n",
    "    {\n",
    "        'ἔσθ’': 'ἔστι',\n",
    "        'γ’': 'γέ',\n",
    "        'μ’': 'μή',\n",
    "        'τοσαῦτ’': 'τοσοῦτος',\n",
    "        'ἆρ’': 'ἆρα',\n",
    "        'προσῆλθ’': 'προσῆλθε',\n",
    "        'θ’': 'θε',\n",
    "        'ἐνθάδ’': 'ἐνθάδε',\n",
    "        'ἔστ’': 'ἔστε',\n",
    "        'τοτ’': 'τοτε',\n",
    "        'σ’': 'σε',\n",
    "        'οὔτ’': 'οὔτε',\n",
    "        'ἠδ’': 'ἠδη',\n",
    "        'τ’': 'τε',\n",
    "    }\n",
    "    )\n",
    "#Normalize ELISION to unaccented keys and normalized accented values\n",
    "ELISION_norm = {plainLow(k) + '’': normalize(NFC, v) for k, v in ELISION.items()}\n",
    "\n",
    "attributes = {'id', 'cols', 'hand', 'subtype', 'evidence', 'lang', 'value', 'direct', '{http://www.w3.org/XML/1998/namespace}id', 'status', 'from', 'to', 'corresp', 'who', 'key', 'ed', 'rows', 'cause', 'source', '{http://www.w3.org/XML/1998/namespace}lang', 'extent', 'part', 'targOrder', 'anchored', 'ana', 'target', 'quantity', 'default', 'unit', 'cert', 'reason', 'org', 'TEIform', 'instant', 'n', 'type', 'role', 'rend', 'place', 'break', 'desc', 'sample', 'met', 'resp', 'url'}\n",
    "attrib_type = {'*marturi/a', '*pro/klhsis', 'sphragis', 'proverb', 'bekker page', 'NarrProof', 'noclass', 'footnot', 'hexameter', 'complaint', 'statute', 'Parabasis', 'tetrameters', 'antiprelude', 'anapests', 'fo6tnote', 'marginaAl', 'marginalXXXIVv', 'Text', 'Continued', 'summary', 'proepirrheme', 'mesode', '*yh/fisma', 'prose', 'agreement', 'marginal919a', 'fragment', '*grafh/', 'num', 'footnote1', 'commentary', '*)ekmarturi/a', 'law', 'marginaΕl', '*xro/nos', 'festival', 'alternative', 'subsection', 'noparse', 'section', '*)ara/', 'challenge', 'footnτote', 'margin', 'eleg', 'meter', 'toc', 'footno1te', 'index', 'ethnic', 'Book', 'decree', 'marpginal', 'winner', 'boo', 'altnum', 'marginale', 'trimeter', 'Agon', 'episode', 'schedule', 'catchword', '*dialogismo\\\\s tw=n *(hmerw=n', 'marginai', 'Parodos', 'dates', 'footnte', 'marginalB', 'margina6l', 'margina70rl', 'footno3te', '*yhfi/smata', 'marginalW', 'proagon', 'prelude', 'salutation', 'margial', 'poem', 'monody', 'indictment', 'oath', '*sunhtopi/a *boiwtw=n kai\\\\ *fwke/wn', 'editorial', 'sling', 'testimonium', 'marginalHdt.', 'epirrheme', 'verse paraphrase', '*do/gma *summa/xwn', 'troch', '*sunqh=kai', 'Τfootnote', 'Antikatakeleusmos', 'lease', 'corr', 'strophe', 'footote', '*no/mos', 'continued', 'antepirrheme', 'Epirrheme', 'Lyric-Scene', 'iamb', 'footnot2e', 'm5arginal', 'translation', 'worktitle', 'margi4nal', 'antiproepirrheme', 'verse', 'will', 'names', 'resolution', 'marginal77v', 'antistrophe', '*do/gma *sune/drwn', 'dactyls', 'witnesses', 'inscription', 'group', 'footnote', 'mentioned', 'verse-paraphrase', 'clause', 'margina', 'depositions', 'foonote', 'chapter', 'footn2ote', 'subscription', 'Verse', 'nomorph', 'fo1otnote', 'intro', 'prologue', 'reply', 'Episode', 'Katakeleusmos', 'margpinal', 'constellation', 'elegiacs', 'antikatakeleusmos', 'explanation', 'place', 'language', 'desc', 'footnote2', 'tetralogy', 'marginal', 'part', 'nomSac', 'Choral', 'katakeleusmenos', 'trochees', '*marturi/ai', 'deposition', 'foo1tnote', 'month', 'marginalE', 'inscript', 'parabasis', 'marginapl', 'märginal', 'speaker', 'marginalC', 'subtitle', 'antipnigos', 'dact', 'suggestion', 'counter-plea', 'person', 'Extract', 'pnigos', 'direct', 'subtext', 'unspecified', 'katakeleusmos', 'textpart', 'term', 'emph', 'marginl', 'dialogue', '*diaqh=kai', 'close', 'Prologue', 'marginalp', 'Name', 'witness', 'terms', 'marginaDl', 'orig', '*ma/rtures', 'race', 'text', '*)epistolh/', 'header', 'footnot1e', 'foodnote', 'marinalΑB', 'iambic', '*(/orkoi', 'title', 'main', 'epode', 'book', 'marginaΣl', 'sub', 'choral', 'letter', 'oracle', 'Papyr', 'antikatakeleusmenos', 'marginael', 'paraphrase', 'iambics', 'antepirrhema', '*yh/fisma peri\\\\ *dwrea\\\\s toi=s a)po\\\\ *fulh=s', 'Exodus', 'drama', 'margipnal', 'lyric', 'fotnote', 'bibliography', 'spoken', 'lemma', 'Prose', 'margnal', '*no/moi', 'argument', 'epirrhema', 'edition', 'work', 'margina15vl'}\n",
    "attrib_subtype = {'hexameter', 'Parabasis', 'tetrameters', 'anapests', 'antiprelude', 'source', 'sentence', 'comment', 'page', 'Letter', 'fragment', 'conspectus', 'Antepirrheme', 'commentary', 'TOC', 'subsection', 'section', 'quaestio', 'subdsection', 'toc', 'auctorm', 'index', 'fabula', ' chapter', 'epistle', 'ephymn.', 'Book', 'chapterer', 'preface', 'exordium', 'Agon', 'castlist', 'episode', 'Parodos', 'proagon', 'prelude', 'poem', 'monody', 'chap0ter', 'epirrheme', 'ephymnion', 'Antikatakeleusmos', 'chaptser', 'strophe', 'dramatispersonae', 'line', 'antepirrheme', 'Epirrheme', 'sectionn', 'Lyric-Scene', 'ii_loci', 'sction', 'sigla', 'auctores', 'chaspter', 'verse', 'antistrohe', 'Pnigos', 'ancient', 'Antipnigos', 'antistrophe', 'volume', 'dactyls', 'haeresis', 'wolfii', 'chapter', 'appendix', ' section', 'iii_loci', 'number', 'Episode', 'Katakeleusmos', 'paragraph', 'antikatakeleusmos', 'ephymn', 'aphorism', 'corrigenda', 'part', 'Choral', 'hypothesis', 'katakeleusmenos', 'trochees', 'subection', 'parabasis', 'essay', 'proode', 'autorum', 'antipnigos', 'pnigos', 'kommos', 'epigram', 'katakeleusmos', 'addenda', 'dialogue', 'close', 'Prologue', 'supplementa', 'setion', 'ducangii', 'setence', 'entry', 'chapter1', 'index.1', 'chapters', 'epode', 'book', 'epigraph', 'speech', 'loci', 'letter', 'choral', 'antikatakeleusmenos', 'iambics', 'trochaic', 'iv_loci', 'Exodus', 'type', 'lyric', 'index.2', 'homilia', 'work'}\n",
    "tag_names = {'head', 'pb', 'note', 'hi', 'lg', 'gap', 'div1', 'seg', 'div2', 'sic', 'del', 'add', 'milestone', 'title', 'q', 'div', 'p', 'l', 'lb', 'argument', 'sp', 'div3', 'num', 'quote', 'speaker', 'bibl', 'date', 'ab', 'lemma', 'foreign'} # Biblical and Patristic literature only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of the TF director\n",
    "\n",
    "The function `authorWork(path)` reads some metadata from the sourcefiles to process them properly. Then we process the xml-files by reading them and calling the `cv.walk()` function. As a result, valid TF-packages should be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorWork(path):\n",
    "    author = None\n",
    "    editor = None\n",
    "    book = None\n",
    "    afound = False\n",
    "    efound = False\n",
    "    bfound = False\n",
    "    TitleStmt = False\n",
    "    metaTaglist = []\n",
    "    metaData = {}\n",
    "    \n",
    "    with open(path) as xml:\n",
    "        data = ' '.join([line.strip() for line in list(takewhile(lambda line: not bodyStartRE.search(line), xml.readlines()))])\\\n",
    "                      .replace('<', '#!#<')\\\n",
    "                      .replace('>', '>#!#')\\\n",
    "                      .split('#!#')\n",
    "        for elem in data:\n",
    "            elem = elem.strip('{ ,.}')\n",
    "            if elem == '':\n",
    "                continue\n",
    "            if elem.startswith('<body'):\n",
    "                break\n",
    "            elif elem.startswith('<titleStmt'):\n",
    "                TitleStmt = True\n",
    "            elif elem.startswith('</titleStmt'):\n",
    "                TitleStmt = False\n",
    "            elif TitleStmt == True:\n",
    "                if elem.startswith('<'):\n",
    "                    tag_split = elem.find(' ') if not elem.find(' ') == -1 else elem.find('>')\n",
    "                    metaTaglist.append(elem[1:tag_split])\n",
    "                else:\n",
    "                    if metaTaglist[-1] in metaData:\n",
    "                        metaData[metaTaglist[-1]] += \\\n",
    "                            f', {elem}' if not elem in metaData[metaTaglist[-1]] else ''\n",
    "                    else:\n",
    "                        metaData[metaTaglist[-1]] = elem\n",
    "        if not 'author' in metaData and not 'editor' in metaData:\n",
    "            TitleStmt = False\n",
    "            for elem in data:\n",
    "                elem = elem.strip('{ ,.}')\n",
    "                if elem.startswith('<body'):\n",
    "                    break\n",
    "                if elem == '':\n",
    "                    continue \n",
    "                elif elem.startswith('<biblStruct'):\n",
    "                    TitleStmt = True\n",
    "                elif elem.startswith('</biblStruct'):\n",
    "                    TitleStmt = False\n",
    "                elif TitleStmt == True:\n",
    "                    if elem.startswith('<'):\n",
    "                        tag_split = elem.find(' ') if not elem.find(' ') == -1 else elem.find('>')\n",
    "                        metaTaglist.append(elem[1:tag_split])\n",
    "                    else:\n",
    "                        if metaTaglist[-1] in metaData:\n",
    "                            metaData[metaTaglist[-1]] += \\\n",
    "                                f', {elem}' if not elem in metaData[metaTaglist[-1]] else ''\n",
    "                        else:\n",
    "                            metaData[metaTaglist[-1]] = elem\n",
    "\n",
    "    author = metaData['author'].title() if 'author' in metaData \\\n",
    "                else metaData['editor'].title() if 'editor' in metaData \\\n",
    "                else 'undefined'\n",
    "    \n",
    "    book = metaData['title'].replace('(Greek)', '').replace('.', '').replace(',', '').replace('Machine readable text', '').strip().title()\n",
    "    return (author, book)\n",
    "\n",
    "COUNTER1 = 0\n",
    "COUNTER2 = 0\n",
    "\n",
    "for xmlfile in glob.glob(SRC_DIR+'/**/*grc*.xml', recursive=True):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/canonical-greekLit/data/tlg0059/tlg004/tlg0059.tlg004.perseus-grc2.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg2042'+'/**/*grc*.xml', recursive=True):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg0031/tlg004/tlg0031.tlg004.perseus-grc2.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg0555/tlg002/tlg0555.tlg002.opp-grc1.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg0555/tlg001/tlg0555.tlg001.opp-grc1.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR +'/tlg0555/**/*grc*.xml', recursive=True):\n",
    "    if COUNTER1 >= 1:\n",
    "        print('\\n\\n')\n",
    "    COUNTER1 +=1\n",
    "\n",
    "    tm.info(f'parsing {xmlfile}\\n')\n",
    "    (author, book) = authorWork(xmlfile)\n",
    "    if os.path.isdir(f'{TF_DIR}/{author}/{book}/tf/{VERSION}'):\n",
    "        C = 1\n",
    "        while os.path.isdir(f'{TF_DIR}/{author}/{C}_{book}/tf/{VERSION}'):\n",
    "            C +=1\n",
    "        else:\n",
    "            TF_PATH = f'{TF_DIR}/{author}/{C}_{book}/tf/{VERSION}'\n",
    "    else:\n",
    "        TF_PATH = f'{TF_DIR}/{author}/{book}/tf/{VERSION}'\n",
    "    TF = Fabric(locations=TF_PATH)\n",
    "    cv = CV(TF)\n",
    "    x = Conversion(xmlfile)\n",
    "    slotType = 'word'\n",
    "    good = cv.walk(\n",
    "        x.director,\n",
    "        x.slotType,\n",
    "        otext=x.otext,\n",
    "        generic=x.generic,\n",
    "        intFeatures=x.intFeatures,\n",
    "        featureMeta=x.featureMeta,\n",
    "        warn=False,\n",
    "    )\n",
    "    if good: COUNTER2 +=1\n",
    "tm.info(f'{COUNTER2} of {COUNTER1} works have successfully been converted!')\n",
    "lemmatizer_open.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "kwargs = {'attrib_errors': {'Eusebus': 'Eusebius'}}\n",
    "elem = '<div textpart = \"   chapter\" ref=\" Eusebus   \">' \n",
    "\n",
    "def attribClean(elem, **kwargs):\n",
    "    elem = elem.strip('<>\\ ')\n",
    "    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "    tag = elem[:elem.find(' ')]\n",
    "    attribs = {k.strip(): v.strip('\" ') for k, v in [elem.split('=\"') \\\n",
    "        for elem in elem[elem.find(' '):].split('\" ')]}\n",
    "    if 'attrib_errors' in kwargs:\n",
    "        attribs = {k: (kwargs['attrib_errors'][v] if v in kwargs['attrib_errors'] else v)\\\n",
    "                   for k, v in attribs.items()}\n",
    "    return (tag, attribs)\n",
    "\n",
    "attribClean(elem, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '<di t/ >'\n",
    "print(s.strip('<>/ '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribClean(elem):\n",
    "    elem = elem.strip('<> ')\n",
    "    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "    tag = elem[:elem.find(' ')]\n",
    "    attribs = {k.strip(): v.strip('\" ') for k, v in [elem.split('=\"') \\\n",
    "        for elem in elem[elem.find(' '):].split('\" ')]}\n",
    "    if 'attrib_errors' in kwargs:\n",
    "        attribs = {k: (kwargs['attrib_errors'][v] if v in kwargs['attrib_errors'] else v)\\\n",
    "                   for k, v in attribs.items()}\n",
    "    return attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    name = 'tester'\n",
    "    \n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "        return text.lower() + cls.name\n",
    "\n",
    "dictio = {'norm': Test.normalize}\n",
    "        \n",
    "test = {\n",
    "    'lang': 'Custom',\n",
    "    'version': '1.0',\n",
    "    'slot_type': 'word',\n",
    "    'udnorm': 'NFD',\n",
    "    'dir_struct': ['author', 'book', 'editor'],\n",
    "    'sentence_delimit': ['.', ';'],\n",
    "    'lang_processor': Test,\n",
    "}\n",
    "\n",
    "# x = test['lang_processor']('test')\n",
    "print(dictio['norm']('DiT IS een TeST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .helpertools.tokenizer import splitWord\n",
    "\n",
    "splitWord('.,dit.is?!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''This basic tokenize method splits a string \n",
    "    on spaces, without returning empty strings.\n",
    "    '''\n",
    "    return list(filter(None, string.strip().split(' ')))\n",
    "#     return string.split(' ')\n",
    "\n",
    "s = ' dit   is een   hele mond   vol  '\n",
    "tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictio = {\"a\": (1,), \"b\": (1, 2, 3, 4), \"n\": (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)}\n",
    "# print(max(dictio, key=lambda key: len(v) for k, v in dictio.items() ))\n",
    "max(dictio, key=lambda key: dictio[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dictio = {\n",
    "    'text_formats':     {'orig': {'structure': '{orig} ',\n",
    "                                  'metadata': 'original format of the word including punctuation'},\n",
    "                         'main': {'structure': '{main} ',\n",
    "                                  'metadata': 'normalized format of the word excluding punctuation'},\n",
    "                         'norm': {'structure': '{norm} ',\n",
    "                                  'metadata': 'normalized format (James Tauber) of the word excluding punctuation'},\n",
    "                         'plain': {'structure': '{plain} ',\n",
    "                                   'metadata': 'plain format in lowercase'},\n",
    "                         'beta_plain': {'structure': '{beta_plain} ',\n",
    "                                        'metadata': 'plain format in lowercase betacode (=Greek in Roman characters'},\n",
    "                         'lemma': {'structure': '{lemma} ',\n",
    "                                   'metadata': 'possible lemmata of the original words'},\n",
    "                        },\n",
    "}\n",
    "\n",
    "# for i in dictio['text_formats']:\n",
    "#     dic[i] = dictio['text_formats'][i]['structure']\n",
    "    \n",
    "dic = {**{k: v['structure'] for k, v in dictio['text_formats'].items()}, **{1:1}}\n",
    "print(dic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plainLow('ἔχοντα/ἔχονται')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = ['a', 'b', 'c', 'd', 'e']\n",
    "index = check.index('c')\n",
    "print(check[:index-1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "langsettings = {'text_formats': {'fmt:text-orig-full': {'name': 'orig',\n",
    "                                                'format': '{pre}{orig}{post} ',\n",
    "                                                'metadata': 'original format of the word including punctuation'},\n",
    "                         'fmt:text-orig-main': {'name': 'main',\n",
    "                                                'format': '{main} ',\n",
    "                                                'metadata': 'normalized format of the word excluding punctuation'},\n",
    "                         'fmt:text-orig-norm': {'name': 'norm', \n",
    "                                                'format': '{norm} ',\n",
    "                                                'metadata': 'normalized format (James Tauber) of the word excluding punctuation'},\n",
    "                         'fmt:text-orig-plain': {'name': 'plain',\n",
    "                                                'format': '{plain} ',\n",
    "                                                'metadata': 'plain format in lowercase'},\n",
    "                         'fmt:text-orig-beta-plain': {'name': 'beta_plain',\n",
    "                                                'format': '{beta_plain} ',\n",
    "                                                'metadata': 'plain format in lowercase betacode (=Greek in Roman characters'},\n",
    "                         'fmt:text-orig-lemma': {'name': 'lemma',\n",
    "                                                'format': '{lemma} ',\n",
    "                                                'metadata': 'possible lemmata of the original words'},\n",
    "                        },}\n",
    "# print(langsettings['text_formats'])\n",
    "pprint({v['name']: v['metadata'] for k, v in langsettings['text_formats'].items()},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = OrderedSet()\n",
    "p.update(('a', 'b'))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'name'\n",
    "value = 'value'\n",
    "dictio = {}\n",
    "dictio.setdefault(key, {}).update(value)\n",
    "print(dictio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
