{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TEI XML to Text-Fabric Convertor\n",
    "\n",
    "XML-TEI textfiles can be converted to [Text-Fabric format](https://dans-labs.github.io/text-fabric/Model/File-formats/) by using this convertor. It has been designed for Greek, but it should also work with minimal adjustments for other languages (except for the implemented lemmatizer).\n",
    "\n",
    "See this [readme](https://github.com/pthu/patristics) for more information about the corpus and this work.\n",
    "\n",
    "See this [notebook](https://nbviewer.jupyter.org/github/annotation/banks/blob/master/programs/convert.ipynb) for a simple setup for a tf conversion if you like to build your own convertor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import betacode.conv\n",
    "# import ray\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from pprint import pprint\n",
    "from os import path\n",
    "from glob import glob\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import takewhile\n",
    "from ordered_set import OrderedSet\n",
    "from unicodedata import category, normalize\n",
    "from tf.fabric import Fabric, Timestamp\n",
    "from tf.convert.walker import CV\n",
    "from pprint import pprint\n",
    "# from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "from cltk.corpus.greek.alphabet import filter_non_greek\n",
    "from greek_normalisation.normalise import Normaliser\n",
    "# from greek_normalisation.norm_data import ELISIONS, MOVABLE\n",
    "\n",
    "# Local imports\n",
    "from helpertools.lemmatizer import lemmatize\n",
    "from helpertools.unicodetricks import *\n",
    "from helpertools.xmlparser import xmlSplitter, dataParser, metadataReader, attribsAnalysis #, lenAttribsDict, sectionElems\n",
    "from tf_config import langsettings\n",
    "from data.tlge_metadata import tlge_metadata\n",
    "from data.attrib_errors import error_dict\n",
    "from convertor_metadata import convertor_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate timer\n",
    "tm = Timestamp()\n",
    "# Initiate multiprocessing tool\n",
    "# ray.init()\n",
    "\n",
    "class csvConversion:\n",
    "    def __init__(self, data, metadat, header=True, sLemmatizer=None, lang='generic', **kwargs):\n",
    "        self.data              = data\n",
    "        self.lang              = lang\n",
    "        self.langsettings      = kwargs\n",
    "        self.metadata          = metadat\n",
    "        self.header            = header\n",
    "        self.sections          = list(filter(None, data[0].split('\\t')[:-1])) if header == True \\\n",
    "                                 else ( list(filter(None, self.metadata['citation_scheme'].split('/'))) if 'citation_scheme' in self.metadata \\\n",
    "                                        else list(filter(None, input(\"No header data could be found; please enter an appropriate header: \").split())) )\n",
    "        self.structs           = list(tuple(self.sections) + ('_sentence', '_phrase'))\n",
    "        self.lemmatizer        = sLemmatizer\n",
    "\n",
    "        # TF SPECIFIC VARIABLES\n",
    "        self.slotType         = self.langsettings['slot_type']\n",
    "        self.intFeatures      = set()\n",
    "        self.generic          = metadat\n",
    "                        \n",
    "        # Definition of text formats\n",
    "        self.otext = {\n",
    "            **{k: v['format'] for k, v in self.langsettings['text_formats'].items()}, \\\n",
    "            **{'sectionTypes': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'sectionFeatures': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'structureTypes': f'_book,{\",\".join(self.structs)}'}, \\\n",
    "            **{'structureFeatures': f'_book,{\",\".join(self.structs)}'}\n",
    "        }\n",
    "\n",
    "        # These are the feature metadata that are present in all tf-packages to be produced... \n",
    "        # Other metadata will be added during the run of the director()...\n",
    "        self.featureMeta = {v['name']: {'description': v['metadata']} \\\n",
    "                            for k, v in self.langsettings['text_formats'].items()}\n",
    "        \n",
    "        \n",
    "    def director(self, cv):\n",
    "        nonIntFeatures = {'otype', 'oslots',}\n",
    "        counter = dict(_sentence=1, _phrase=1)\n",
    "        cur = {}\n",
    "        linked_features_dict = {}\n",
    "        lemma_counter = [0, 0]\n",
    "        udnorm = self.langsettings['udnorm']\n",
    "        \n",
    "        cur['_book'] = cv.node('_book')\n",
    "        cv.feature(cur['_book'], _book=self.metadata['title'])\n",
    "        cv.meta('_book', description=self.metadata['title_full'])\n",
    "        nonIntFeatures.add('_book')\n",
    "        \n",
    "        if self.header == True:\n",
    "            csv_data = self.data[1:]\n",
    "        else:\n",
    "            csv_data = self.data\n",
    "            \n",
    "        head = False\n",
    "#         last_ref = None\n",
    "        res_text = None\n",
    "        non_splitters = self.langsettings['tokenizer_args']['non_splitters']\n",
    "        \n",
    "        for line in csv_data:\n",
    "#             print(f'line = {line}')\n",
    "            splitline = line.split('\\t')\n",
    "#             print(f'splitline = {splitline}')\n",
    "            ref = splitline[:-1]\n",
    "#             print(f'ref = {ref}')\n",
    "            text = splitline[-1].strip()\n",
    "#             print(f'text = {text}')\n",
    "            \n",
    "            # Handle sectioning\n",
    "            ind = 0\n",
    "            for sec in self.sections:\n",
    "                num = ind + 1\n",
    "                if sec in cur and cv.active(cur[sec]):\n",
    "                    cur_sec = cv.get(sec, cur[sec])\n",
    "                    new_sec = ref[ind]\n",
    "                    if not cur_sec == new_sec:\n",
    "                        for s in self.sections[:ind:-1]:\n",
    "                            cv.terminate(cur[s])\n",
    "                        cv.terminate(cur[sec])\n",
    "                        cur[sec] = cv.node(sec)\n",
    "                        cv.feature(cur[sec], **{sec: ref[ind]})\n",
    "                        cv.meta(sec, description=f'structure feature of the {num}{\"st\" if num == 1 else \"\"}{\"nd\" if num == 2 else \"\"}{\"rd\" if num == 3 else \"\"}{\"th\" if num > 3 else \"\"} level',)\n",
    "                else:\n",
    "                    cur[sec] = cv.node(sec)\n",
    "                    cv.feature(cur[sec], **{sec: ref[ind]})\n",
    "                    cv.meta(sec, description=f'structure feature of the {num}{\"st\" if num == 1 else \"\"}{\"nd\" if num == 2 else \"\"}{\"rd\" if num == 3 else \"\"}{\"th\" if num > 3 else \"\"} level',)\n",
    "                if not ref[ind].isdigit():\n",
    "                    nonIntFeatures.add(sec)\n",
    "                ind +=1    \n",
    "#             last_ref = ref\n",
    "            \n",
    "            # Handle text\n",
    "            if res_text != None:\n",
    "                text = res_text + text\n",
    "                res_text = None\n",
    "            if text.endswith(non_splitters):\n",
    "                ptext = text.split(' ')\n",
    "                res_text = ptext[-1].rstrip(''.join(non_splitters))\n",
    "                text = ' '.join(ptext[:-1])\n",
    "\n",
    "            if self.lang == 'greek':\n",
    "                try:\n",
    "                    text.encode('ascii')\n",
    "                    text = normalize(udnorm, betacode.conv.beta_to_uni(text))\n",
    "                except UnicodeEncodeError:\n",
    "                    text = normalize(udnorm, text)\n",
    "\n",
    "            for token in self.langsettings['tokenizer'](text, **self.langsettings['tokenizer_args']):\n",
    "#                 print(f'token = {token}')\n",
    "                pre, origword, post = token\n",
    "                            \n",
    "                # Handle headers\n",
    "                if '{' in pre:\n",
    "                    head = True\n",
    "                \n",
    "                if not plainLow(token[1]):\n",
    "                    if set(pre) & self.langsettings['sentence_delimit']:\n",
    "                        if '_phrase' in cur:\n",
    "                            cv.terminate(cur['_phrase'])\n",
    "                            counter['_phrase'] +=1\n",
    "                        if '_sentence' in cur:\n",
    "                            cv.terminate(cur['_sentence'])\n",
    "                            counter['_sentence'] +=1\n",
    "                    try:\n",
    "                        cv.resume(w)\n",
    "                        orig = cv.get('orig', w) + pre\n",
    "                        try:\n",
    "                            post = cv.get('post', w) + pre\n",
    "                            cv.feature(w, post=post)\n",
    "                        except:\n",
    "                            pass\n",
    "                        cv.feature(w, orig=orig)\n",
    "                        cv.terminate(w)\n",
    "                        continue\n",
    "                    except UnboundLocalError:\n",
    "                        continue\n",
    "                for s in ('_phrase', '_sentence'):\n",
    "                    if s not in cv.activeTypes():\n",
    "                        cur[s] = cv.node(s)\n",
    "                        cv.feature(cur[s], **{s: counter[s]})  \n",
    "\n",
    "                token_norm = self.langsettings['replace_func'](token) \\\n",
    "                          if 'replace_func' in self.langsettings \\\n",
    "                          else token\n",
    "#                     print(f'token_norm = {token_norm}')\n",
    "                _, words, __ = token_norm\n",
    "                            \n",
    "                if head == True:\n",
    "                    if '}' in post:\n",
    "                        head = False\n",
    "                    if not '}' in pre:\n",
    "                        content = normalize(udnorm, f'{pre}{origword}{post}')\n",
    "                        if 'head' in cur and not cv.linked(cur['head']):\n",
    "                            content = f\"{cv.get('head', cur['head'])} {content}\"\n",
    "                            cv.feature(cur['head'], **{'head': content})\n",
    "                            continue\n",
    "                        if 'head' in cv.activeTypes() and cv.linked(cur['head']):\n",
    "                            cv.terminate(cur['head'])\n",
    "                        cur['head'] = cv.node('head')\n",
    "                        cv.feature(cur['head'], **{'head': content})\n",
    "                        cv.meta('head', description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                        nonIntFeatures.add('head')\n",
    "                        continue\n",
    "                    else:\n",
    "                        head = False\n",
    "                        if 'head' in cur and not cv.linked(cur['head']):\n",
    "                            content = f\"{cv.get('head', cur['head'])} {pre}\"\n",
    "                            cv.feature(cur['head'], **{'head': content})\n",
    "                            \n",
    "                words = tuple((('', word, '') for word in words.split(' ')))\n",
    "                preAssigned = False\n",
    "                origAssigned = False\n",
    "                postAssigned = False\n",
    "                for word in words:\n",
    "                    w = cv.slot()\n",
    "                    if preAssigned == False:\n",
    "                        cv.feature(w, pre=pre)\n",
    "                        cv.meta('pre', description='pre gives non-letter characters at the start of a word',)\n",
    "                        nonIntFeatures.add('pre')\n",
    "                        preAssigned = True\n",
    "                    if postAssigned == False:\n",
    "                        cv.feature(w, post=post)\n",
    "                        cv.meta('post', description='post gives non-letter characters at the end of a word',)\n",
    "                        nonIntFeatures.add('post')\n",
    "                        postAssigned = True\n",
    "\n",
    "                    for _, form in self.langsettings['text_formats'].items():\n",
    "                        name = form['name']\n",
    "                        func = form['function']\n",
    "                        meta = form['metadata']\n",
    "                        nonIntFeatures.add(name)\n",
    "\n",
    "                        if name == 'orig':\n",
    "                            if origAssigned == False:\n",
    "                                cv.feature(w, **{name: func(token)})\n",
    "                                cv.meta(name, description=meta)\n",
    "                                origAssigned = True\n",
    "                            else:\n",
    "                                cv.feature(w, **{name: ''})\n",
    "                        elif name == 'lemma':\n",
    "                            if len(words) == 1:\n",
    "                                lemma = func(token, self.lemmatizer)\n",
    "                            else:\n",
    "                                lemma = func(word, self.lemmatizer)\n",
    "                            cv.feature(w, **{name: lemma})\n",
    "                            cv.meta(name, description=meta)\n",
    "                            if lemma.startswith('*'):\n",
    "                                lemma_counter[1] +=1\n",
    "                            else:\n",
    "                                lemma_counter[0] +=1\n",
    "#                             print(f'lemma = {lemma}')\n",
    "#                             print(f'lemma_counter = {lemma_counter}')\n",
    "                        else:\n",
    "                            cv.feature(w, **{name: func(word)})\n",
    "                            cv.meta(name, description=meta)\n",
    "\n",
    "                if post != '':\n",
    "                    cv.feature(w, post=post)\n",
    "                    cv.meta('post', description='post gives non-letter characters at the end of a word',)\n",
    "                    nonIntFeatures.add('post')\n",
    "                    if set(post) & self.langsettings['phrase_delimit'] | self.langsettings['sentence_delimit']: # and TEXT == True:\n",
    "                        cv.terminate(cur['_phrase'])\n",
    "                        counter['_phrase'] +=1\n",
    "                    if set(post) & self.langsettings['sentence_delimit']: # and TEXT == True:\n",
    "                        cv.terminate(cur['_sentence'])\n",
    "                        counter['_sentence'] +=1\n",
    "            continue\n",
    "        \n",
    "        for ntp in self.structs[::-1]:\n",
    "            if ntp in cur: cv.terminate(cur[ntp])\n",
    "        for ntp in cur:\n",
    "            if ntp in cur: cv.terminate(cur[ntp])\n",
    "\n",
    "        if not lemma_counter == [0, 0]:\n",
    "            cv.meta('lemma', **{'coverage_ratio': f'{round(lemma_counter[0] / ((lemma_counter[0] + lemma_counter[1]) / 100 ), 2)}%'})\n",
    "        cv.meta('_sentence', description=f\"sentences defined by the following delimiters: {self.langsettings['sentence_delimit']}\",)\n",
    "        cv.meta('_phrase', description=f\"phrases defined by the following delimiters: {self.langsettings['phrase_delimit']}\",)\n",
    "        for feature in cv.metaData:\n",
    "            if feature in nonIntFeatures:\n",
    "                cv.meta(feature, valueType='str')\n",
    "            else:\n",
    "                if feature == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    cv.meta(feature, valueType='int')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class xmlConversion:\n",
    "    def __init__(self, data, meta, sLemmatizer=None, lang='generic', **kwargs):\n",
    "        self.data              = data\n",
    "        self.lang              = lang\n",
    "        self.langsettings      = kwargs\n",
    "        self.analyzed_dict, \\\n",
    "        self.sections          = attribsAnalysis(self.data, lang=self.lang, **kwargs)\n",
    "        self.structs           = tuple(tuple(self.sections) + ('_sentence', '_phrase'))\n",
    "        self.metadata          = meta\n",
    "        self.lemmatizer        = sLemmatizer\n",
    "\n",
    "        # TF SPECIFIC VARIABLES\n",
    "        self.slotType         = self.langsettings['slot_type']\n",
    "        self.intFeatures      = set()\n",
    "        self.generic          = meta\n",
    "#         pprint(self.generic)\n",
    "                        \n",
    "        # Definition of text formats\n",
    "        self.otext = {\n",
    "            **{k: v['format'] for k, v in self.langsettings['text_formats'].items()}, \\\n",
    "            **{'sectionTypes': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'sectionFeatures': f'{\",\".join(self.sections[:2] + [self.sections[-1]] if len(self.sections) > 2 else self.sections)}'}, \\\n",
    "            **{'structureTypes': f'_book,{\",\".join(self.structs)}'}, \\\n",
    "            **{'structureFeatures': f'_book,{\",\".join(self.structs)}'}\n",
    "        }\n",
    "\n",
    "        # These are the feature metadata that are present in all tf-packages to be produced... \n",
    "        # Other metadata will be added during the run of the director()...\n",
    "        self.featureMeta = {v['name']: {'description': v['metadata']} \\\n",
    "                            for k, v in self.langsettings['text_formats'].items()}\n",
    "\n",
    "    \n",
    "    def director(self, cv):\n",
    "        nonIntFeatures = {'otype', 'oslots',}\n",
    "        counter = dict(_sentence=1, _phrase=1)\n",
    "        cur = {}\n",
    "        tagList = []\n",
    "        linked_features_dict = {}\n",
    "        lemma_counter = [0, 0]\n",
    "        udnorm = self.langsettings['udnorm']\n",
    "        \n",
    "        tagList.append('_book')\n",
    "        cur['_book'] = cv.node('_book')\n",
    "        cv.feature(cur['_book'], _book=self.metadata['title'])\n",
    "        cv.meta('_book', description=\"bookname as given in the metadata part of the xml\")\n",
    "        nonIntFeatures.add('_book')\n",
    "        \n",
    "        for code, content in self.data:\n",
    "            if code == 'text':\n",
    "                if tagList[-1] in self.langsettings['non_text_tags']:\n",
    "                    tag = tagList[-1]\n",
    "                    content = normalize(udnorm, content)\n",
    "                    if not cv.linked(cur[tag]):\n",
    "                        content = f'{cv.get(tag, cur[tag])} {content}'\n",
    "                        cv.feature(cur[tag], **{tag: content})\n",
    "                    else:\n",
    "                        cv.feature(cur[tag], **{tag: content})\n",
    "                        cv.meta(tag, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                    nonIntFeatures.add(tag)\n",
    "                    continue\n",
    "     \n",
    "                if self.lang == 'greek':\n",
    "                    try:\n",
    "                        content.encode('ascii')\n",
    "                        content = normalize(udnorm, betacode.conv.beta_to_uni(content))\n",
    "                    except UnicodeEncodeError:\n",
    "                        content = normalize(udnorm, content)\n",
    "                if not set(self.structs) <= cv.activeTypes():        \n",
    "                    for struct in self.structs:\n",
    "                        if struct not in cv.activeTypes() and struct not in {'_phrase', '_sentence'}:\n",
    "                            cur[struct] = cv.node(struct)\n",
    "                            cv.feature(cur[struct], **{struct: 0})\n",
    "                         \n",
    "                for token in self.langsettings['tokenizer'](content, **self.langsettings['tokenizer_args']):\n",
    "#                     print(f'token = {token}')\n",
    "                    pre, _, post = token\n",
    "                    if not plainLow(token[1]):\n",
    "                        if set(pre) & self.langsettings['sentence_delimit']:\n",
    "                            if '_phrase' in cur:\n",
    "                                cv.terminate(cur['_phrase'])\n",
    "                                counter['_phrase'] +=1\n",
    "                            if '_sentence' in cur:\n",
    "                                cv.terminate(cur['_sentence'])\n",
    "                                counter['_sentence'] +=1\n",
    "                        try:\n",
    "                            cv.resume(w)\n",
    "                            orig = cv.get('orig', w) + pre\n",
    "                            try:\n",
    "                                post = cv.get('post', w) + pre\n",
    "                                cv.feature(w, post=post)\n",
    "                            except:\n",
    "                                pass\n",
    "                            cv.feature(w, orig=orig)\n",
    "                            cv.terminate(w)\n",
    "                            continue\n",
    "                        except UnboundLocalError:\n",
    "                            continue\n",
    "                    for s in ('_phrase', '_sentence'):\n",
    "                        if s not in cv.activeTypes():\n",
    "                            cur[s] = cv.node(s)\n",
    "                            cv.feature(cur[s], **{s: counter[s]})  \n",
    "                            \n",
    "                    token_norm = self.langsettings['replace_func'](token) \\\n",
    "                              if 'replace_func' in self.langsettings \\\n",
    "                              else token\n",
    "#                     print(f'token_norm = {token_norm}')\n",
    "                    _, words, __ = token_norm\n",
    "                    words = tuple((('', word, '') for word in words.split(' ')))\n",
    "                            \n",
    "                    preAssigned = False\n",
    "                    origAssigned = False\n",
    "                    postAssigned = False\n",
    "                    for word in words:\n",
    "                        w = cv.slot()\n",
    "                        if preAssigned == False:\n",
    "                            cv.feature(w, pre=pre)\n",
    "                            cv.meta('pre', description='pre gives non-letter characters at the start of a word',)\n",
    "                            nonIntFeatures.add('pre')\n",
    "                            preAssigned = True\n",
    "                        if postAssigned == False:\n",
    "                            cv.feature(w, post=post)\n",
    "                            cv.meta('post', description='post gives non-letter characters at the end of a word',)\n",
    "                            nonIntFeatures.add('post')\n",
    "                            postAssigned = True\n",
    "        \n",
    "                        for _, form in self.langsettings['text_formats'].items():\n",
    "                            name = form['name']\n",
    "                            func = form['function']\n",
    "                            meta = form['metadata']\n",
    "                            nonIntFeatures.add(name)\n",
    "                            \n",
    "                            if name == 'orig':\n",
    "                                if origAssigned == False:\n",
    "                                    cv.feature(w, **{name: func(token)})\n",
    "                                    cv.meta(name, description=meta)\n",
    "                                    origAssigned = True\n",
    "                                else:\n",
    "                                    cv.feature(w, **{name: ''})\n",
    "                            elif name == 'lemma':\n",
    "                                if len(words) == 1:\n",
    "                                    lemma = func(token, self.lemmatizer)\n",
    "                                else:\n",
    "                                    lemma = func(word, self.lemmatizer)\n",
    "                                cv.feature(w, **{name: lemma})\n",
    "                                cv.meta(name, description=meta)\n",
    "                                if lemma.startswith('*'):\n",
    "                                    lemma_counter[1] +=1\n",
    "                                else:\n",
    "                                    lemma_counter[0] +=1\n",
    "                            else:\n",
    "                                cv.feature(w, **{name: func(word)})\n",
    "                                cv.meta(name, description=meta)\n",
    "        \n",
    "                    if post != '':\n",
    "                        cv.feature(w, post=post)\n",
    "                        cv.meta('post', description='post gives non-letter characters at the end of a word',)\n",
    "                        nonIntFeatures.add('post')\n",
    "                        if set(post) & self.langsettings['phrase_delimit'] | self.langsettings['sentence_delimit']: # and TEXT == True:\n",
    "                            cv.terminate(cur['_phrase'])\n",
    "                            counter['_phrase'] +=1\n",
    "                        if set(post) & self.langsettings['sentence_delimit']: # and TEXT == True:\n",
    "                            cv.terminate(cur['_sentence'])\n",
    "                            counter['_sentence'] +=1\n",
    "                continue\n",
    "        \n",
    "        \n",
    "            elif code == 'closeTag':\n",
    "                if tagList[-1] in self.sections:\n",
    "                    index = self.sections.index(tagList[-1])\n",
    "                    for ntp in self.sections[:index:-1]:\n",
    "                        if ntp in cur: \n",
    "                            if not cv.linked(cur[ntp]):\n",
    "                                cv.slot()\n",
    "                            cv.terminate(cur[ntp])\n",
    "                    if not cv.linked(cur[tagList[-1]]):\n",
    "                        cv.slot()\n",
    "                    \n",
    "                elif tagList[-1] in self.langsettings['non_text_tags']:\n",
    "                    del tagList[-1]\n",
    "                    continue\n",
    "                if tagList[-1] in linked_features_dict:\n",
    "                    for i in linked_features_dict[tagList[-1]]:\n",
    "                        cv.terminate(cur[i])\n",
    "                    del linked_features_dict[tagList[-1]]\n",
    "                cv.terminate(cur[tagList[-1]])\n",
    "                del tagList[-1]\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            elif code in {'openAttrTag','closedAttrTag'}:\n",
    "                tag_name, attribs = content\n",
    "                value_key, name_keys = self.analyzed_dict[tag_name]\n",
    "                value = attribs[value_key]\n",
    "                if name_keys == 'tag':\n",
    "                    name = tag_name[0]\n",
    "                else:\n",
    "                    name = '-'.join([attribs[key] for key in name_keys])\n",
    "                if not value.isdigit():\n",
    "                    nonIntFeatures.add(name)\n",
    "                if code == 'openAttrTag':\n",
    "                    tagList.append(name)\n",
    "                if name in self.structs:\n",
    "                    if name in cur and cv.get(name, cur[name]) == 0 and value.isdigit():\n",
    "                        cv.feature(cur[name], **{name: str(int(value) - 1)})\n",
    "                    ind = self.structs.index(name)\n",
    "                    for struct in self.structs[:ind:-1]:\n",
    "                        if struct in cur: \n",
    "                            if not cv.linked(cur[struct]):\n",
    "                                cv.slot()\n",
    "                            cv.terminate(cur[struct])\n",
    "                    if name in cur:\n",
    "                        if not cv.linked(cur[name]):\n",
    "                            cv.slot()\n",
    "                        cv.terminate(cur[name])\n",
    "                    for struct in self.structs[:ind]:\n",
    "                        if not struct in cv.activeTypes():\n",
    "                            cur[struct] = cv.node(struct)\n",
    "                            cv.feature(cur[struct], **{struct: 0})\n",
    "                    cur[name] = cv.node(name)\n",
    "                    cv.feature(cur[name], **{name: value})\n",
    "                    cv.meta(name, description=f'structure feature of the {ind}{\"st\" if ind == 1 else \"\"}{\"nd\" if ind == 2 else \"\"}{\"rd\" if ind == 3 else \"\"}{\"th\" if ind > 3 else \"\"} level',)\n",
    "                else:\n",
    "                    if name in cur and cv.linked(cur[name]):\n",
    "                        cv.terminate(cur[name])\n",
    "                        cur[name] = cv.node(name)\n",
    "                        cv.feature(cur[name], **{name: value})\n",
    "                    elif name in cur and not cv.linked(cur[name]):\n",
    "                        pass\n",
    "                    else:\n",
    "                        cur[name] = cv.node(name)\n",
    "                        cv.feature(cur[name], **{name: value})\n",
    "                        cv.meta(name, description='no feature metadata have been provided; look at the name of the feature and at the data itself to get some clues')\n",
    "                if set(attribs) & self.langsettings['feature_attribs']:\n",
    "                    features = tuple(set(attribs) & self.langsettings['feature_attribs'])\n",
    "                    for f in features:\n",
    "                        if f in cur:\n",
    "                            cv.terminate(cur[f])\n",
    "                        cur[f] = cv.node(f)\n",
    "                        cv.feature(cur[f], **{f: attribs[f]})\n",
    "                        cv.meta(f, description='no feature metadata have been provided; look at the name of the feature and at the data itself to get some clues')\n",
    "                        if not attribs[f].isdigit():\n",
    "                            nonIntFeatures.add(f)\n",
    "                        if name in linked_features_dict:\n",
    "                            linked_features_dict[name].append(f)\n",
    "                        else:\n",
    "                            linked_features_dict[name] = [f]\n",
    "                continue\n",
    "\n",
    "            \n",
    "            elif code == 'openTag':\n",
    "                tag_name = content\n",
    "                tagList.append(tag_name)\n",
    "                if tag_name in self.langsettings['non_text_tags']:\n",
    "                    nonIntFeatures.add(tag_name)\n",
    "                    if not tag_name in cur:\n",
    "                        cur[tag_name] = cv.node(tag_name)\n",
    "                        cv.feature(cur[tag_name], **{tag_name: ''})\n",
    "                        cv.meta(tag_name, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                        continue\n",
    "                    elif tag_name in cur and cv.linked(cur[tag_name]):\n",
    "                        cv.terminate(cur[tag_name])\n",
    "                        cur[tag_name] = cv.node(tag_name)\n",
    "                        cv.feature(cur[tag_name], **{tag_name: ''})\n",
    "                        cv.meta(tag_name, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                        continue\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    if tag_name in cur:\n",
    "                        cv.terminate(cur[tag_name])\n",
    "                    if tag_name in counter:\n",
    "                        counter[tag_name] +=1\n",
    "                    else:\n",
    "                        counter[tag_name] = 1\n",
    "                cur[tag_name] = cv.node(tag_name)\n",
    "                cv.feature(cur[tag_name], **{tag_name: counter[tag_name]})\n",
    "                cv.meta(tag_name, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                continue\n",
    "      \n",
    "        \n",
    "            elif code == 'openCloseTag':\n",
    "#                 tag_name = content[1:-2]\n",
    "                tag_name = content\n",
    "                counter[tag_name] = 1 if tag_name not in counter else counter[tag_name] + 1\n",
    "                if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                cur[tag_name] = cv.node(tag_name)\n",
    "                cv.feature(cur[tag_name], **{tag_name: counter[tag_name]})\n",
    "                cv.meta(tag_name, description=\"open-close-tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                continue\n",
    "                \n",
    "            elif code == 'comment':\n",
    "                continue\n",
    "                \n",
    "            elif code == 'bodyStop':\n",
    "                for ntp in cur:\n",
    "                    if not ntp in self.sections and not ntp == '_book':\n",
    "#                         if not cv.linked(cur[ntp]):\n",
    "#                             cv.slot()\n",
    "                        cv.terminate(cur[ntp])\n",
    "                for ntp in self.sections[::-1]:\n",
    "                    if not cv.linked(cur[ntp]):\n",
    "                        cv.slot()\n",
    "                    cv.terminate(cur[ntp])\n",
    "                cv.terminate(cur['_book'])\n",
    "                if tagList:\n",
    "                    del tagList[-1]\n",
    "                break\n",
    "        if not lemma_counter == [0, 0]:\n",
    "            cv.meta('lemma', **{'coverage_ratio': f'{round(lemma_counter[0] / ((lemma_counter[0] + lemma_counter[1]) / 100 ), 2)}%'})\n",
    "        cv.meta('_sentence', description=f\"sentences defined by the following delimiters: {self.langsettings['sentence_delimit']}\",)\n",
    "        cv.meta('_phrase', description=f\"phrases defined by the following delimiters: {self.langsettings['phrase_delimit']}\",)\n",
    "        for feature in cv.metaData:\n",
    "            if feature in nonIntFeatures:\n",
    "                cv.meta(feature, valueType='str')\n",
    "            else:\n",
    "                if feature == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    cv.meta(feature, valueType='int')\n",
    "        # Final check of tags\n",
    "        tm.indent(level=1)\n",
    "        if len(tagList) == 0:\n",
    "            tm.info('No tag mistake(s) found...')\n",
    "        else:\n",
    "            tm.info(str(len(tagList)) + ' tag error(s) found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(input_path, output_path, lang='generic',\n",
    "            version='2.0', metadata=convertor_metadata, langsettings=langsettings):\n",
    "    '''The convert function is the core of the tei2tf module\n",
    "    \n",
    "    It takes the following arguments:\n",
    "    in_path:  the path that contains the TEI formatted texts\n",
    "    out_path: the path to which the tf-files would be written\n",
    "    **kwargs: a dictionarry that is usually derived from the\n",
    "              config.py file, that contains all important\n",
    "              parameters for the conversion (see documentation)\n",
    "    '''\n",
    "    \n",
    "    tm           = Timestamp()\n",
    "    langsettings = langsettings[lang]\n",
    "    udnorm       = langsettings['udnorm']\n",
    "    slot_type    = langsettings['slot_type']\n",
    "    dir_struct   = langsettings['dir_struct']\n",
    "    sLemmatizer  = langsettings['lemmatizer']()\n",
    "    count1       = 0     # counts the number input files\n",
    "    count2       = 0     # counts the number of successfully processed files\n",
    "    \n",
    "    # input-output file management\n",
    "    inpath = path.expanduser(input_path)\n",
    "    outpath = path.expanduser(output_path)\n",
    "    \n",
    "    global process_file\n",
    "    \n",
    "    def process_file(file):\n",
    "        if file.endswith('.csv'):\n",
    "#             count1 +=1\n",
    "            tm.info(f'parsing {file}\\n')\n",
    "            filename = path.splitext(file)[0].split('/')[-1]\n",
    "            with open(file, 'r') as file_open:\n",
    "                data = file_open.readlines()\n",
    "                metadat = tlge_metadata[filename]\n",
    "#                 pprint(tlge_metadata)\n",
    "                metadat.update(metadata)\n",
    "#                 pprint(metadat)\n",
    "                \n",
    "                # definition of output dir structure on the basis of metadata\n",
    "                dirs = []\n",
    "                for i in dir_struct:\n",
    "                    assigned = False\n",
    "                    for j in i:\n",
    "                        if j in metadat:\n",
    "                            dirs.append(metadat[j])\n",
    "                            assigned = True\n",
    "                            break\n",
    "                    if assigned == False:\n",
    "                        dirs.append(f'unknown {\"-\".join(i)}')\n",
    "\n",
    "                # dirs is a list of lists of which the tagnames used are defined in config.py\n",
    "                # they usually correspond to something like (author, work, editor/edition)\n",
    "                # in case of multiple editions of the same work, a number will be prefixed\n",
    "                C = 1\n",
    "                if path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                    while path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                        C +=1\n",
    "                    else:\n",
    "                        TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "                else:\n",
    "                    TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "\n",
    "                # setting up the text-fabric engine\n",
    "                TF = Fabric(locations=TF_PATH, silent=True)\n",
    "                cv = CV(TF, silent=True)\n",
    "                # initiating the Conversion class that provides all\n",
    "                # necessary data and methods for cv.walk()\n",
    "                x = csvConversion(data, metadat, sLemmatizer=sLemmatizer, lang=lang, **langsettings)\n",
    "                # running cv.walk() to generate the tf-files\n",
    "                good = cv.walk(\n",
    "                    x.director,\n",
    "                    slotType=slot_type,\n",
    "                    otext=x.otext,\n",
    "                    generic=x.generic,\n",
    "                    intFeatures=x.intFeatures,\n",
    "                    featureMeta=x.featureMeta,\n",
    "                    warn=False,\n",
    "                )\n",
    "                # Count number of successfully converted files\n",
    "                if good: \n",
    "#                     count2 +=1\n",
    "                    tm.info('Conversion was successful...\\n')\n",
    "                else:\n",
    "                    tm.info('Unfortunately, conversion was not successful...\\n')\n",
    "   \n",
    "        elif file.endswith('.xml'):\n",
    "#             count1 +=1\n",
    "            if count1 > 1: print('\\n')\n",
    "            tm.info(f'parsing {file}\\n')\n",
    "\n",
    "            # creation of data to extract metadata\n",
    "            # and to inject later into the Conversion object\n",
    "            data = dataParser(xmlSplitter(file), lang=lang)\n",
    "            body_index, metadat = metadataReader(data, lang=lang, **langsettings['metadata'])\n",
    "            metadat.update(metadata)\n",
    "    #         pprint(metadata)\n",
    "\n",
    "            # definition of output dir structure on the basis of metadata\n",
    "            dirs = []\n",
    "            for i in dir_struct:\n",
    "                assigned = False\n",
    "                for j in i:\n",
    "                    if j in metadat:\n",
    "                        dirs.append(metadat[j])\n",
    "                        assigned = True\n",
    "                        break\n",
    "                if assigned == False:\n",
    "                    dirs.append(f'unknown {\"-\".join(i)}')\n",
    "\n",
    "            # dirs is a list of lists of which the tagnames used are defined in config.py\n",
    "            # they usually correspond to something like (author, work, editor/edition)\n",
    "            # in case of multiple editions of the same work, a number will be prefixed\n",
    "            C = 1\n",
    "            if path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                while path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                    C +=1\n",
    "                else:\n",
    "                    TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "            else:\n",
    "                TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "\n",
    "            # setting up the text-fabric engine\n",
    "            TF = Fabric(locations=TF_PATH, silent=True)\n",
    "            cv = CV(TF, silent=True)\n",
    "            # initiating the Conversion class that provides all\n",
    "            # necessary data and methods for cv.walk()\n",
    "            x = xmlConversion(data[body_index:], metadat, sLemmatizer=sLemmatizer, lang=lang, **langsettings)\n",
    "            # running cv.walk() to generate the tf-files\n",
    "            good = cv.walk(\n",
    "                x.director,\n",
    "                slotType=slot_type,\n",
    "                otext=x.otext,\n",
    "                generic=x.generic,\n",
    "                intFeatures=x.intFeatures,\n",
    "                featureMeta=x.featureMeta,\n",
    "                warn=False,\n",
    "            )\n",
    "            # Count number of successfully converted files\n",
    "            if good: \n",
    "#             count2 +=1\n",
    "                tm.info(f'Conversion of {file.split(\"/\")[-1]} was successful...!\\n')\n",
    "            else:\n",
    "                tm.info('Unfortunately, conversion of {file.split(\"/\")[-1]} was not successful...\\n')\n",
    "\n",
    "    # Looping through the inpath and running the tf-conversion\n",
    "#     for xmlfile in glob(f'{inpath}/**/*grc*.xml', recursive=True):\n",
    "    \n",
    "    \n",
    "#     for file in glob(f'{inpath}/**/*.*', recursive=True):\n",
    "    file_list = glob(f'{inpath}/**/*.*', recursive=True)\n",
    "#     for file in file_list:\n",
    "#         process_file(file)\n",
    "                                 \n",
    "    pool = Pool()\n",
    "    pool.map(process_file, file_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "        \n",
    "#     tm.info(f'{count2} of {count1} works have successfully been converted!')\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert('~/github/pthu/sources/pt', '~/github/pthu/out', lang='greek')\n",
    "# convert('~/github/pthu/sources/sourcetexts', '~/github/pthu/out/sources', lang='greek')\n",
    "# convert('~/github/pthu/sources/sourcetexts/First1KGreek', '~/github/pthu/greek_literature/Open Greek and Latin Project', lang='greek')\n",
    "# convert('~/github/pthu/sources/sourcetexts/canonical-greekLit', '~/github/pthu/greek_literature/Perseus Digital Library', lang='greek')\n",
    "# convert('~/github/pthu/sources/sourcetexts/canonical-greekLit/data/tlg0007/tlg081', '~/github/pthu/out/sources', lang='greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the library of the Open Greek and Latin Project\n",
    "# convert('~/github/pthu/sources/sourcetexts/First1KGreek', '~/github/pthu/greek_literature/Open Greek and Latin Project', lang='greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the library of the Perseus Digital Library\n",
    "# convert('~/github/pthu/sources/sourcetexts/canonical-greekLit', '~/github/pthu/greek_literature/Perseus Digital Library', lang='greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csv files\n",
    "# convert('~/github/tlgu-1/TEST/csv_test', '~/github/tlgu-1/TEST/csv_test/out', lang='greek')\n",
    "# convert('~/github/tlgu-1/TEST/test/', '~/github/tlgu-1/TEST/test/out', lang='greek')\n",
    "# convert('~/github/tlgu-1/out/csv', '~/github/tlgu-1/out/tf', lang='greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert manuscripts in XML (Münster & Birmingham)\n",
    "convert('~/github/pthu/sources/manuscripts/test/subtest', '~/github/pthu/sources/manuscripts/test/out', lang='greek_ntmss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1, 2, 3, 4, 5)\n",
    "ind = 0\n",
    "print(a[:ind:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '{Ζα !!![_ _ _]}'\n",
    "print(splitPunc(s))\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
