{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TEI XML to Text-Fabric Convertor\n",
    "\n",
    "XML-TEI textfiles can be converted to [Text-Fabric format](https://dans-labs.github.io/text-fabric/Model/File-formats/) by using this convertor. It has been designed for Greek, but it should also work with minimal adjustments for other languages (except for the implemented lemmatizer).\n",
    "\n",
    "See this [readme](https://github.com/pthu/patristics) for more information about the corpus and this work.\n",
    "\n",
    "See this [notebook](https://nbviewer.jupyter.org/github/annotation/banks/blob/master/programs/convert.ipynb) for a simple setup for a tf conversion if you like to build your own convertor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import betacode.conv\n",
    "\n",
    "from os import expanduser, path\n",
    "from glob import glob\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import takewhile\n",
    "from ordered_set import OrderedSet\n",
    "from unicodedata import category, normalize\n",
    "from tf.fabric import Fabric, Timestamp\n",
    "from tf.convert.walker import CV\n",
    "from pprint import pprint\n",
    "from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "from cltk.corpus.greek.alphabet import filter_non_greek\n",
    "from greek_normalisation.normalise import Normaliser\n",
    "from greek_normalisation.norm_data import ELISION, MOVABLE\n",
    "\n",
    "# Local imports\n",
    "from helpertools.lemmatizer import lemmatize\n",
    "from helpertools.unicodetricks import *\n",
    "from tffabric import config, languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(input_path, output_path, **kwargs):\n",
    "    '''The convert function is the core of the tei2tf module\n",
    "    \n",
    "    It takes the following arguments:\n",
    "    in_path:  the path that contains the TEI formatted texts\n",
    "    out_path: the path to which the tf-files would be written\n",
    "    **kwargs: a dictionarry that is usually derived from the\n",
    "              config.py file, that contains all important\n",
    "              parameters for the conversion (see documentation)\n",
    "    '''\n",
    "    tm         = Timestamp()\n",
    "    version    = kwargs['version']\n",
    "    slot_type  = kwargs['slotType']\n",
    "    dir_struct = kwargs['fileName'] #TODO write function that derives the requested data!\n",
    "    count1     = 0     # counts the number input files\n",
    "    count2     = 0     # counts the number of successfully processed files\n",
    "    \n",
    "    # input-output file management\n",
    "    inpath = expanduser(input_path)\n",
    "    outpath = expanduser(out_path)\n",
    "    \n",
    "    # Looping through the inpath and running the tf-conversion\n",
    "    for xmlfile in glob(f'{inpath}/**/*grc*.xml', recursive=True):\n",
    "        count1 +=1\n",
    "        if count1 > 1: print('\\n')\n",
    "        tm.info(f'parsing {xmlfile}\\n')\n",
    "        # dirs is a tuple of directory names defined in config.py\n",
    "        # they usually correspond to (author, work)\n",
    "        dirs = dirStruct(xmlfile, dir_struct) \n",
    "        # in case of multiple editions of the same work, a number will be prefixed\n",
    "        if path.isdir(f'{outpath}/{\"/\".join(dirs)}/tf/{version}'):\n",
    "            C = 1\n",
    "            while path.isdir(f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'):\n",
    "                C +=1\n",
    "            else:\n",
    "                TF_PATH = f'{outpath}/{\"/\".join(dirs)}/{C}/tf/{version}'\n",
    "        else:\n",
    "            TF_PATH = f'{outpath}/{\"/\".join(dirs)}/tf/{version}'\n",
    "            \n",
    "        # setting up the text-fabric engine\n",
    "        TF = Fabric(locations=TF_PATH)\n",
    "        cv = CV(TF)\n",
    "        # initiating the Conversion class that provides all\n",
    "        # necessary data and methods for cv.walk()\n",
    "        x = Conversion(xmlfile, kwargs)\n",
    "        # running cv.walk() to generate the tf-files\n",
    "        good = cv.walk(\n",
    "            x.director,\n",
    "            slotType,\n",
    "            otext=x.otext,\n",
    "            generic=x.generic,\n",
    "            intFeatures=x.intFeatures,\n",
    "            featureMeta=x.featureMeta,\n",
    "            warn=False,\n",
    "        )\n",
    "        # Count number of successfully converted files\n",
    "        if good: COUNTER2 +=1\n",
    "    tm.info(f'{COUNTER2} of {COUNTER1} works have successfully been converted!')\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO1 = '~/github/pthu'\n",
    "REPO2 = REPO1 + '/sources'\n",
    "VERSION = '1.0'\n",
    "\n",
    "# Define subcorpus to convert\n",
    "SOURCE = 'athenaeus'\n",
    "LOC = 'athenaeus'\n",
    "# Define the source where the (sub)corpus can be found\n",
    "SRC_DIR = os.path.expanduser(f'{REPO2}/{SOURCE}')\n",
    "# Define the export path\n",
    "TF_DIR = os.path.expanduser(f'{REPO1}/{LOC}') \n",
    "# Define the version of the export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup of the Convertor Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML FUNCTIONS:\n",
    "\n",
    "def xmlSplitter(xmlfile):\n",
    "    '''The xmlReader reads a XML file completely into memory,\n",
    "    while splitting the text on \"<\" and \">\" into a list.\n",
    "    '''\n",
    "    \n",
    "    #TODO: concatenate lines ending with '-'!\n",
    "    with open(xmlfile) as xml:\n",
    "        # the filter function ensures that no empty strings are returned\n",
    "        data = list(filter(None, \n",
    "                  ' '.join([line.strip() for line in xml.readlines()])\\\n",
    "                  .replace('<', '#!#<')\\\n",
    "                  .replace('>', '>#!#')\\\n",
    "                  .split('#!#')\n",
    "                     ))\n",
    "    return data\n",
    "\n",
    "def attribClean(elem, **kwargs):\n",
    "    '''attribClean reads an XML tag and processes a \n",
    "    thorough normalization on it, consisting of:\n",
    "    - strip() of whitespace\n",
    "    - normalization of elements with attributes into:\n",
    "        <name attrib1=\"attribname1\" attrib2=\"attribname2\">\n",
    "    - correction of mistakes defined in kwargs['attrib_errors']\n",
    "      kwargs is usually defined in config.py.\n",
    "      \n",
    "    The function returns a tuple with tag and attribs dict:\n",
    "    (tag, {keys: values})\n",
    "    '''\n",
    "    # clean the elem\n",
    "    elem = elem.strip('<>\\ ')\n",
    "    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "    # define the tag\n",
    "    tag = elem[:elem.find(' ')]\n",
    "    # convert the attributes to a dict\n",
    "    attribs = {k.strip(): v.strip('\" ') for k, v in [elem.split('=\"') \\\n",
    "        for elem in elem[elem.find(' '):].split('\" ')]}\n",
    "    # correct any mistakes in the attributes \n",
    "    # if 'attrib_errors' is provided in the config.py\n",
    "    if 'attrib_errors' in kwargs:\n",
    "        attribs = {k: (kwargs['attrib_errors'][v] \\\n",
    "                           if v in kwargs['attrib_errors'] \\\n",
    "                           else v) \\\n",
    "                       for k, v in attribs.items()}\n",
    "    return (tag, attribs)\n",
    "\n",
    "def xmlParser(elem, **kwargs):\n",
    "    '''The xmlParser is able to parse the elements\n",
    "    created by xmlSplitter(xmlfile). It returns a tuple\n",
    "    containing the type and normalized element: (type, elem)\n",
    "    \n",
    "    Normalization of element involves:\n",
    "    - strip() of whitespace\n",
    "    - normalization of elements with attributes into:\n",
    "        <name attrib1=\"attribname1\" attrib2=\"attribname2\">\n",
    "    - correction of mistakes defined in kwargs['attrib_errors']\n",
    "      (see attribClean)\n",
    "    '''\n",
    "    # RE PATTERNS\n",
    "    commentFullRE = re.compile(r'^<!--.*?-->$')\n",
    "    commentStartRE = re.compile(r'^<!--.*')\n",
    "    commentStopRE = re.compile(r'.*-->$')\n",
    "    openTagRE = re.compile(r'<[^/=]+?>')\n",
    "    closeTagRE = re.compile(r'</.+?>')\n",
    "    opencloseTagRE = re.compile(r'<[^/=]+?/ *?>')\n",
    "    openAttrTagRE = re.compile(r'<.+?=.+?[^/] *?>')\n",
    "    closedAttrTagRE = re.compile(r'<.+?=.+?/ *?>')\n",
    "    \n",
    "    # Application of patterns:\n",
    "    if commentFullRE.fullmatch(elem):\n",
    "        return ('commentFull', '')\n",
    "    elif commentStartRE.fullmatch(elem):\n",
    "        return ('commentStart', '')\n",
    "    elif commentStopRE.fullmatch(elem):\n",
    "        return ('commentStop', '')\n",
    "    elif openTagRE.fullmatch(elem):\n",
    "        return ('openTag', elem.strip('<> '))\n",
    "    elif closeTagRE.fullmatch(elem):\n",
    "        return ('closeTag', elem.strip('<>/ '))\n",
    "    elif opencloseTagRE.fullmatch(elem):\n",
    "        return ('opencloseTag', elem.strip('<>/ '))\n",
    "    elif openAttrTagRE.fullmatch(elem):\n",
    "        return ('openAttrTag', attribClean(elem, **kwargs))\n",
    "    elif closedAttrTagRE.fullmatch(elem):\n",
    "        return ('closedAttrTag', attribClean(elem, **kwargs))\n",
    "\n",
    "    \n",
    "# NORMALIZATION AND LEMMATIZER FUNCTIONS:\n",
    "def normalization(**kwargs):\n",
    "    \n",
    "\n",
    "def lemmatizer(lemma_dict_path):\n",
    "    '''the lemmatizer takes a Python dictionary provided\n",
    "    in the path (.py or .pickle).\n",
    "    If a .py file is provided, it needs to be in the same\n",
    "    folder as tei2tf. In that case, the same directory needs\n",
    "    an __init__.py file.\n",
    "    A .pickle file can be located anywhere, as long as the \n",
    "    path points to it.\n",
    "    '''\n",
    "    if lemma_dict_path.endswith('.pickle'):\n",
    "        lemmatizer_open = open(path.expanduser(lemma_dict_path), 'rb')\n",
    "        lemma_dict = pickle.load(lemmatizer_open)\n",
    "        lemmatizer_open.close()\n",
    "        return lemma_dict\n",
    "    else:\n",
    "        import lemma_dict_path\n",
    "        \n",
    "    \n",
    "def lemmatize(word, lemmatizer, **kwargs):\n",
    "    if 'udnorm' in kwargs:\n",
    "        word = normalize(kwargs['udnorm'], word.lower())\n",
    "    else: \n",
    "        word = word.lower()\n",
    "    if word in lemmatizer:\n",
    "        lemma = ','.join(lemmatizer[word])\n",
    "    else:\n",
    "        lemma = f'*{word}'\n",
    "    return lemma\n",
    "    \n",
    "    \n",
    "def metaData(self, **kwargs):\n",
    "    data = xmlSplitter(xmlfile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RE PATTERNS METADATA\n",
    "authorRE = re.compile(r'<author>|<author .+?>') #[^>]*?(.+)</author>')\n",
    "editorRE = re.compile(r'<editor>|<editor .+?>') #[^>]*?(.+)</editor>')\n",
    "titleRE = re.compile(r'<title>|<title .+?>')    #[^>]*?(.+)</title>')\n",
    "bodyStartRE = re.compile(r'<body>|<body .+?>')\n",
    "bodyStopRE = re.compile(r'</body>|</body .+?>')\n",
    "\n",
    "\n",
    "\n",
    "# Load Lemmatizer\n",
    "lemmatizer_open = open(os.path.expanduser(f'{REPO1}/tei_to_tf/helpertools/data/lemmatizer.pickle'), 'rb')\n",
    "lemmatizer = pickle.load(lemmatizer_open)\n",
    "\n",
    "# Set up James Tauber's normalizer\n",
    "jt_normalise = Normaliser().normalise\n",
    "# use: jt_normalise('greek_word') --> (greek_word, [type])\n",
    "\n",
    "# Set up betacode to unicode convertor\n",
    "beta_to_uni = Replacer()\n",
    "#use: beta_to_uni.beta_code(betacode_text)\n",
    "\n",
    "# Unicode standards\n",
    "NFD = 'NFD'\n",
    "NFC = 'NFC'\n",
    "\n",
    "class Conversion:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data = self.dataPreprocessor(self.path)\n",
    "        self.metadata = self.metaData(self.data)[0]\n",
    "        self.body_index = self.metaData(self.data)[1]\n",
    "        self.attribs_dict = self.attribsDict(self.data)[0]\n",
    "        self.opentags = self.attribsDict(self.data)[1]\n",
    "        self.len_attribs_dict = self.lenAttribsDict(self.attribs_dict)        \n",
    "        self.section_elems =  self.sectionElems(self.attribs_dict)\n",
    "\n",
    "        # TF SPECIFIC VARIABLES\n",
    "        self.slotType = 'word'\n",
    "        self.intFeatures = set()\n",
    "        self.generic = {'name': 'Patristic corpus from Perseus',\n",
    "                        'compiler': 'Ernst Boogert',\n",
    "                        'institution': 'Protestant Theological University (PThU) Amsterdam/Groningen, The Netherlands',\n",
    "                        'source1': 'Perseus Digital Library',\n",
    "                        'source2': 'Open Greek and Latin Project',\n",
    "                        'url1': 'https://github.com/PerseusDL/canonical-greekLit',\n",
    "                        'url2': 'https://github.com/OpenGreekAndLatin/First1KGreek',\n",
    "                        'lang': 'grc',\n",
    "                        'license': self.metadata['license'] if 'license' in self.metadata else 'not provided by source',\n",
    "                        'availableStructure': \",\".join(self.section_elems),\n",
    "                        'version': '1.0',\n",
    "                        'purpose': 'Making Perseus TEI formatted text available in TF',\n",
    "                        'status': 'initial unchecked conversion',\n",
    "                        '_book': self.metadata['titleStmt']['title'].replace('(Greek)', '').replace('.', '').replace(',', '').replace('Machine readable text', '').strip(),\n",
    "                        'author': self.metadata['titleStmt']['author'] \\\n",
    "                                    if 'author' in self.metadata['titleStmt'] else 'not provided',\n",
    "                        'editor': self.metadata['titleStmt']['editor'] \\\n",
    "                                    if 'editor' in self.metadata['titleStmt'] else 'not provided',\n",
    "                        'edition': ', '.join([v for k, v in self.metadata['biblStruct'].items()]) + '.',\n",
    "                        }\n",
    "        # Definition of text formats\n",
    "        self.otext = {'fmt:text-orig-full': '{orig} ',\n",
    "                      'fmt:text-orig-main': '{main} ',\n",
    "                      'fmt:text-orig-norm': '{norm} ',\n",
    "                      'fmt:text-orig-plain': '{plain} ',\n",
    "#                       'fmt:text-orig-beta-full': '{beta_orig} ',\n",
    "                      'fmt:text-orig-beta-plain': '{beta_plain} ',\n",
    "                      \n",
    "                      'fmt:text-orig-lemma': '{lemma} ',\n",
    "        # Definition of:\n",
    "                      # main sections (=bookname + first two section levels)\n",
    "                      'sectionTypes': f'{\"\" if not self.section_elems else \",\".join(self.section_elems[:2])},_sentence',\n",
    "                      'sectionFeatures': f'{\"\" if not self.section_elems else \",\".join(self.section_elems[:2])},_sentence',\n",
    "#                       'sectionTypes': f'_book{\"\" if not self.section_elems else \",\" + \",\".join(self.section_elems[:2])}',\n",
    "#                       'sectionFeatures': f'_book{\"\" if not self.section_elems else \",\" + \",\".join(self.section_elems[:2])}',\n",
    "                      \n",
    "                      \n",
    "                      # structure (=bookname +all available levels)          \n",
    "                      'structureTypes': f'_book{\"\" if not self.section_elems else \",\" + \",\".join(self.section_elems[:])}',\n",
    "                      'structureFeatures': f'_book{\"\" if not self.section_elems else \",\" + \",\".join(self.section_elems[:])}',\n",
    "                      }\n",
    "        # These are the feature metadata that are present in all tf-packages to be produced... \n",
    "        # Other metadata will be added during the run of the director()...\n",
    "        self.featureMeta = {\n",
    "                '_sentence': {\n",
    "                    'description': 'numbering of sentences with \".\" as its delimiter',\n",
    "                },\n",
    "                '_book': {\n",
    "                    'description': 'the title of the book',\n",
    "                },\n",
    "                'orig': {\n",
    "                    'description': 'the original form of the text in unicode (UFD norm), including accents and punctuation; if the original text was in betacode, it has been converted to unicode without any normalization',\n",
    "                },\n",
    "                'main': {\n",
    "                    'description': 'the original form of the text in unicode (UFD norm), but extensively normalized (no punctuation and other trailing characters, no elision, normalization of accents.',\n",
    "                },\n",
    "                'norm': {\n",
    "                    'description': 'a normalized form of uni_main, according to the normalization of James Tauber: https://github.com/jtauber/greek-normalisation',\n",
    "                },\n",
    "                'plain': {\n",
    "                    'description': 'the plain form of the text in unicode stripped of all accents and punctuation',\n",
    "                },\n",
    "#                 'beta_orig': {\n",
    "#                     'description': 'the original form of the text, including accents and punctuation; this could be unicode or betacode, depending on the format of in the source',\n",
    "#                 },\n",
    "#                 'beta_main': {\n",
    "#                     'description': 'the original form of the text in betacode, but excluding punctuation and other trailing characters',\n",
    "#                 },\n",
    "                'beta_plain': {\n",
    "                    'description': 'the plain form of the text in betacode stripped of all accents and punctuation',\n",
    "                },\n",
    "                'lemma': {\n",
    "                    'description': 'the lemmatized form of the text tries to return as much as possible the words as a comma-separated list of possible lemmata. If no lemma could be found, the word is preceded by a \"*\". The lemmata have been defined by using the normalized text',\n",
    "                },\n",
    "            }\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    def metaData(self, data):\n",
    "        titleStmt = False\n",
    "        biblStruct = False\n",
    "        metaTaglist = []\n",
    "        metadata = {}\n",
    "        Comment = False\n",
    "        for elem in data:\n",
    "            if bodyStartRE.match(elem):\n",
    "                body_index = data.index(elem) + 1\n",
    "                break\n",
    "            elem = elem.strip('{ ,.}')\n",
    "            if Comment == False:\n",
    "                if elem == '':\n",
    "                    continue\n",
    "                elif commentStartRE.fullmatch(elem):\n",
    "                    if commentFullRE.fullmatch(elem):\n",
    "                        continue\n",
    "                    Comment = True\n",
    "                    continue\n",
    "                elif 'license' in elem.lower() or 'creative commons' in elem.lower():\n",
    "                    metadata['license'] = elem\n",
    "                    continue\n",
    "                else:\n",
    "                    if elem.startswith('<titleStmt'):\n",
    "                        titleStmt = True\n",
    "                        metadata['titleStmt'] = {}\n",
    "                    elif elem.startswith('</titleStmt'):\n",
    "                        titleStmt = False\n",
    "                    elif elem.startswith(('<biblStruct', '<sourceDesc')):\n",
    "                        biblStruct = True\n",
    "                        metadata['biblStruct'] = {}\n",
    "                    elif elem.startswith(('</biblStruct', '</sourceDesc')):\n",
    "                        biblStruct = False\n",
    "                    elif titleStmt == True:\n",
    "                        if elem.startswith('<'):\n",
    "                            tag_split = elem.find(' ') if not elem.find(' ') == -1 else elem.find('>')\n",
    "                            metaTaglist.append(elem[1:tag_split])\n",
    "                        else:\n",
    "                            if metaTaglist[-1] in metadata['titleStmt']:\n",
    "                                metadata['titleStmt'][metaTaglist[-1]] += \\\n",
    "                                  f', {elem}' if not elem in metadata['titleStmt'][metaTaglist[-1]] else ''\n",
    "                            else:\n",
    "                                metadata['titleStmt'][metaTaglist[-1]] = elem\n",
    "                    elif biblStruct == True:\n",
    "                        if elem.startswith('<'):\n",
    "                            tag_split = elem.find(' ') if not elem.find(' ') == -1 else elem.find('>')\n",
    "                            metaTaglist.append(elem[1:tag_split])\n",
    "                        else:\n",
    "                            if metaTaglist[-1] in metadata['biblStruct']:\n",
    "                                metadata['biblStruct'][metaTaglist[-1]] += \\\n",
    "                                  f', {elem}' if not elem in metadata['biblStruct'][metaTaglist[-1]] else ''\n",
    "                            else:\n",
    "                                metadata['biblStruct'][metaTaglist[-1]] = elem\n",
    "            else:\n",
    "                if commentStopRE.fullmatch(elem):\n",
    "                    Comment = False\n",
    "                continue\n",
    "        return (metadata, body_index)\n",
    "    \n",
    "    def attribsDict(self, data):\n",
    "        attribs_dict = {}\n",
    "        opentags = set()\n",
    "        Comment = False\n",
    "        for elem in data[self.body_index:]:\n",
    "            elem = elem.strip()\n",
    "            if Comment == False:\n",
    "                if elem == '':\n",
    "                    continue\n",
    "                elif commentStartRE.fullmatch(elem):\n",
    "                    if commentFullRE.fullmatch(elem):\n",
    "                        continue\n",
    "                    Comment = True\n",
    "                    continue\n",
    "                elif openAttrTagRE.fullmatch(elem):\n",
    "                    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "                    tag_split = elem.find(' ')\n",
    "                    tag = elem[1:tag_split]\n",
    "                    if tag.startswith('div'): \n",
    "                        tag = 'div'\n",
    "                    attribs = {key: val.strip() for key, val in [elem.split('=\"') for elem in elem[tag_split:-1].strip().split('\" ')]}\n",
    "                    for key, val in attribs.items():\n",
    "                        if val.strip('\"') in CORR_ATTRIB_VALS:\n",
    "                            attribs[key] = CORR_ATTRIB_VALS[val.strip('\"')]\n",
    "                        else:\n",
    "                            attribs[key] = val.strip('\"')\n",
    "                    if 'type' in attribs:\n",
    "                        if attribs['type'] == 'edition':\n",
    "                            continue\n",
    "                    tag_name = tuple((tag, tuple(key for key in attribs.keys() if key not in {'corresp', 'merge', 'resp'})))\n",
    "                    if tag_name in attribs_dict:\n",
    "                        for attrib in attribs:\n",
    "                            if attrib in attribs_dict[tag_name]:\n",
    "                                attribs_dict[tag_name][attrib].add(attribs[attrib])\n",
    "                            else:\n",
    "                                attribs_dict[tag_name][attrib] = OrderedSet([attribs[attrib]])\n",
    "                    else:\n",
    "                        attribs_dict[tag_name] = {k: OrderedSet([v]) for k, v in attribs.items()}\n",
    "                    opentags.add(tag_name)\n",
    "                elif closedAttrTagRE.fullmatch(elem):\n",
    "                    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "                    tag_split = elem.find(' ')\n",
    "                    tag = elem[1:tag_split]\n",
    "                    if tag.startswith('div'):\n",
    "                        tag = 'div'\n",
    "                    attribs = {key: val.strip() for key, val in [elem.split('=\"') for elem in elem[tag_split:-2].strip().split('\" ')]}\n",
    "                    for key, val in attribs.items():\n",
    "                        if val.strip('\"') in CORR_ATTRIB_VALS:\n",
    "                            attribs[key] = CORR_ATTRIB_VALS[val.strip('\"')]\n",
    "                        else:\n",
    "                            attribs[key] = val.strip('\"')\n",
    "                    tag_name = tuple((tag, tuple(key for key in attribs.keys() if key not in {'corresp', 'merge', 'resp'})))\n",
    "                    if tag_name in attribs_dict:\n",
    "                        for attrib in attribs:\n",
    "                            if attrib in attribs_dict[tag_name]:\n",
    "                                attribs_dict[tag_name][attrib].add(attribs[attrib])\n",
    "                            else:\n",
    "                                attribs_dict[tag_name][attrib] = OrderedSet([attribs[attrib]])\n",
    "                    else:\n",
    "                        attribs_dict[tag_name] = {k: OrderedSet([v]) for k, v in attribs.items()}\n",
    "                else:\n",
    "                    continue        \n",
    "            else:\n",
    "                if commentStopRE.fullmatch(elem):\n",
    "                    Comment = False\n",
    "                continue\n",
    "#         pprint(attribs_dict)\n",
    "        return attribs_dict, opentags\n",
    "    \n",
    "    def lenAttribsDict(self, dictionary):\n",
    "        return {key: {k: len(v) for k, v in val.items()} for key, val in dictionary.items()}\n",
    "    \n",
    "    def sectionElems(self, dictionary):\n",
    "        section_list = []\n",
    "        nonSections = nonSections = {'altpage', 'altnumbering', 'altref', 'mspage',}\n",
    "        for key, val in dictionary.items():\n",
    "            if key[0].startswith('div'):\n",
    "                number = False\n",
    "                sections = False\n",
    "                for k, v in val.items():\n",
    "                    if len(v) > 1 and sorted(v)[0][0].isdigit(): # The sorted guarantees that the numbers are in front\n",
    "                        number = True\n",
    "                    elif len(v) >= 1 and not sorted(v)[0][0].isdigit() and not v[0].startswith(('urn', 'textpart')):\n",
    "                        section_list = list(v)\n",
    "                        sections = True\n",
    "                if number == True and sections == True: # Identification of sectioning units\n",
    "                    break\n",
    "        if len(section_list) <= 2:\n",
    "            for key, val in dictionary.items():\n",
    "                if key[0].startswith('milestone') and all(i in key[1] for i in ('unit', 'n')):\n",
    "                    if sorted(val['n'])[0].isdigit():\n",
    "                        section_list.extend([i for i in val['unit'] if not i in nonSections])\n",
    "#         pprint(section_list)\n",
    "        return section_list\n",
    "    \n",
    "    def director(self, cv):\n",
    "        tm = Timestamp()  \n",
    "        Comment = False\n",
    "        NegatedEditionTag = False\n",
    "        nonIntFeatures = {'otype', 'oslots',}\n",
    "        excludeTags = {'head', 'note', 'title', 'bibl'}\n",
    "        counter = dict(_sentence=1, word=0)\n",
    "        cur = {}\n",
    "        tagList = []\n",
    "        closedSectionList = []\n",
    "        data = self.data\n",
    "        lemma_counter = [0, 0]\n",
    "        \n",
    "        tagList.append('_book')\n",
    "        cur['_book'] = cv.node('_book')\n",
    "        cv.feature(cur['_book'], _book=self.generic['_book'])\n",
    "        nonIntFeatures.add('_book')\n",
    "\n",
    "        for elem in data[self.body_index:]:\n",
    "#             print(elem)\n",
    "#             print(tagList)\n",
    "            elem = elem.strip()\n",
    "            if Comment == False:\n",
    "                if commentStartRE.fullmatch(elem): #DONE\n",
    "                    if commentFullRE.fullmatch(elem):\n",
    "                        continue\n",
    "                    Comment = True\n",
    "                    continue\n",
    "\n",
    "                elif openTagRE.fullmatch(elem): #DONE\n",
    "#                     print(f'openTagRE = {elem}')\n",
    "                    # These are the features linked to the coming nodes\n",
    "                    tag_name = elem[1:-1]\n",
    "                    tagList.append(tag_name)\n",
    "                    if tag_name in cur:\n",
    "                        cv.terminate(cur[tag_name])\n",
    "                    if not tag_name in excludeTags:\n",
    "                        if tag_name in counter:\n",
    "                            counter[tag_name] +=1\n",
    "                        else:\n",
    "                            counter[tag_name] = 1    \n",
    "                        cur[tag_name] = cv.node(tag_name)\n",
    "                        cv.feature(cur[tag_name], **{tag_name: counter[tag_name]})\n",
    "                        cv.meta(tag_name, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                    else:\n",
    "                        if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                        cur[tag_name] = cv.node(tag_name)\n",
    "                    continue\n",
    "\n",
    "                elif closeTagRE.fullmatch(elem): #DONE\n",
    "#                     print(f'closeTagRE = {elem}')\n",
    "                    # These are the signs showing the close of a feature belonging to preceding nodes\n",
    "                    if bodyStopRE.fullmatch(elem):\n",
    "                        if NegatedEditionTag == True:\n",
    "                            for ntp in cur:\n",
    "                                if not ntp in self.section_elems and not ntp == '_book':\n",
    "                                    cv.terminate(cur[ntp])\n",
    "                            for ntp in self.section_elems[::-1]:\n",
    "                                cv.terminate(cur[ntp])\n",
    "                            cv.terminate(cur['_book'])\n",
    "                            break\n",
    "                        else:\n",
    "                            for ntp in cur:\n",
    "                                if not ntp in self.section_elems and not ntp == '_book':\n",
    "                                    cv.terminate(cur[ntp])\n",
    "                            for ntp in self.section_elems[::-1]:\n",
    "                                cv.terminate(cur[ntp])\n",
    "                            cv.terminate(cur['_book'])\n",
    "                            del tagList[-1]\n",
    "                            break\n",
    "                    if tagList[-1] in excludeTags:\n",
    "                        pass\n",
    "                    elif tagList[-1] in self.section_elems:\n",
    "                        index = self.section_elems.index(tagList[-1])\n",
    "                        for ntp in self.section_elems[:index:-1]:\n",
    "                            if ntp in cur: cv.terminate(cur[ntp])     \n",
    "                    elif not cv.linked(cur[tagList[-1]]):\n",
    "                        pass\n",
    "#                     else:\n",
    "#                         cv.terminate(cur[tagList[-1]])\n",
    "                    del tagList[-1]\n",
    "\n",
    "                elif openAttrTagRE.fullmatch(elem):\n",
    "#                     print(f'openAttrTagRE = {elem}')\n",
    "                    # These are the features linked to coming nodes\n",
    "                    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "                    tag_split = elem.find(' ')\n",
    "                    attribs = {key: val.strip() for key, val in [elem.split('=\"') for elem in elem[tag_split:-1].strip().split('\" ')]}\n",
    "                    for key, val in attribs.items():\n",
    "                        if val.strip('\"') in CORR_ATTRIB_VALS:\n",
    "                            attribs[key] = CORR_ATTRIB_VALS[val.strip('\"')]\n",
    "                        else:\n",
    "                            attribs[key] = val.strip('\"')\n",
    "                    if NegatedEditionTag == False:        \n",
    "                        if 'type' in attribs:\n",
    "                            if attribs['type'] == 'edition':\n",
    "                                NegatedEditionTag = True\n",
    "                                self.generic['urn'] = attribs['n'] if 'n' in attribs else 'not provided'\n",
    "                                continue\n",
    "                    tag_name = elem[1:tag_split]\n",
    "                    if tag_name.startswith('div'):\n",
    "                        tag_name = 'div'\n",
    "                    tag = tuple((tag_name, tuple(key for key in attribs.keys() if key not in {'corresp', 'merge', 'resp'})))\n",
    "                    if tag_name in excludeTags:\n",
    "                        if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                        tagList.append(tag_name)\n",
    "                        cur[tag_name] = cv.node(tag_name)\n",
    "                        continue\n",
    "                    highest_value_attrib = max(self.len_attribs_dict[tag], \n",
    "                                               key=lambda key: self.len_attribs_dict[tag][key])\n",
    "                    sec = False\n",
    "                    \n",
    "                    for v in attribs.values():\n",
    "                        if v in self.section_elems[:]:\n",
    "                            sec = True\n",
    "                            value = v\n",
    "                    if sec == True:\n",
    "                        for k, v in attribs.items():\n",
    "                            if v == value:\n",
    "                                if v == self.section_elems[0] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[::-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'first section level'\n",
    "                                elif len(self.section_elems) > 1 and v == self.section_elems[1] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:0:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'second section level'\n",
    "                                elif len(self.section_elems) > 2 and v == self.section_elems[2] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:1:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'third section level'\n",
    "                                elif len(self.section_elems) > 3 and v == self.section_elems[3] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:2:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'fourth section level'    \n",
    "                                elif len(self.section_elems) > 4 and v == self.section_elems[4] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:3:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'fifth section level'    \n",
    "                                elif len(self.section_elems) > 5 and v == self.section_elems[5] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:4:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'sixth section level'   \n",
    "                                else: #elif k == 'n': # in case k == 'n'!\n",
    "#                                     print(f'openAttrTagRE = {elem}')\n",
    "                                    v = attribs['subtype'] if 'subtype' in attribs else v\n",
    "                                    tagList.append(v)\n",
    "                                    content = attribs[highest_value_attrib].strip()\n",
    "                                    desc = 'not provided'\n",
    "                                    if v in self.section_elems:\n",
    "                                        index = self.section_elems.index(v) - 1\n",
    "                                        for ntp in self.section_elems[:index:-1]:\n",
    "                                            if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    if v in cur: cv.terminate(cur[v])\n",
    "                                    cur[v] = cv.node(v)\n",
    "                                    if not content.isdigit():\n",
    "                                        nonIntFeatures.add(v)\n",
    "                                    cv.feature(cur[v], **{v: content})\n",
    "                                    cv.meta(v, description=desc,)\n",
    "                                    break\n",
    "                                tagList.append(v)\n",
    "                                content = attribs['n'].strip() if 'n' in attribs else attribs[highest_value_attrib].strip()\n",
    "                                if v in cur: cv.terminate(cur[v])\n",
    "                                cur[v] = cv.node(v)\n",
    "                                if not content.isdigit():\n",
    "                                    nonIntFeatures.add(v)\n",
    "                                cv.feature(cur[v], **{v: content})\n",
    "                                if 'corresp' in attribs:\n",
    "                                    cv.feature(cur[v], **{'corresp': attribs['corresp']})\n",
    "                                    nonIntFeatures.add('corresp')\n",
    "                                    cv.meta('corresp', description='this feature shows a correspondence with another source at the place indicated')\n",
    "                                cv.meta(v, description=desc,)\n",
    "                                break\n",
    "                    else:\n",
    "                        # If only one attrib differs: it cannot be made clear which name to choose, hence choose everything\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        attribList = []\n",
    "                        for attr in self.len_attribs_dict[tag]:\n",
    "                            if self.len_attribs_dict[tag][attr] > 1:\n",
    "                                attribList.append(attr)\n",
    "                        if len(attribList) > 1:\n",
    "                            \n",
    "                            tag_name += '-' + '-'.join([v for k, v in attribs.items() \n",
    "                                                        if k in attribList \n",
    "                                                        and not k == highest_value_attrib\n",
    "                                                        and not v[0].isdigit()])\n",
    "                            if tag_name.endswith('-'):\n",
    "                                tag_name += '-'.join([v for k, v in attribs.items() \n",
    "                                                        if k in attribList \n",
    "                                                        and not v[0].isdigit()])\n",
    "                        content = attribs['n'] if 'n' in attribs else attribs[highest_value_attrib]\n",
    "                        tagList.append(tag_name)\n",
    "                        if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                        cur[tag_name] = cv.node(tag_name)\n",
    "                        if not content.isdigit():\n",
    "                            nonIntFeatures.add(tag_name)\n",
    "                        cv.feature(cur[tag_name], **{tag_name: content})\n",
    "                        cv.meta(tag_name, description=\"not provided\",)\n",
    "                        continue\n",
    "                        \n",
    "                elif closedAttrTagRE.fullmatch(elem):\n",
    "#                     print(f'closedAttrTagRE = {elem}')\n",
    "                    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "                    tag_split = elem.find(' ')\n",
    "                    attribs = {key: val.strip() for key, val in [elem.split('=\"') for elem in elem[tag_split:-2].strip().split('\" ')]}\n",
    "                    for key, val in attribs.items():\n",
    "                        if val.strip('\"') in CORR_ATTRIB_VALS:\n",
    "                            attribs[key] = CORR_ATTRIB_VALS[val.strip('\"')]\n",
    "                        else:\n",
    "                            attribs[key] = val.strip('\"')\n",
    "                    tag_name = elem[1:tag_split]\n",
    "                    if tag_name.startswith('div'):\n",
    "                        tag_name = 'div'\n",
    "                    tag = tuple((tag_name, tuple(key for key in attribs.keys() if key not in {'corresp', 'merge', 'resp'})))\n",
    "                    highest_value_attrib = max(self.len_attribs_dict[tag], \n",
    "                                               key=lambda key: self.len_attribs_dict[tag][key])\n",
    "                    sec = False\n",
    "                    for v in attribs.values():\n",
    "                        if v in self.section_elems[:]:\n",
    "                            sec = True\n",
    "                            value = v\n",
    "                            break\n",
    "                    if sec == True:\n",
    "                        for k, v in attribs.items():\n",
    "                            if v == value:\n",
    "                                if v == self.section_elems[0] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[::-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'first section level'\n",
    "                                elif len(self.section_elems) > 1 and v == self.section_elems[1] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:0:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'second section level'\n",
    "                                elif len(self.section_elems) > 2 and v == self.section_elems[2] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:1:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'third section level'\n",
    "                                elif len(self.section_elems) > 3 and v == self.section_elems[3] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:2:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'fourth section level'    \n",
    "                                elif len(self.section_elems) > 4 and v == self.section_elems[4] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:3:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'fifth section level'    \n",
    "                                elif len(self.section_elems) > 5 and v == self.section_elems[5] and not k == 'n':\n",
    "                                    for ntp in self.section_elems[:4:-1]:\n",
    "                                        if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    desc = 'sixth section level'    \n",
    "                                else: #elif k == 'n': # in case k == 'n'!\n",
    "#                                     print(f'openAttrTagRE = {elem}')\n",
    "                                    v = attribs['subtype'] if 'subtype' in attribs else v\n",
    "                                    content = attribs[highest_value_attrib].strip()\n",
    "                                    desc = 'not provided'\n",
    "                                    if v in self.section_elems:\n",
    "                                        index = self.section_elems.index(v) - 1\n",
    "                                        for ntp in self.section_elems[:index:-1]:\n",
    "                                            if ntp in cur: cv.terminate(cur[ntp])\n",
    "                                    if v in cur: cv.terminate(cur[v])\n",
    "                                    cur[v] = cv.node(v)\n",
    "                                    if not content.isdigit():\n",
    "                                        nonIntFeatures.add(v)\n",
    "                                    cv.feature(cur[v], **{v: content})\n",
    "                                    cv.meta(v, description=desc,)\n",
    "                                    break\n",
    "                                content = attribs['n'].strip() if 'n' in attribs else attribs[highest_value_attrib].strip()\n",
    "                                if v in cur: cv.terminate(cur[v])\n",
    "                                cur[v] = cv.node(v)                            \n",
    "                                if tag in self.opentags:\n",
    "                                    n = cv.slot()    \n",
    "                                if not content.isdigit():\n",
    "                                    nonIntFeatures.add(v)\n",
    "                                cv.feature(cur[v], **{v: content})\n",
    "                                if 'corresp' in attribs:\n",
    "                                    cv.feature(cur[v], **{'corresp': attribs['corresp']})\n",
    "                                    nonIntFeatures.add('corresp')\n",
    "                                    cv.meta('corresp', description='this feature shows a correspondence with another source at the place indicated')\n",
    "                                cv.meta(v, description=desc,)\n",
    "                                break\n",
    "\n",
    "                    else:\n",
    "                        attribList = []\n",
    "                        for attr in self.len_attribs_dict[tag]:\n",
    "                            if self.len_attribs_dict[tag][attr] > 1:\n",
    "                                attribList.append(attr)\n",
    "                        if len(attribList) > 1:\n",
    "                            tag_name += '-' + '-'.join([v for k, v in attribs.items() \n",
    "                                                        if k in attribList \n",
    "                                                        and not k == highest_value_attrib\n",
    "                                                        and not v[0].isdigit()])\n",
    "                            if tag_name.endswith('-'):\n",
    "                                tag_name += '-'.join([v for k, v in attribs.items() \n",
    "                                                        if k in attribList \n",
    "#                                                         and not k == highest_value_attrib\n",
    "                                                        and not v[0].isdigit()])\n",
    "                        content = attribs['n'].strip() if 'n' in attribs else attribs[highest_value_attrib].strip()\n",
    "                        if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                        cur[tag_name] = cv.node(tag_name)\n",
    "                        if not content.isdigit():\n",
    "                            nonIntFeatures.add(tag_name)\n",
    "                        cv.feature(cur[tag_name], **{tag_name: content})\n",
    "                        cv.meta(tag_name, description=\"not given\",)\n",
    "                        continue\n",
    "\n",
    "                elif opencloseTagRE.fullmatch(elem):\n",
    "#                     print(f'opencloseTagRE = {elem}')\n",
    "                    tag_name = elem[1:-2]\n",
    "                    counter[tag_name] = 1 if tag_name not in counter else counter[tag_name] + 1\n",
    "                    if tag_name in cur: cv.terminate(cur[tag_name])\n",
    "                    cur[tag_name] = cv.node(tag_name)\n",
    "                    cv.feature(cur[tag_name], **{tag_name: counter[tag_name]})\n",
    "                    cv.meta(tag_name, description=\"open-close-tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "\n",
    "                else: # These are the text nodes themselves\n",
    "                    if re.fullmatch(r'\\s*', elem):\n",
    "                        continue\n",
    "                    else:\n",
    "                        for sec in self.section_elems:\n",
    "                            if sec not in cv.activeTypes():\n",
    "                                if sec == self.section_elems[-1]:\n",
    "                                    cur[sec] = cv.node(sec)\n",
    "                                    cv.feature(cur[sec], **{sec: 1})\n",
    "                                else:\n",
    "                                    cur[sec] = cv.node(sec)\n",
    "                                    cv.feature(cur[sec], **{sec: 0})\n",
    "                                    \n",
    "                        assigned = False\n",
    "                        for tag in tagList:\n",
    "                            if tag in excludeTags and tag in cv.activeTypes():\n",
    "                                elem = normalize(NFD, elem)\n",
    "#                                 n = cv.slot()\n",
    "#                                 cv.feature(n, **{tag: elem})\n",
    "#                                 cur[tag] = cv.node(tag)\n",
    "                                cv.feature(cur[tag], **{tag: elem})\n",
    "                                cv.meta(tag, description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "                                nonIntFeatures.add(tag)\n",
    "                                assigned = True\n",
    "                                break\n",
    "                        if assigned == True:\n",
    "                            continue\n",
    "                        \n",
    "#                         if tagList[-1] in excludeTags:\n",
    "#                             if tagList[-1] in cur and cv.linked(cur[tagList[-1]]): \n",
    "#                                 cv.terminate(cur[tagList[-1]])\n",
    "#                             elem = normalize(NFD, elem)\n",
    "#                             cur[tagList[-1]] = cv.node(tagList[-1])\n",
    "#                             cv.feature(cur[tagList[-1]], **{tagList[-1]: elem})\n",
    "#                             cv.meta(tagList[-1], description=\"open tag without further specification. See the name of the .tf-file for it's meaning\",)\n",
    "#                             nonIntFeatures.add(tagList[-1])\n",
    "#                             continue\n",
    "\n",
    "                        # In this stage, the unicode NFC format will be used, \n",
    "                        # to prevent that letter accents at the start of a word\n",
    "                        # will be chopped off; later we change to NFD\n",
    "                        try:\n",
    "                            elem.encode('ascii')\n",
    "#                             elem_uni = filter_non_greek(\n",
    "#                                            normalize(NFC, beta_to_uni.beta_code(elem))\n",
    "#                                        )\n",
    "                            elem_uni = normalize(NFC, beta_to_uni.beta_code(elem))\n",
    "                        except UnicodeEncodeError:\n",
    "#                             elem_uni = filter_non_greek(\n",
    "#                                            normalize(NFC, elem)\n",
    "#                                        )\n",
    "                            elem_uni = normalize(NFC, elem)\n",
    "                        # elem_uni is now containing a string of Greek text \n",
    "                        # with NFC normalization only\n",
    "                        for word in elem_uni.split():\n",
    "                            # word contains the original form of a Greek word\n",
    "                            if word == '':\n",
    "                                continue\n",
    "                                \n",
    "                            \n",
    "                            #counter['word'] +=1\n",
    "                            \n",
    "                            # pass the original form of the word into cv.feature\n",
    "                            for (preWord, midWord, postWord) in splitPunc(word):\n",
    "                                # midWord_pl will be used for various normalization actions\n",
    "                                midWord_pl = plainLow(midWord)\n",
    "                                if midWord_pl == '' or midWord_pl == 'ʼ': # ʼ is a single letter modifier\n",
    "                                    if '.' in postWord:\n",
    "                                        cv.terminate(cur['_sentence'])\n",
    "                                        counter['_sentence'] +=1\n",
    "                                    try:\n",
    "                                        \n",
    "                                        cv.resume(w)\n",
    "                                        orig = cv.get('orig', w) + preWord + midWord + postWord\n",
    "                                        try:\n",
    "                                            post = cv.get('post', w) + preWord + midWord + postWord\n",
    "                                            cv.feature(w, post=post)\n",
    "                                        # If midWord does not exist, any other sign \n",
    "                                        # is authomatically assigned to preWord not postWord.\n",
    "                                        # This preWord is bound to the end of the previous word\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                        cv.feature(w, orig=orig)\n",
    "                                        cv.terminate(w)\n",
    "                                        break\n",
    "                                    except UnboundLocalError:\n",
    "                                        break\n",
    "                                # Then we need to check for any elided form\n",
    "                                if '_sentence' not in cv.activeTypes():\n",
    "                                    cur['_sentence'] = cv.node('_sentence')\n",
    "                                    cv.feature(cur['_sentence'], _sentence=counter['_sentence'])\n",
    "                                \n",
    "                                if midWord_pl + '’' in ELISION_norm and postWord != '':\n",
    "                                    # we normalize the elision accent (many different ones have been used!)\n",
    "                                    # for the original form (any elision accent will be replaced by the standard one)\n",
    "                                    word = preWord + midWord + '’' + postWord[1:]\n",
    "                                    # we modify the midWord and the postWord\n",
    "                                    midWord = ELISION_norm[midWord_pl + '’'] # midWord gets the un-elided form!\n",
    "                                    postWord = postWord[1:]          # postWord loses the elision accent!\n",
    "                                # Deletion of movable-nu\n",
    "                                if midWord_pl.endswith(('εν', 'σιν', 'στιν')) and len(midWord_pl) >= 3:\n",
    "                                    midWord = midWord[:-1]\n",
    "                                # Handling final-sigma\n",
    "                                if midWord_pl.endswith('σ'):\n",
    "                                    midWord = midWord[:-1] + 'ς'\n",
    "                                # Handling various forms of ου\n",
    "                                if midWord_pl in ('ουχ', 'ουκ'):\n",
    "                                    midWord = midWord[:-1]\n",
    "                                # Handling ἐξ\n",
    "                                if midWord_pl == 'εξ':\n",
    "                                    midWord = midWord[:-1] + 'κ'\n",
    "                                # Definition of formats\n",
    "                                midWord_main = normalize(NFD, midWord.lower())\n",
    "                                midWord_norm = normalize(NFD, jt_normalise(midWord)[0])\n",
    "                                midWord_plain = plainLow(midWord)\n",
    "                                midWord_beta_plain = betacode.conv.uni_to_beta(midWord_plain)\n",
    "                                # Lemmatization and counter for calculating the coverage ratio\n",
    "                                lemma = lemmatize(midWord_main, lemmatizer)\n",
    "                                if lemma.startswith('*'):\n",
    "                                    lemma = lemmatize(midWord_norm, lemmatizer)\n",
    "                                    if lemma.startswith('*'):\n",
    "                                        lemma = lemmatize(midWord_plain, lemmatizer)\n",
    "                                if lemma.startswith('*'):\n",
    "                                    lemma_counter[1] +=1\n",
    "                                else:\n",
    "                                    lemma_counter[0] +=1\n",
    "                                \n",
    "                                # After the pre-processing, we continue to assigning everything\n",
    "                                # Slot assignment!\n",
    "                                w = cv.slot()\n",
    "                                # Feature assignment\n",
    "                                cv.feature(w, orig=word)\n",
    "                                cv.feature(w, main=midWord_main)\n",
    "                                cv.feature(w, norm=midWord_norm)\n",
    "                                cv.feature(w, plain=midWord_plain)\n",
    "                                cv.feature(w, beta_plain=midWord_beta_plain)\n",
    "                                cv.feature(w, lemma=lemma)\n",
    "#                                 cv.feature(w, _sentence=counter['_sentence'])\n",
    "                                \n",
    "                                # Creation of sentence feature at the start of the process\n",
    "#                                 if counter['_sentence'] == 0:\n",
    "#                                     counter['_sentence'] +=1\n",
    "#                                     cur['_sentence'] = cv.node('_sentence')\n",
    "#                                     cv.feature(cur['_sentence'], _sentence=counter['_sentence'])\n",
    "#                                     cv.feature(w, _sentence=counter['_sentence'])\n",
    "                                \n",
    "                                if preWord != '':\n",
    "                                    cv.feature(w, pre=preWord)\n",
    "                                    cv.meta('pre', description='pre gives non-letter characters at the start of a word',)\n",
    "                                    nonIntFeatures.add('pre')\n",
    "                                if postWord != '':\n",
    "                                    cv.feature(w, post=postWord)\n",
    "                                    cv.meta('post', description='post gives non-letter characters at the end of a word',)\n",
    "                                    nonIntFeatures.add('post')\n",
    "                                    if '.' in postWord:\n",
    "                                        cv.terminate(cur['_sentence'])\n",
    "                                        counter['_sentence'] +=1\n",
    "#                                         cur['_sentence'] = cv.node('_sentence')\n",
    "# #                                         cv.feature(cur['_sentence'], _sentence=counter['_sentence'])\n",
    "#                                         cv.feature(w, _sentence=counter['_sentence'])\n",
    "                                        \n",
    "            else:\n",
    "                if commentStopRE.fullmatch(elem):\n",
    "                    Comment = False\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        nonIntFeatures.update(('word', 'orig', 'main', 'norm', 'plain', 'beta_plain', 'lemma'))        \n",
    "        cv.meta('lemma', **{'coverage ratio': f'{round(lemma_counter[0] / ((lemma_counter[0] + lemma_counter[1]) / 100 ), 2)}%'})\n",
    "        for feature in cv.metaData:\n",
    "            if feature in nonIntFeatures:\n",
    "                cv.meta(feature, valueType='str')\n",
    "            else:\n",
    "                if feature == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    cv.meta(feature, valueType='int')\n",
    "        # Final check of tags\n",
    "        tm.indent(level=1)\n",
    "        if len(tagList) == 0:\n",
    "            tm.info('No tag mistake(s) found...')\n",
    "        else:\n",
    "            tm.info(str(len(tagList)) + ' tag error(s) found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_ATTRIB_VALS = {\n",
    "    '{http://www.w3.org/XML/1998/namespace}id': 'id',\n",
    "    '{http://www.w3.org/XML/1998/namespace}lang': 'lang',\n",
    "    'boo': 'book',\n",
    "    'fo1otnote': 'footnote',\n",
    "    'fo6tnote': 'footnote',\n",
    "    'foo1tnote': 'footnote',\n",
    "    'foodnote': 'footnote',\n",
    "    'foonote': 'footnote',\n",
    "    'footn2ote': 'footnote',\n",
    "    'footno1te': 'footnote',\n",
    "    'footno3te': 'footnote',\n",
    "    'footnot': 'footnote',\n",
    "    'footnot1e': 'footnote',\n",
    "    'footnot2e': 'footnote',\n",
    "    'footnote1': 'footnote',\n",
    "    'footnote2': 'footnote',\n",
    "    'footnte': 'footnote',\n",
    "    'footnτote': 'footnote',\n",
    "    'footote': 'footnote',\n",
    "    'fotnote': 'footnote',\n",
    "    'Τfootnote': 'footnote',\n",
    "    'm5arginal': 'marginal',\n",
    "    'margi4nal': 'marginal',\n",
    "    'margial': 'marginal',\n",
    "    'marginael': 'marginal',\n",
    "    'marginai': 'marginal',\n",
    "    'marginale': 'marginal',\n",
    "    'marginalp': 'marginal',\n",
    "    'marginapl': 'marginal',\n",
    "    'marginaΕl': 'marginal',\n",
    "    'marginaΣl': 'marginal',\n",
    "    'marginl': 'marginal',\n",
    "    'margipnal': 'marginal',\n",
    "    'margnal': 'marginal',\n",
    "    'margpinal': 'marginal',\n",
    "    'marinalΑB': 'marginal',\n",
    "    'marpginal': 'marginal',\n",
    "    'märginal': 'marginal',\n",
    "    ' chapter': 'chapter',\n",
    "    ' section': 'section',\n",
    "    'antistrohe': 'antistrophe',\n",
    "    'chap0ter': 'chapter',\n",
    "    'chapter1': 'chapter',\n",
    "    'chapterer': 'chapter',\n",
    "    'chapters': 'chapter',\n",
    "    'chaptser': 'chapter',\n",
    "    'chaspter': 'chapter',\n",
    "    'ephymn.': 'ephymn',\n",
    "    'sction': 'section',\n",
    "    'sectionn': 'section',\n",
    "    'setence': 'sentence',\n",
    "    'setion': 'section',\n",
    "    'secton': 'section',\n",
    "    'subdsection': 'subsection',\n",
    "    'subection': 'subsection',\n",
    "    'pargraph': 'paragraph',\n",
    "}\n",
    "\n",
    "\n",
    "# Update the list of James Tauber with some additional forms\n",
    "ELISION.update(\n",
    "    {\n",
    "        'ἔσθ’': 'ἔστι',\n",
    "        'γ’': 'γέ',\n",
    "        'μ’': 'μή',\n",
    "        'τοσαῦτ’': 'τοσοῦτος',\n",
    "        'ἆρ’': 'ἆρα',\n",
    "        'προσῆλθ’': 'προσῆλθε',\n",
    "        'θ’': 'θε',\n",
    "        'ἐνθάδ’': 'ἐνθάδε',\n",
    "        'ἔστ’': 'ἔστε',\n",
    "        'τοτ’': 'τοτε',\n",
    "        'σ’': 'σε',\n",
    "        'οὔτ’': 'οὔτε',\n",
    "        'ἠδ’': 'ἠδη',\n",
    "        'τ’': 'τε',\n",
    "    }\n",
    "    )\n",
    "#Normalize ELISION to unaccented keys and normalized accented values\n",
    "ELISION_norm = {plainLow(k) + '’': normalize(NFC, v) for k, v in ELISION.items()}\n",
    "\n",
    "attributes = {'id', 'cols', 'hand', 'subtype', 'evidence', 'lang', 'value', 'direct', '{http://www.w3.org/XML/1998/namespace}id', 'status', 'from', 'to', 'corresp', 'who', 'key', 'ed', 'rows', 'cause', 'source', '{http://www.w3.org/XML/1998/namespace}lang', 'extent', 'part', 'targOrder', 'anchored', 'ana', 'target', 'quantity', 'default', 'unit', 'cert', 'reason', 'org', 'TEIform', 'instant', 'n', 'type', 'role', 'rend', 'place', 'break', 'desc', 'sample', 'met', 'resp', 'url'}\n",
    "attrib_type = {'*marturi/a', '*pro/klhsis', 'sphragis', 'proverb', 'bekker page', 'NarrProof', 'noclass', 'footnot', 'hexameter', 'complaint', 'statute', 'Parabasis', 'tetrameters', 'antiprelude', 'anapests', 'fo6tnote', 'marginaAl', 'marginalXXXIVv', 'Text', 'Continued', 'summary', 'proepirrheme', 'mesode', '*yh/fisma', 'prose', 'agreement', 'marginal919a', 'fragment', '*grafh/', 'num', 'footnote1', 'commentary', '*)ekmarturi/a', 'law', 'marginaΕl', '*xro/nos', 'festival', 'alternative', 'subsection', 'noparse', 'section', '*)ara/', 'challenge', 'footnτote', 'margin', 'eleg', 'meter', 'toc', 'footno1te', 'index', 'ethnic', 'Book', 'decree', 'marpginal', 'winner', 'boo', 'altnum', 'marginale', 'trimeter', 'Agon', 'episode', 'schedule', 'catchword', '*dialogismo\\\\s tw=n *(hmerw=n', 'marginai', 'Parodos', 'dates', 'footnte', 'marginalB', 'margina6l', 'margina70rl', 'footno3te', '*yhfi/smata', 'marginalW', 'proagon', 'prelude', 'salutation', 'margial', 'poem', 'monody', 'indictment', 'oath', '*sunhtopi/a *boiwtw=n kai\\\\ *fwke/wn', 'editorial', 'sling', 'testimonium', 'marginalHdt.', 'epirrheme', 'verse paraphrase', '*do/gma *summa/xwn', 'troch', '*sunqh=kai', 'Τfootnote', 'Antikatakeleusmos', 'lease', 'corr', 'strophe', 'footote', '*no/mos', 'continued', 'antepirrheme', 'Epirrheme', 'Lyric-Scene', 'iamb', 'footnot2e', 'm5arginal', 'translation', 'worktitle', 'margi4nal', 'antiproepirrheme', 'verse', 'will', 'names', 'resolution', 'marginal77v', 'antistrophe', '*do/gma *sune/drwn', 'dactyls', 'witnesses', 'inscription', 'group', 'footnote', 'mentioned', 'verse-paraphrase', 'clause', 'margina', 'depositions', 'foonote', 'chapter', 'footn2ote', 'subscription', 'Verse', 'nomorph', 'fo1otnote', 'intro', 'prologue', 'reply', 'Episode', 'Katakeleusmos', 'margpinal', 'constellation', 'elegiacs', 'antikatakeleusmos', 'explanation', 'place', 'language', 'desc', 'footnote2', 'tetralogy', 'marginal', 'part', 'nomSac', 'Choral', 'katakeleusmenos', 'trochees', '*marturi/ai', 'deposition', 'foo1tnote', 'month', 'marginalE', 'inscript', 'parabasis', 'marginapl', 'märginal', 'speaker', 'marginalC', 'subtitle', 'antipnigos', 'dact', 'suggestion', 'counter-plea', 'person', 'Extract', 'pnigos', 'direct', 'subtext', 'unspecified', 'katakeleusmos', 'textpart', 'term', 'emph', 'marginl', 'dialogue', '*diaqh=kai', 'close', 'Prologue', 'marginalp', 'Name', 'witness', 'terms', 'marginaDl', 'orig', '*ma/rtures', 'race', 'text', '*)epistolh/', 'header', 'footnot1e', 'foodnote', 'marinalΑB', 'iambic', '*(/orkoi', 'title', 'main', 'epode', 'book', 'marginaΣl', 'sub', 'choral', 'letter', 'oracle', 'Papyr', 'antikatakeleusmenos', 'marginael', 'paraphrase', 'iambics', 'antepirrhema', '*yh/fisma peri\\\\ *dwrea\\\\s toi=s a)po\\\\ *fulh=s', 'Exodus', 'drama', 'margipnal', 'lyric', 'fotnote', 'bibliography', 'spoken', 'lemma', 'Prose', 'margnal', '*no/moi', 'argument', 'epirrhema', 'edition', 'work', 'margina15vl'}\n",
    "attrib_subtype = {'hexameter', 'Parabasis', 'tetrameters', 'anapests', 'antiprelude', 'source', 'sentence', 'comment', 'page', 'Letter', 'fragment', 'conspectus', 'Antepirrheme', 'commentary', 'TOC', 'subsection', 'section', 'quaestio', 'subdsection', 'toc', 'auctorm', 'index', 'fabula', ' chapter', 'epistle', 'ephymn.', 'Book', 'chapterer', 'preface', 'exordium', 'Agon', 'castlist', 'episode', 'Parodos', 'proagon', 'prelude', 'poem', 'monody', 'chap0ter', 'epirrheme', 'ephymnion', 'Antikatakeleusmos', 'chaptser', 'strophe', 'dramatispersonae', 'line', 'antepirrheme', 'Epirrheme', 'sectionn', 'Lyric-Scene', 'ii_loci', 'sction', 'sigla', 'auctores', 'chaspter', 'verse', 'antistrohe', 'Pnigos', 'ancient', 'Antipnigos', 'antistrophe', 'volume', 'dactyls', 'haeresis', 'wolfii', 'chapter', 'appendix', ' section', 'iii_loci', 'number', 'Episode', 'Katakeleusmos', 'paragraph', 'antikatakeleusmos', 'ephymn', 'aphorism', 'corrigenda', 'part', 'Choral', 'hypothesis', 'katakeleusmenos', 'trochees', 'subection', 'parabasis', 'essay', 'proode', 'autorum', 'antipnigos', 'pnigos', 'kommos', 'epigram', 'katakeleusmos', 'addenda', 'dialogue', 'close', 'Prologue', 'supplementa', 'setion', 'ducangii', 'setence', 'entry', 'chapter1', 'index.1', 'chapters', 'epode', 'book', 'epigraph', 'speech', 'loci', 'letter', 'choral', 'antikatakeleusmenos', 'iambics', 'trochaic', 'iv_loci', 'Exodus', 'type', 'lyric', 'index.2', 'homilia', 'work'}\n",
    "tag_names = {'head', 'pb', 'note', 'hi', 'lg', 'gap', 'div1', 'seg', 'div2', 'sic', 'del', 'add', 'milestone', 'title', 'q', 'div', 'p', 'l', 'lb', 'argument', 'sp', 'div3', 'num', 'quote', 'speaker', 'bibl', 'date', 'ab', 'lemma', 'foreign'} # Biblical and Patristic literature only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of the TF director\n",
    "\n",
    "The function `authorWork(path)` reads some metadata from the sourcefiles to process them properly. Then we process the xml-files by reading them and calling the `cv.walk()` function. As a result, valid TF-packages should be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = Timestamp()\n",
    "\n",
    "def authorWork(path):\n",
    "    author = None\n",
    "    editor = None\n",
    "    book = None\n",
    "    afound = False\n",
    "    efound = False\n",
    "    bfound = False\n",
    "    TitleStmt = False\n",
    "    metaTaglist = []\n",
    "    metaData = {}\n",
    "    \n",
    "    with open(path) as xml:\n",
    "        data = ' '.join([line.strip() for line in list(takewhile(lambda line: not bodyStartRE.search(line), xml.readlines()))])\\\n",
    "                      .replace('<', '#!#<')\\\n",
    "                      .replace('>', '>#!#')\\\n",
    "                      .split('#!#')\n",
    "        for elem in data:\n",
    "            elem = elem.strip('{ ,.}')\n",
    "            if elem == '':\n",
    "                continue\n",
    "            if elem.startswith('<body'):\n",
    "                break\n",
    "            elif elem.startswith('<titleStmt'):\n",
    "                TitleStmt = True\n",
    "            elif elem.startswith('</titleStmt'):\n",
    "                TitleStmt = False\n",
    "            elif TitleStmt == True:\n",
    "                if elem.startswith('<'):\n",
    "                    tag_split = elem.find(' ') if not elem.find(' ') == -1 else elem.find('>')\n",
    "                    metaTaglist.append(elem[1:tag_split])\n",
    "                else:\n",
    "                    if metaTaglist[-1] in metaData:\n",
    "                        metaData[metaTaglist[-1]] += \\\n",
    "                            f', {elem}' if not elem in metaData[metaTaglist[-1]] else ''\n",
    "                    else:\n",
    "                        metaData[metaTaglist[-1]] = elem\n",
    "        if not 'author' in metaData and not 'editor' in metaData:\n",
    "            TitleStmt = False\n",
    "            for elem in data:\n",
    "                elem = elem.strip('{ ,.}')\n",
    "                if elem.startswith('<body'):\n",
    "                    break\n",
    "                if elem == '':\n",
    "                    continue \n",
    "                elif elem.startswith('<biblStruct'):\n",
    "                    TitleStmt = True\n",
    "                elif elem.startswith('</biblStruct'):\n",
    "                    TitleStmt = False\n",
    "                elif TitleStmt == True:\n",
    "                    if elem.startswith('<'):\n",
    "                        tag_split = elem.find(' ') if not elem.find(' ') == -1 else elem.find('>')\n",
    "                        metaTaglist.append(elem[1:tag_split])\n",
    "                    else:\n",
    "                        if metaTaglist[-1] in metaData:\n",
    "                            metaData[metaTaglist[-1]] += \\\n",
    "                                f', {elem}' if not elem in metaData[metaTaglist[-1]] else ''\n",
    "                        else:\n",
    "                            metaData[metaTaglist[-1]] = elem\n",
    "\n",
    "    author = metaData['author'].title() if 'author' in metaData \\\n",
    "                else metaData['editor'].title() if 'editor' in metaData \\\n",
    "                else 'undefined'\n",
    "    \n",
    "    book = metaData['title'].replace('(Greek)', '').replace('.', '').replace(',', '').replace('Machine readable text', '').strip().title()\n",
    "    return (author, book)\n",
    "\n",
    "COUNTER1 = 0\n",
    "COUNTER2 = 0\n",
    "\n",
    "for xmlfile in glob.glob(SRC_DIR+'/**/*grc*.xml', recursive=True):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/canonical-greekLit/data/tlg0059/tlg004/tlg0059.tlg004.perseus-grc2.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg2042'+'/**/*grc*.xml', recursive=True):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg0031/tlg004/tlg0031.tlg004.perseus-grc2.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg0555/tlg002/tlg0555.tlg002.opp-grc1.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR + '/tlg0555/tlg001/tlg0555.tlg001.opp-grc1.xml'):\n",
    "# for xmlfile in glob.glob(SRC_DIR +'/tlg0555/**/*grc*.xml', recursive=True):\n",
    "    if COUNTER1 >= 1:\n",
    "        print('\\n\\n')\n",
    "    COUNTER1 +=1\n",
    "\n",
    "    tm.info(f'parsing {xmlfile}\\n')\n",
    "    (author, book) = authorWork(xmlfile)\n",
    "    if os.path.isdir(f'{TF_DIR}/{author}/{book}/tf/{VERSION}'):\n",
    "        C = 1\n",
    "        while os.path.isdir(f'{TF_DIR}/{author}/{C}_{book}/tf/{VERSION}'):\n",
    "            C +=1\n",
    "        else:\n",
    "            TF_PATH = f'{TF_DIR}/{author}/{C}_{book}/tf/{VERSION}'\n",
    "    else:\n",
    "        TF_PATH = f'{TF_DIR}/{author}/{book}/tf/{VERSION}'\n",
    "    TF = Fabric(locations=TF_PATH)\n",
    "    cv = CV(TF)\n",
    "    x = Conversion(xmlfile)\n",
    "    slotType = 'word'\n",
    "    good = cv.walk(\n",
    "        x.director,\n",
    "        x.slotType,\n",
    "        otext=x.otext,\n",
    "        generic=x.generic,\n",
    "        intFeatures=x.intFeatures,\n",
    "        featureMeta=x.featureMeta,\n",
    "        warn=False,\n",
    "    )\n",
    "    if good: COUNTER2 +=1\n",
    "tm.info(f'{COUNTER2} of {COUNTER1} works have successfully been converted!')\n",
    "lemmatizer_open.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "kwargs = {'attrib_errors': {'Eusebus': 'Eusebius'}}\n",
    "elem = '<div textpart = \"   chapter\" ref=\" Eusebus   \">' \n",
    "\n",
    "def attribClean(elem, **kwargs):\n",
    "    elem = elem.strip('<>\\ ')\n",
    "    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "    tag = elem[:elem.find(' ')]\n",
    "    attribs = {k.strip(): v.strip('\" ') for k, v in [elem.split('=\"') \\\n",
    "        for elem in elem[elem.find(' '):].split('\" ')]}\n",
    "    if 'attrib_errors' in kwargs:\n",
    "        attribs = {k: (kwargs['attrib_errors'][v] if v in kwargs['attrib_errors'] else v)\\\n",
    "                   for k, v in attribs.items()}\n",
    "    return (tag, attribs)\n",
    "\n",
    "attribClean(elem, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '<di t/ >'\n",
    "print(s.strip('<>/ '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribClean(elem):\n",
    "    elem = elem.strip('<> ')\n",
    "    elem = re.sub(r'\\s*=\\s*\"\\s*', '=\"', elem)\n",
    "    tag = elem[:elem.find(' ')]\n",
    "    attribs = {k.strip(): v.strip('\" ') for k, v in [elem.split('=\"') \\\n",
    "        for elem in elem[elem.find(' '):].split('\" ')]}\n",
    "    if 'attrib_errors' in kwargs:\n",
    "        attribs = {k: (kwargs['attrib_errors'][v] if v in kwargs['attrib_errors'] else v)\\\n",
    "                   for k, v in attribs.items()}\n",
    "    return attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    name = 'tester'\n",
    "    \n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "        return text.lower() + cls.name\n",
    "\n",
    "dictio = {'norm': Test.normalize}\n",
    "        \n",
    "test = {\n",
    "    'lang': 'Custom',\n",
    "    'version': '1.0',\n",
    "    'slot_type': 'word',\n",
    "    'udnorm': 'NFD',\n",
    "    'dir_struct': ['author', 'book', 'editor'],\n",
    "    'sentence_delimit': ['.', ';'],\n",
    "    'lang_processor': Test,\n",
    "}\n",
    "\n",
    "# x = test['lang_processor']('test')\n",
    "print(dictio['norm']('DiT IS een TeST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .helpertools.tokenizer import splitWord\n",
    "\n",
    "splitWord('.,dit.is?!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dit', 'is', 'een', 'hele', 'mond', 'vol']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(string):\n",
    "    '''This basic tokenize method splits a string \n",
    "    on spaces, without returning empty strings.\n",
    "    '''\n",
    "    return list(filter(None, string.strip().split(' ')))\n",
    "#     return string.split(' ')\n",
    "\n",
    "s = ' dit   is een   hele mond   vol  '\n",
    "tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
